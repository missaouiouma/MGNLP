[
    {
        "id": "acronym_identification",
        "data": {
            "description": "Acronym identification training and development sets for the acronym identification task at SDU@AAAI-21.\n",
            "url": "https://github.com/amirveyseh/AAAI-21-SDU-shared-task-1-AI",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "token-classification-other-acronym-identification"
            ]
        }
    },
    {
        "id": "ade_corpus_v2",
        "data": {
            "description": " ADE-Corpus-V2  Dataset: Adverse Drug Reaction Data.\n This is a dataset for Classification if a sentence is ADE-related (True) or not (False) and Relation Extraction between Adverse Drug Event and Drug.\n DRUG-AE.rel provides relations between drugs and adverse effects.\n DRUG-DOSE.rel provides relations between drugs and dosages.\n ADE-NEG.txt provides all sentences in the ADE corpus that DO NOT contain any drug-related adverse effects.\n",
            "url": "https://www.sciencedirect.com/science/article/pii/S1532046412000615",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification",
                "token-classification"
            ],
            "tasks": [
                "coreference-resolution",
                "fact-checking"
            ]
        }
    },
    {
        "id": "adversarial_qa",
        "data": {
            "description": "AdversarialQA is a Reading Comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles using an adversarial model-in-the-loop.\nWe use three different models; BiDAF (Seo et al., 2016), BERT-Large (Devlin et al., 2018), and RoBERTa-Large (Liu et al., 2019) in the annotation loop and construct three datasets; D(BiDAF), D(BERT), and D(RoBERTa), each with 10,000 training examples, 1,000 validation, and 1,000 test examples.\nThe adversarial human annotation paradigm ensures that these datasets consist of questions that current state-of-the-art models (at least the ones used as adversaries in the annotation loop) find challenging.\n",
            "url": "https://adversarialqa.github.io/",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "adv_glue",
        "data": {
            "description": "Adversarial GLUE Benchmark (AdvGLUE) is a comprehensive robustness evaluation benchmark\nthat focuses on the adversarial robustness evaluation of language models. It covers five\nnatural language understanding tasks from the famous GLUE tasks and is an adversarial\nversion of GLUE benchmark.\n",
            "url": "https://adversarialglue.github.io/",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference",
                "sentiment-classification",
                "text-classification-other-paraphrase-identification",
                "text-classification-other-qa-nli"
            ]
        }
    },
    {
        "id": "aeslc",
        "data": {
            "description": "\nA collection of email messages of employees in the Enron Corporation.\n\nThere are two features:\n  - email_body: email body text.\n  - subject_line: email subject text.\n",
            "url": "https://github.com/ryanzhumich/AESLC",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "summarization-other-email-headline-generation",
                "summarization-other-conversations-summarization",
                "summarization-other-multi-document-summarization",
                "summarization-other-aspect-based-summarization"
            ]
        }
    },
    {
        "id": "afrikaans_ner_corpus",
        "data": {
            "description": "Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.\n",
            "url": "https://repo.sadilar.org/handle/20.500.12185/299",
            "license": "",
            "givenLicense": "other",
            "language": [
                "af"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "ag_news",
        "data": {
            "description": "AG is a collection of more than 1 million news articles. News articles have been\ngathered from more than 2000 news sources by ComeToMyHead in more than 1 year of\nactivity. ComeToMyHead is an academic news search engine which has been running\nsince July, 2004. The dataset is provided by the academic comunity for research\npurposes in data mining (clustering, classification, etc), information retrieval\n(ranking, search, etc), xml, data compression, data streaming, and any other\nnon-commercial activity. For more information, please refer to the link\nhttp://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n\nThe AG's news topic classification dataset is constructed by Xiang Zhang\n(xiang.zhang@nyu.edu) from the dataset above. It is used as a text\nclassification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann\nLeCun. Character-level Convolutional Networks for Text Classification. Advances\nin Neural Information Processing Systems 28 (NIPS 2015).\n",
            "url": "http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "topic-classification"
            ]
        }
    },
    {
        "id": "ai2_arc",
        "data": {
            "description": "A new dataset of 7,787 genuine grade-school level, multiple-choice science questions, assembled to encourage research in\n advanced question-answering. The dataset is partitioned into a Challenge Set and an Easy Set, where the former contains\n only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm. We are also\n including a corpus of over 14 million science sentences relevant to the task, and an implementation of three neural baseline models for this dataset. We pose ARC as a challenge to the community.\n",
            "url": "https://allenai.org/data/arc",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en-US"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa",
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "air_dialogue",
        "data": {
            "description": "AirDialogue, is a large dataset that contains 402,038 goal-oriented conversations. To collect this dataset, we create a contextgenerator which provides travel and flight restrictions. Then the human annotators are asked to play the role of a customer or an agent and interact with the goal of successfully booking a trip given the restrictions.\n",
            "url": "https://worksheets.codalab.org/worksheets/0xa79833f4b3c24f4188cee7131b120a59",
            "license": "cc-by-nc-4.0",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "conversational",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-generation",
                "dialogue-modeling",
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "ajgt_twitter_ar",
        "data": {
            "description": "Arabic Jordanian General Tweets (AJGT) Corpus consisted of 1,800 tweets annotated as positive and negative. Modern Standard Arabic (MSA) or Jordanian dialect.\n",
            "url": "https://github.com/komari6/Arabic-twitter-corpus-AJGT",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "allegro_reviews",
        "data": {
            "description": "Allegro Reviews is a sentiment analysis dataset, consisting of 11,588 product reviews written in Polish and extracted \nfrom Allegro.pl - a popular e-commerce marketplace. Each review contains at least 50 words and has a rating on a scale \nfrom one (negative review) to five (positive review).\n\nWe recommend using the provided train/dev/test split. The ratings for the test set reviews are kept hidden. \nYou can evaluate your model using the online evaluation tool available on klejbenchmark.com.\n",
            "url": "https://github.com/allegro/klejbenchmark-allegroreviews",
            "license": "CC BY-SA 4.0",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "pl"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-scoring",
                "text-scoring"
            ]
        }
    },
    {
        "id": "allocine",
        "data": {
            "description": " Allocine Dataset: A Large-Scale French Movie Reviews Dataset.\n This is a dataset for binary sentiment classification, made of user reviews scraped from Allocine.fr.\n It contains 100k positive and 100k negative reviews divided into 3 balanced splits: train (160k reviews), val (20k) and test (20k).\n",
            "url": "https://github.com/TheophileBlard/french-sentiment-analysis-with-bert",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "fr"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "alt",
        "data": {
            "description": "The ALT project aims to advance the state-of-the-art Asian natural language processing (NLP) techniques through the open collaboration for developing and using ALT. It was first conducted by NICT and UCSY as described in Ye Kyaw Thu, Win Pa Pa, Masao Utiyama, Andrew Finch and Eiichiro Sumita (2016). Then, it was developed under ASEAN IVO as described in this Web page. The process of building ALT began with sampling about 20,000 sentences from English Wikinews, and then these sentences were translated into the other languages. ALT now has 13 languages: Bengali, English, Filipino, Hindi, Bahasa Indonesia, Japanese, Khmer, Lao, Malay, Myanmar (Burmese), Thai, Vietnamese, Chinese (Simplified Chinese).\n",
            "url": "https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "bn",
                "en",
                "fil",
                "hi",
                "id",
                "ja",
                "km",
                "lo",
                "ms",
                "my",
                "th",
                "vi",
                "zh"
            ],
            "categories": [
                "translation",
                "token-classification",
                "natural-language-processing"
            ],
            "tasks": [
                "parsing"
            ]
        }
    },
    {
        "id": "amazon_polarity",
        "data": {
            "description": "The Amazon reviews dataset consists of reviews from amazon.\nThe data span a period of 18 years, including ~35 million reviews up to March 2013.\nReviews include product and user information, ratings, and a plaintext review.\n",
            "url": "https://registry.opendata.aws/",
            "license": "Apache License 2.0",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "amazon_reviews_multi",
        "data": {
            "description": "We provide an Amazon product reviews dataset for multilingual text classification. The dataset contains reviews in English, Japanese, German, French, Chinese and Spanish, collected between November 1, 2015 and November 1, 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID and the coarse-grained product category (e.g. ‘books’, ‘appliances’, etc.) The corpus is balanced across stars, so each star rating constitutes 20% of the reviews in each language.\n\nFor each language, there are 200,000, 5,000 and 5,000 reviews in the training, development and test sets respectively. The maximum number of reviews per reviewer is 20 and the maximum number of reviews per product is 20. All reviews are truncated after 2,000 characters, and all reviews are at least 20 characters long.\n\nNote that the language of a review does not necessarily match the language of its marketplace (e.g. reviews from amazon.de are primarily written in German, but could also be written in English, etc.). For this reason, we applied a language detection algorithm based on the work in Bojanowski et al. (2017) to determine the language of the review text and we removed reviews that were not written in the expected language.\n",
            "url": "https://registry.opendata.aws/amazon-reviews-ml/",
            "license": "By accessing the Multilingual Amazon Reviews Corpus (\"Reviews Corpus\"), you agree that the Reviews Corpus is an Amazon Service subject to the Amazon.com Conditions of Use (https://www.amazon.com/gp/help/customer/display.html/ref=footer_cou?ie=UTF8&nodeId=508088) and you agree to be bound by them, with the following additional conditions:\n\nIn addition to the license rights granted under the Conditions of Use, Amazon or its content providers grant you a limited, non-exclusive, non-transferable, non-sublicensable, revocable license to access and use the Reviews Corpus for purposes of academic research. You may not resell, republish, or make any commercial use of the Reviews Corpus or its contents, including use of the Reviews Corpus for commercial research, such as research related to a funding or consultancy contract, internship, or other relationship in which the results are provided for a fee or delivered to a for-profit organization. You may not (a) link or associate content in the Reviews Corpus with any personal information (including Amazon customer accounts), or (b) attempt to determine the identity of the author of any content in the Reviews Corpus. If you violate any of the foregoing conditions, your license to access and use the Reviews Corpus will automatically terminate without prejudice to any of the other rights or remedies Amazon may have.\n",
            "givenLicense": "other",
            "language": [
                "de",
                "en",
                "es",
                "fr",
                "ja",
                "zh"
            ],
            "categories": [
                "summarization",
                "text-generation",
                "fill-mask",
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "language-modeling",
                "masked-language-modeling",
                "sentiment-classification",
                "sentiment-scoring",
                "topic-classification"
            ]
        }
    },
    {
        "id": "amazon_us_reviews",
        "data": {
            "description": "Amazon Customer Reviews (a.k.a. Product Reviews) is one of Amazons iconic products. In a period of over two decades since the first review in 1995, millions of Amazon customers have contributed over a hundred million reviews to express opinions and describe their experiences regarding products on the Amazon.com website. This makes Amazon Customer Reviews a rich source of information for academic researchers in the fields of Natural Language Processing (NLP), Information Retrieval (IR), and Machine Learning (ML), amongst others. Accordingly, we are releasing this data to further research in multiple disciplines related to understanding customer product experiences. Specifically, this dataset was constructed to represent a sample of customer evaluations and opinions, variation in the perception of a product across geographical regions, and promotional intent or bias in reviews.\n\nOver 130+ million customer reviews are available to researchers as part of this release. The data is available in TSV files in the amazon-reviews-pds S3 bucket in AWS US East Region. Each line in the data files corresponds to an individual review (tab delimited, with no quote and escape characters).\n\nEach Dataset contains the following columns:\n\n- marketplace: 2 letter country code of the marketplace where the review was written.\n- customer_id: Random identifier that can be used to aggregate reviews written by a single author.\n- review_id: The unique ID of the review.\n- product_id: The unique Product ID the review pertains to. In the multilingual dataset the reviews for the same product in different countries can be grouped by the same product_id.\n- product_parent: Random identifier that can be used to aggregate reviews for the same product.\n- product_title: Title of the product.\n- product_category: Broad product category that can be used to group reviews (also used to group the dataset into coherent parts).\n- star_rating: The 1-5 star rating of the review.\n- helpful_votes: Number of helpful votes.\n- total_votes: Number of total votes the review received.\n- vine: Review was written as part of the Vine program.\n- verified_purchase: The review is on a verified purchase.\n- review_headline: The title of the review.\n- review_body: The review text.\n- review_date: The date the review was written.\n",
            "url": "https://s3.amazonaws.com/amazon-reviews-pds/readme.html",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "ambig_qa",
        "data": {
            "description": "AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark. We find that over half of the questions in NQ-open are ambiguous. The types of ambiguity are diverse and sometimes subtle, many of which are only apparent after examining evidence provided by a very large text corpus.  AMBIGNQ, a dataset with\n14,042 annotations on NQ-OPEN questions containing diverse types of ambiguity.\nWe provide two distributions of our new dataset AmbigNQ: a full version with all annotation metadata and a light version with only inputs and outputs.\n",
            "url": "https://nlp.cs.washington.edu/ambigqa/",
            "license": "CC BY-SA 3.0",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "americas_nli",
        "data": {
            "description": "AmericasNLI is an extension of XNLI (Conneau et al., 2018) – a natural language inference (NLI) dataset covering 15 high-resource languages – to 10 low-resource indigenous languages spoken in the Americas: Ashaninka, Aymara, Bribri, Guarani, Nahuatl, Otomi, Quechua, Raramuri, Shipibo-Konibo, and Wixarika. As with MNLI, the goal is to predict textual entailment (does sentence A imply/contradict/neither sentence B) and is a classification task (given two sentences, predict one of three labels).\n",
            "url": "TODO",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ay",
                "bzd",
                "cni",
                "gn",
                "hch",
                "nah",
                "oto",
                "qu",
                "shp",
                "tar"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference"
            ]
        }
    },
    {
        "id": "ami",
        "data": {
            "description": "The AMI Meeting Corpus consists of 100 hours of meeting recordings. The recordings use a range of signals\nsynchronized to a common timeline. These include close-talking and far-field microphones, individual and\nroom-view video cameras, and output from a slide projector and an electronic whiteboard. During the meetings,\nthe participants also have unsynchronized pens available to them that record what is written. The meetings\nwere recorded in English using three different rooms with different acoustic properties, and include mostly\nnon-native speakers. \n\nFar field audio of single microphone. This configuration only includes audio belonging the first microphone, *i.e.* 1-1, of the microphone array.",
            "url": "https://groups.inf.ed.ac.uk/ami/corpus/",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "automatic-speech-recognition"
            ],
            "tasks": []
        }
    },
    {
        "id": "amttl",
        "data": {
            "description": "Chinese word segmentation (CWS) trained from open source corpus faces dramatic performance drop\nwhen dealing with domain text, especially for a domain with lots of special terms and diverse\nwriting styles, such as the biomedical domain. However, building domain-specific CWS requires\nextremely high annotation cost. In this paper, we propose an approach by exploiting domain-invariant\nknowledge from high resource to low resource domains. Extensive experiments show that our mode\nachieves consistently higher accuracy than the single-task CWS and other transfer learning\nbaselines, especially when there is a large disparity between source and target domains.\n\nThis dataset is the accompanied medical Chinese word segmentation (CWS) dataset.\nThe tags are in BIES scheme.\n\nFor more details see https://www.aclweb.org/anthology/C18-1307/\n",
            "url": "https://www.aclweb.org/anthology/C18-1307/",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "zh"
            ],
            "categories": [
                "token-classification",
                "natural-language-processing"
            ],
            "tasks": [
                "parsing"
            ]
        }
    },
    {
        "id": "anli",
        "data": {
            "description": "The Adversarial Natural Language Inference (ANLI) is a new large-scale NLI benchmark dataset, \nThe dataset is collected via an iterative, adversarial human-and-model-in-the-loop procedure.\nANLI is much more difficult than its predecessors including SNLI and MNLI.\nIt contains three rounds. Each round has train/dev/test splits.\n",
            "url": "https://github.com/facebookresearch/anli/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "app_reviews",
        "data": {
            "description": "It is a large dataset of Android applications belonging to 23 differentapps categories, which provides an overview of the types of feedback users report on the apps and documents the evolution of the related code metrics. The dataset contains about 395 applications of the F-Droid repository, including around 600 versions, 280,000 user reviews (extracted with specific text mining approaches)\n",
            "url": "https://giograno.me/assets/pdf/workshop/wama17.pdf",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "sentiment-scoring"
            ]
        }
    },
    {
        "id": "aquamuse",
        "data": {
            "description": "AQuaMuSe is a novel scalable approach to automatically mine dual query based multi-document summarization datasets for extractive and abstractive summaries using question answering dataset (Google Natural Questions) and large document corpora (Common Crawl)",
            "url": "https://github.com/google-research-datasets/aquamuse",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "other",
                "question-answering",
                "text2text-generation"
            ],
            "tasks": [
                "abstractive-qa",
                "extractive-qa",
                "query-based-multi-document-summarization"
            ]
        }
    },
    {
        "id": "aqua_rat",
        "data": {
            "description": "A large-scale dataset consisting of approximately 100,000 algebraic word problems. \nThe solution to each question is explained step-by-step using natural language. \nThis data is used to train a program generation model that learns to generate the explanation, \nwhile generating the program that solves the question.\n",
            "url": "https://github.com/deepmind/AQuA",
            "license": "Copyright 2017 Google Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "arabic_billion_words",
        "data": {
            "description": "Abu El-Khair Corpus is an Arabic text corpus, that includes more than five million newspaper articles.\nIt contains over a billion and a half words in total, out of which, there are about three million unique words.\nThe corpus is encoded with two types of encoding, namely: UTF-8, and Windows CP-1256.\nAlso it was marked with two mark-up languages, namely: SGML, and XML.\n",
            "url": "http://abuelkhair.net/index.php/en/arabic/abu-el-khair-corpus",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "arabic_pos_dialect",
        "data": {
            "description": "The Dialectal Arabic Datasets contain four dialects of Arabic, Etyptian (EGY), Levantine (LEV), Gulf (GLF), and Maghrebi (MGR). Each dataset consists of a set of 350 manually segmented and POS tagged tweets.\n",
            "url": "https://alt.qcri.org/resources/da_resources/",
            "license": "",
            "givenLicense": "apache-2.0",
            "language": [
                "ar"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "part-of-speech-tagging"
            ]
        }
    },
    {
        "id": "arabic_speech_corpus",
        "data": {
            "description": "This Speech corpus has been developed as part of PhD work carried out by Nawar Halabi at the University of Southampton.\nThe corpus was recorded in south Levantine Arabic\n(Damascian accent) using a professional studio. Synthesized speech as an output using this corpus has produced a high quality, natural voice.\nNote that in order to limit the required storage for preparing this dataset, the audio\nis stored in the .flac format and is not converted to a float32 array. To convert, the audio\nfile to a float32 array, please make use of the `.map()` function as follows:\n\n\n```python\nimport soundfile as sf\n\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n",
            "url": "http://en.arabicspeechcorpus.com/arabic-speech-corpus.zip",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "ar"
            ],
            "categories": [
                "automatic-speech-recognition"
            ],
            "tasks": []
        }
    },
    {
        "id": "arcd",
        "data": {
            "description": " Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions      posed by crowdworkers on Wikipedia articles.\n",
            "url": "https://github.com/husseinmozannar/SOQAL/tree/master/data",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "ar-SA"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "ar_cov19",
        "data": {
            "description": "ArCOV-19 is an Arabic COVID-19 Twitter dataset that covers the period from 27th of January till 30th of April 2020. ArCOV-19 is designed to enable research under several domains including natural language processing, information retrieval, and social computing, among others\n",
            "url": "https://gitlab.com/bigirqu/ArCOV-19",
            "license": "",
            "givenLicense": "",
            "language": [
                "ar"
            ],
            "categories": [
                "other",
                "text-mining"
            ],
            "tasks": [
                "data-mining"
            ]
        }
    },
    {
        "id": "ar_res_reviews",
        "data": {
            "description": "Dataset of 8364 restaurant reviews scrapped from qaym.com in Arabic for sentiment analysis\n",
            "url": "https://github.com/hadyelsahar/large-arabic-sentiment-analysis-resouces",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "ar_sarcasm",
        "data": {
            "description": "ArSarcasm is a new Arabic sarcasm detection dataset.\nThe dataset was created using previously available Arabic sentiment analysis datasets (SemEval 2017 and ASTD)\n and adds sarcasm and dialect labels to them. The dataset contains 10,547 tweets, 1,682 (16%) of which are sarcastic.\n",
            "url": "https://github.com/iabufarha/ArSarcasm",
            "license": "MIT",
            "givenLicense": "mit",
            "language": [
                "ar"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification",
                "sarcasm-detection"
            ]
        }
    },
    {
        "id": "arsentd_lev",
        "data": {
            "description": "\nThe Arabic Sentiment Twitter Dataset for Levantine dialect (ArSenTD-LEV) contains 4,000 tweets written in Arabic and equally retrieved from Jordan, Lebanon, Palestine and Syria.\n",
            "url": "http://oma-project.com/ArSenL/ArSenTD_Lev_Intro",
            "license": "",
            "givenLicense": "other",
            "language": [
                "apc",
                "apj"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification",
                "topic-classification"
            ]
        }
    },
    {
        "id": "art",
        "data": {
            "description": "the Abductive Natural Language Inference Dataset from AI2\n",
            "url": "https://leaderboard.allenai.org/anli/submissions/get-started",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "arxiv_dataset",
        "data": {
            "description": "A dataset of 1.7 million arXiv articles for applications like trend analysis, paper recommender engines, category prediction, co-citation networks, knowledge graph construction and semantic search interfaces.\n",
            "url": "https://www.kaggle.com/Cornell-University/arxiv",
            "license": "",
            "givenLicense": "cc0-1.0",
            "language": [
                "en"
            ],
            "categories": [
                "translation",
                "summarization",
                "text-retrieval"
            ],
            "tasks": [
                "document-retrieval",
                "entity-linking-retrieval",
                "explanation-generation",
                "fact-checking-retrieval",
                "text-simplification"
            ]
        }
    },
    {
        "id": "ascent_kb",
        "data": {
            "description": "This dataset contains 8.9M commonsense assertions extracted by the Ascent pipeline (https://ascent.mpi-inf.mpg.de/).\n",
            "url": "https://ascent.mpi-inf.mpg.de/",
            "license": "The Creative Commons Attribution 4.0 International License. https://creativecommons.org/licenses/by/4.0/",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "other",
                "conversational"
            ],
            "tasks": [
                "knowledge-base"
            ]
        }
    },
    {
        "id": "aslg_pc12",
        "data": {
            "description": "Synthetic English-ASL Gloss Parallel Corpus 2012\n",
            "url": "https://achrafothman.net/site/asl-smt/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "asnq",
        "data": {
            "description": "ASNQ is a dataset for answer sentence selection derived from\nGoogle's Natural Questions (NQ) dataset (Kwiatkowski et al. 2019).\n\nEach example contains a question, candidate sentence, label indicating whether or not\nthe sentence answers the question, and two additional features -- \nsentence_in_long_answer and short_answer_in_sentence indicating whether ot not the \ncandidate sentence is contained in the long_answer and if the short_answer is in the candidate sentence.\n\nFor more details please see \nhttps://arxiv.org/pdf/1911.04118.pdf\n\nand \n\nhttps://research.google/pubs/pub47761/\n",
            "url": "https://github.com/alexa/wqa_tanda#answer-sentence-natural-questions-asnq",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "asset",
        "data": {
            "description": "ASSET is a dataset for evaluating Sentence Simplification systems with multiple rewriting transformations,\nas described in \"ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations\".\nThe corpus is composed of 2000 validation and 359 test original sentences that were each simplified 10 times by different annotators.\nThe corpus also contains human judgments of meaning preservation, fluency and simplicity for the outputs of several automatic text simplification systems.\n",
            "url": "https://github.com/facebookresearch/asset",
            "license": "Creative Common Attribution-NonCommercial 4.0 International",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification",
                "text2text-generation"
            ],
            "tasks": [
                "simplification-evaluation",
                "text-simplification"
            ]
        }
    },
    {
        "id": "assin",
        "data": {
            "description": "\nThe ASSIN (Avaliação de Similaridade Semântica e INferência textual) corpus is a corpus annotated with pairs of sentences written in \nPortuguese that is suitable for the  exploration of textual entailment and paraphrasing classifiers. The corpus contains pairs of sentences \nextracted from news articles written in European Portuguese (EP) and Brazilian Portuguese (BP), obtained from Google News Portugal \nand Brazil, respectively. To create the corpus, the authors started by collecting a set of news articles describing the \nsame event (one news article from Google News Portugal and another from Google News Brazil) from Google News. \nThen, they employed Latent Dirichlet Allocation (LDA) models to retrieve pairs of similar sentences between sets of news \narticles that were grouped together around the same topic. For that, two LDA models were trained (for EP and for BP) \non external and large-scale collections of unannotated news articles from Portuguese and Brazilian news providers, respectively. \nThen, the authors defined a lower and upper threshold for the sentence similarity score of the retrieved pairs of sentences, \ntaking into account that high similarity scores correspond to sentences that contain almost the same content (paraphrase candidates), \nand low similarity scores correspond to sentences that are very different in content from each other (no-relation candidates).\nFrom the collection of pairs of sentences obtained at this stage, the authors performed some manual grammatical corrections \nand discarded some of the pairs wrongly retrieved. Furthermore, from a preliminary analysis made to the retrieved sentence pairs \nthe authors noticed that the number of contradictions retrieved during the previous stage was very low. Additionally, they also \nnoticed that event though paraphrases are not very frequent, they occur with some frequency in news articles. Consequently, \nin contrast with the majority of the currently available corpora for other languages, which consider as labels “neutral”, “entailment” \nand “contradiction” for the task of RTE, the authors of the ASSIN corpus decided to use as labels “none”, “entailment” and “paraphrase”.\nFinally, the manual annotation of pairs of sentences was performed by human annotators. At least four annotators were randomly \nselected to annotate each pair of sentences, which is done in two steps: (i) assigning a semantic similarity label (a score between 1 and 5, \nfrom unrelated to very similar); and (ii) providing an entailment label (one sentence entails the other, sentences are paraphrases, \nor no relation). Sentence pairs where at least three annotators do not agree on the entailment label were considered controversial \nand thus discarded from the gold standard annotations. The full dataset has 10,000 sentence pairs, half of which in Brazilian Portuguese \nand half in European Portuguese. Either language variant has 2,500 pairs for training, 500 for validation and 2,000 for testing.\n",
            "url": "http://nilc.icmc.usp.br/assin/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "pt"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "natural-language-inference",
                "semantic-similarity-scoring"
            ]
        }
    },
    {
        "id": "assin2",
        "data": {
            "description": "\nThe ASSIN 2 corpus is composed of rather simple sentences. Following the procedures of SemEval 2014 Task 1.\nThe training and validation data are composed, respectively, of 6,500 and 500 sentence pairs in Brazilian Portuguese,\nannotated for entailment and semantic similarity. Semantic similarity values range from 1 to 5, and text entailment\nclasses are either entailment or none. The test data are composed of approximately 3,000 sentence pairs with the same\nannotation. All data were manually annotated.\n",
            "url": "https://sites.google.com/view/assin2",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "pt"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "natural-language-inference",
                "semantic-similarity-scoring"
            ]
        }
    },
    {
        "id": "atomic",
        "data": {
            "description": "This dataset provides the template sentences and\nrelationships defined in the ATOMIC common sense dataset. There are\nthree splits - train, test, and dev.\n\nFrom the authors.\n\nDisclaimer/Content warning: the events in atomic have been\nautomatically extracted from blogs, stories and books written at\nvarious times. The events might depict violent or problematic actions,\nwhich we left in the corpus for the sake of learning the (probably\nnegative but still important) commonsense implications associated with\nthe events. We removed a small set of truly out-dated events, but\nmight have missed some so please email us (msap@cs.washington.edu) if\nyou have any concerns.\n\n",
            "url": "https://homes.cs.washington.edu/~msap/atomic/",
            "license": "The Creative Commons Attribution 4.0 International License. https://creativecommons.org/licenses/by/4.0/",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-common-sense-if-then-reasoning"
            ]
        }
    },
    {
        "id": "autshumato",
        "data": {
            "description": "Multilingual information access is stipulated in the South African constitution. In practise, this\nis hampered by a lack of resources and capacity to perform the large volumes of translation\nwork required to realise multilingual information access. One of the aims of the Autshumato\nproject is to develop machine translation systems for three South African languages pairs.\n",
            "url": "https://repo.sadilar.org/handle/20.500.12185/7/discover?filtertype=database&filter_relational_operator=equals&filter=Multilingual+Text+Corpora%3A+Aligned",
            "license": "",
            "givenLicense": "cc-by-2.5",
            "language": [
                "en",
                "tn",
                "ts",
                "zu"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "babi_qa",
        "data": {
            "description": "The (20) QA bAbI tasks are a set of proxy tasks that evaluate reading\ncomprehension via question answering. Our tasks measure understanding\nin several ways: whether a system is able to answer questions via chaining facts,\nsimple induction, deduction and many more. The tasks are designed to be prerequisites\nfor any system that aims to be capable of conversing with a human.\nThe aim is to classify these tasks into skill sets,so that researchers\ncan identify (and then rectify)the failings of their systems.\n",
            "url": "https://research.fb.com/downloads/babi/",
            "license": "Creative Commons Attribution 3.0 License",
            "givenLicense": "cc-by-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "question-answering-other-chained-qa"
            ]
        }
    },
    {
        "id": "banking77",
        "data": {
            "description": "BANKING77 dataset provides a very fine-grained set of intents in a banking domain.\nIt comprises 13,083 customer service queries labeled with 77 intents.\nIt focuses on fine-grained single-domain intent detection.\n",
            "url": "https://github.com/PolyAI-LDN/task-specific-datasets",
            "license": "Creative Commons Attribution 4.0 International",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification",
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "bbaw_egyptian",
        "data": {
            "description": "The project `Strukturen und Transformationen des Wortschatzes der ägyptischen Sprache`\nis compiling an extensively annotated digital corpus of Egyptian texts.\nThis publication comprises an excerpt of the internal database's contents.\n",
            "url": "https://aaew.bbaw.de/tla/index.html",
            "license": "Creative Commons-Lizenz - CC BY-SA - 4.0 International",
            "givenLicense": "cc-by-4.0",
            "language": [
                "de",
                "en",
                "egy"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "bbc_hindi_nli",
        "data": {
            "description": "This dataset is used to train models for Natural Language Inference Tasks in Low-Resource Languages like Hindi.\n",
            "url": "https://github.com/avinsit123/hindi-nli-data",
            "license": "\nMIT License\n\nCopyright (c) 2019 MIDAS, IIIT Delhi\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
            "givenLicense": "mit",
            "language": [
                "hi"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference"
            ]
        }
    },
    {
        "id": "bc2gm_corpus",
        "data": {
            "description": "Nineteen teams presented results for the Gene Mention Task at the BioCreative II Workshop. \nIn this task participants designed systems to identify substrings in sentences corresponding to gene name mentions. \nA variety of different methods were used and the results varied with a highest achieved F1 score of 0.8721. \nHere we present brief descriptions of all the methods used and a statistical analysis of the results. \nWe also demonstrate that, by combining the results from all submissions, an F score of 0.9066 is feasible, \nand furthermore that the best result makes use of the lowest scoring submissions.\n\nFor more details, see: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2559986/\n\nThe original dataset can be downloaded from: https://biocreative.bioinformatics.udel.edu/resources/corpora/biocreative-ii-corpus/\nThis dataset has been converted to CoNLL format for NER using the following tool: https://github.com/spyysalo/standoff2conll\n",
            "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2559986/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "beans",
        "data": {
            "description": "Beans is a dataset of images of beans taken in the field using smartphone\ncameras. It consists of 3 classes: 2 disease classes and the healthy class.\nDiseases depicted include Angular Leaf Spot and Bean Rust. Data was annotated\nby experts from the National Crops Resources Research Institute (NaCRRI) in\nUganda and collected by the Makerere AI research lab.\n",
            "url": "https://github.com/AI-Lab-Makerere/ibean/",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification"
            ],
            "tasks": [
                "multi-class-image-classification"
            ]
        }
    },
    {
        "id": "best2009",
        "data": {
            "description": "`best2009` is a Thai word-tokenization dataset from encyclopedia, novels, news and articles by\n[NECTEC](https://www.nectec.or.th/) (148,995/2,252 lines of train/test). It was created for\n[BEST 2010: Word Tokenization Competition](https://thailang.nectec.or.th/archive/indexa290.html?q=node/10).\nThe test set answers are not provided publicly.\n",
            "url": "https://aiforthai.in.th/",
            "license": "CC-BY-NC-SA 3.0",
            "givenLicense": "cc-by-nc-sa-3.0",
            "language": [
                "th"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "token-classification-other-word-tokenization"
            ]
        }
    },
    {
        "id": "bianet",
        "data": {
            "description": "A parallel news corpus in Turkish, Kurdish and English.\nBianet collects 3,214 Turkish articles with their sentence-aligned Kurdish or English translations from the Bianet online newspaper.\n3 languages, 3 bitexts\ntotal number of files: 6\ntotal number of tokens: 2.25M\ntotal number of sentence fragments: 0.14M\n",
            "url": "http://opus.nlpl.eu/Bianet.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "ku",
                "tr"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "bible_para",
        "data": {
            "description": "This is a multilingual parallel corpus created from translations of the Bible compiled by Christos Christodoulopoulos and Mark Steedman.\n\n102 languages, 5,148 bitexts\ntotal number of files: 107\ntotal number of tokens: 56.43M\ntotal number of sentence fragments: 2.84M\n",
            "url": "http://opus.nlpl.eu/bible-uedin.php",
            "license": "",
            "givenLicense": "cc0-1.0",
            "language": [
                "acu",
                "af",
                "agr",
                "ake",
                "am",
                "amu",
                "ar",
                "bg",
                "bsn",
                "cak",
                "ceb",
                "ch",
                "chq",
                "chr",
                "cjp",
                "cni",
                "cop",
                "crp",
                "cs",
                "da",
                "de",
                "dik",
                "dje",
                "djk",
                "dop",
                "ee",
                "el",
                "en",
                "eo",
                "es",
                "et",
                "eu",
                "fi",
                "fr",
                "gbi",
                "gd",
                "gu",
                "gv",
                "he",
                "hi",
                "hr",
                "hu",
                "hy",
                "id",
                "is",
                "it",
                "jak",
                "jap",
                "jiv",
                "kab",
                "kbh",
                "kek",
                "kn",
                "ko",
                "la",
                "lt",
                "lv",
                "mam",
                "mi",
                "ml",
                "mr",
                "my",
                "ne",
                "nhg",
                "nl",
                "no",
                "ojb",
                "pck",
                "pes",
                "pl",
                "plt",
                "pot",
                "ppk",
                "pt",
                "quc",
                "quw",
                "ro",
                "rom",
                "ru",
                "shi",
                "sk",
                "sl",
                "sn",
                "so",
                "sq",
                "sr",
                "ss",
                "sv",
                "syr",
                "te",
                "th",
                "tl",
                "tmh",
                "tr",
                "uk",
                "usp",
                "vi",
                "wal",
                "wo",
                "xh",
                "zh",
                "zu"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "bigbench",
        "data": {
            "description": "The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to\nprobe large language models, and extrapolate their future capabilities.\n",
            "url": "https://github.com/google/BIG-bench",
            "license": "Apache License 2.0",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "multiple-choice",
                "question-answering",
                "text-classification",
                "text-generation",
                "zero-shot-classification",
                "other"
            ],
            "tasks": [
                "multiple-choice-qa",
                "extractive-qa",
                "open-domain-qa",
                "closed-domain-qa",
                "fact-checking",
                "acceptability-classification",
                "intent-classification",
                "multi-class-classification",
                "multi-label-classification",
                "text-scoring",
                "hate-speech-detection",
                "language-modeling"
            ]
        }
    },
    {
        "id": "big_patent",
        "data": {
            "description": "\nBIGPATENT, consisting of 1.3 million records of U.S. patent documents\nalong with human written abstractive summaries.\nEach US patent application is filed under a Cooperative Patent Classification\n(CPC) code. There are nine such classification categories:\nA (Human Necessities), B (Performing Operations; Transporting),\nC (Chemistry; Metallurgy), D (Textiles; Paper), E (Fixed Constructions),\nF (Mechanical Engineering; Lightning; Heating; Weapons; Blasting),\nG (Physics), H (Electricity), and\nY (General tagging of new or cross-sectional technology)\nThere are two features:\n  - description: detailed description of patent.\n  - abstract: Patent abastract.\n",
            "url": "https://evasharma.github.io/bigpatent/",
            "license": "Creative Commons Attribution 4.0 International",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "patent-summarization"
            ]
        }
    },
    {
        "id": "billsum",
        "data": {
            "description": "\nBillSum, summarization of US Congressional and California state bills.\n\nThere are several features:\n  - text: bill text.\n  - summary: summary of the bills.\n  - title: title of the bills.\nfeatures for us bills. ca bills does not have.\n  - text_len: number of chars in text.\n  - sum_len: number of chars in summary.\n",
            "url": "https://github.com/FiscalNote/BillSum",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "bills-summarization"
            ]
        }
    },
    {
        "id": "bing_coronavirus_query_set",
        "data": {
            "description": "This dataset was curated from the Bing search logs (desktop users only) over the period of Jan 1st, 2020 – (Current Month - 1). Only searches that were issued many times by multiple users were included. The dataset includes queries from all over the world that had an intent related to the Coronavirus or Covid-19. In some cases this intent is explicit in the query itself (e.g., “Coronavirus updates Seattle”), in other cases it is implicit , e.g. “Shelter in place”. The implicit intent of search queries (e.g., “Toilet paper”) was extracted using random walks on the click graph as outlined in this paper by Microsoft Research. All personal data were removed.\n",
            "url": "https://github.com/microsoft/BingCoronavirusQuerySet",
            "license": "",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification"
            ]
        }
    },
    {
        "id": "biomrc",
        "data": {
            "description": "We introduce BIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.\n",
            "url": "http://nlp.cs.aueb.gr/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "biosses",
        "data": {
            "description": "BIOSSES is a benchmark dataset for biomedical sentence similarity estimation. The dataset comprises 100 sentence pairs, in which each sentence was selected from the TAC (Text Analysis Conference) Biomedical Summarization Track Training Dataset containing articles from the biomedical domain. The sentence pairs were evaluated by five different human experts that judged their similarity and gave scores ranging from 0 (no relation) to 4 (equivalent).\n",
            "url": "https://tabilab.cmpe.boun.edu.tr/BIOSSES/DataSet.html",
            "license": "BIOSSES is made available under the terms of The GNU Common Public License v.3.0.\n",
            "givenLicense": "gpl-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "semantic-similarity-scoring"
            ]
        }
    },
    {
        "id": "biwi_kinect_head_pose",
        "data": {
            "description": "The Biwi Kinect Head Pose Database is acquired with the Microsoft Kinect sensor, a structured IR light device.It contains 15K images of 20 people with 6 females and 14 males where 4 people were recorded twice.\n",
            "url": "https://icu.ee.ethz.ch/research/datsets.html",
            "license": "This database is made available for non-commercial use such as university research and education.",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "other",
                "computer-vision"
            ],
            "tasks": [
                "head-pose-estimation"
            ]
        }
    },
    {
        "id": "blbooks",
        "data": {
            "description": "A dataset comprising of text created by OCR from the 49,455 digitised books, equating to 65,227 volumes (25+ million pages), published between c. 1510 - c. 1900.\nThe books cover a wide range of subject areas including philosophy, history, poetry and literature.\n",
            "url": "https://www.bl.uk/collection-guides/digitised-printed-books",
            "license": "",
            "givenLicense": "cc0-1.0",
            "language": [
                "en",
                "fr",
                "de",
                "es",
                "it",
                "nl"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "other"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling",
                "other-other-digital-humanities-research"
            ]
        }
    },
    {
        "id": "blbooksgenre",
        "data": {
            "description": "This dataset contains metadata for resources belonging to the British Library’s digitised printed books (18th-19th century) collection (bl.uk/collection-guides/digitised-printed-books).\nThis metadata has been extracted from British Library catalogue records.\nThe metadata held within our main catalogue is updated regularly.\nThis metadata dataset should be considered a snapshot of this metadata.\n",
            "url": "doi.org/10.23636/BKHQ-0312",
            "license": "CC0 1.0 Universal Public Domain",
            "givenLicense": "cc0-1.0",
            "language": [
                "en",
                "de",
                "fr",
                "nl"
            ],
            "categories": [
                "text-classification",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "topic-classification",
                "multi-label-classification",
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "blended_skill_talk",
        "data": {
            "description": "A dataset of 7k conversations explicitly designed to exhibit multiple conversation modes: displaying personality, having empathy, and demonstrating knowledge.\n",
            "url": "https://parl.ai/projects/bst/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "conversational"
            ],
            "tasks": [
                "dialogue-generation"
            ]
        }
    },
    {
        "id": "blimp",
        "data": {
            "description": "\nBLiMP is a challenge set for evaluating what language models (LMs) know about\nmajor grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each\ncontaining 1000 minimal pairs isolating specific contrasts in syntax,\nmorphology, or semantics. The data is automatically generated according to\nexpert-crafted grammars.\n",
            "url": "https://github.com/alexwarstadt/blimp/tree/master/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "acceptability-classification"
            ]
        }
    },
    {
        "id": "blog_authorship_corpus",
        "data": {
            "description": "The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.\n\nEach blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)\n\nAll bloggers included in the corpus fall into one of three age groups:\n- 8240 \"10s\" blogs (ages 13-17),\n- 8086 \"20s\" blogs (ages 23-27)\n- 2994 \"30s\" blogs (ages 33-47).\n\nFor each age group there are an equal number of male and female bloggers.\n\nEach blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label urllink.\n\nThe corpus may be freely used for non-commercial research purposes.\n",
            "url": "https://lingcog.blogspot.com/p/datasets.html",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "bn_hate_speech",
        "data": {
            "description": "The Bengali Hate Speech Dataset is a collection of Bengali articles collected from Bengali news articles,\nnews dump of Bengali TV channels, books, blogs, and social media. Emphasis was placed on Facebook pages and\nnewspaper sources because they attract close to 50 million followers and is a common source of opinions\nand hate speech. The raw text corpus contains 250 million articles and the full dataset is being prepared\nfor release. This is a subset of the full dataset.\n\nThis dataset was prepared for hate-speech text classification benchmark on Bengali, an under-resourced language.\n",
            "url": "https://github.com/rezacsedu/Bengali-Hate-Speech-Dataset",
            "license": "MIT License",
            "givenLicense": "mit",
            "language": [
                "bn"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-hate-speech-topic-classification"
            ]
        }
    },
    {
        "id": "bnl_newspapers",
        "data": {
            "description": "Digitised historic newspapers from the Bibliothèque nationale (BnL) - the National Library of Luxembourg.\n",
            "url": "https://data.bnl.lu/data/historical-newspapers/",
            "license": "CC0",
            "givenLicense": "cc0-1.0",
            "language": [
                "ar",
                "da",
                "de",
                "fi",
                "fr",
                "lb",
                "nl",
                "pt"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "bookcorpus",
        "data": {
            "description": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story.This work aims to align books to their movie releases in order to providerich descriptive explanations for visual content that go semantically farbeyond the captions available in current datasets. ",
            "url": "https://yknzhu.wixsite.com/mbweb",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "bookcorpusopen",
        "data": {
            "description": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story.\nThis version of bookcorpus has 17868 dataset items (books). Each item contains two fields: title and text. The title is the name of the book (just the file name) while text contains unprocessed book text. The bookcorpus has been prepared by Shawn Presser and is generously hosted by The-Eye. The-Eye is a non-profit, community driven platform dedicated to the archiving and long-term preservation of any and all data including but by no means limited to... websites, books, games, software, video, audio, other digital-obscura and ideas.\n",
            "url": "https://github.com/soskek/bookcorpus/issues/27",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "boolq",
        "data": {
            "description": "BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally\noccurring ---they are generated in unprompted and unconstrained settings.\nEach example is a triplet of (question, passage, answer), with the title of the page as optional additional context.\nThe text-pair classification setup is similar to existing natural language inference tasks.\n",
            "url": "https://github.com/google-research-datasets/boolean-questions",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "bprec",
        "data": {
            "description": "Dataset consisting of Polish language texts annotated to recognize brand-product relations.\n",
            "url": "https://clarin-pl.eu/dspace/handle/11321/736",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "pl"
            ],
            "categories": [
                "text-retrieval"
            ],
            "tasks": [
                "entity-linking-retrieval"
            ]
        }
    },
    {
        "id": "break_data",
        "data": {
            "description": "Break is a human annotated dataset of natural language questions and their Question Decomposition Meaning Representations\n(QDMRs). Break consists of 83,978 examples sampled from 10 question answering datasets over text, images and databases. \nThis repository contains the Break dataset along with information on the exact data format.\n",
            "url": "https://github.com/allenai/Break",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "brwac",
        "data": {
            "description": "\nThe BrWaC (Brazilian Portuguese Web as Corpus) is a large corpus constructed following the Wacky framework,\nwhich was made public for research purposes. The current corpus version, released in January 2017, is composed by\n3.53 million documents, 2.68 billion tokens and 5.79 million types. Please note that this resource is available\nsolely for academic research purposes, and you agreed not to use it for any commercial applications.\nManually download at https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC\n",
            "url": "https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "pt"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "bsd_ja_en",
        "data": {
            "description": "This is the Business Scene Dialogue (BSD) dataset,\na Japanese-English parallel corpus containing written conversations\nin various business scenarios.\n\nThe dataset was constructed in 3 steps:\n  1) selecting business scenes,\n  2) writing monolingual conversation scenarios according to the selected scenes, and\n  3) translating the scenarios into the other language.\n\nHalf of the monolingual scenarios were written in Japanese\nand the other half were written in English.\n\nFields:\n- id: dialogue identifier\n- no: sentence pair number within a dialogue\n- en_speaker: speaker name in English\n- ja_speaker: speaker name in Japanese\n- en_sentence: sentence in English\n- ja_sentence: sentence in Japanese\n- original_language: language in which monolingual scenario was written\n- tag: scenario\n- title: scenario title\n",
            "url": "https://github.com/tsuruoka-lab/BSD",
            "license": "CC BY-NC-SA 4.0",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "en",
                "ja"
            ],
            "categories": [
                "translation"
            ],
            "tasks": [
                "translation-other-business-conversations-translation"
            ]
        }
    },
    {
        "id": "bswac",
        "data": {
            "description": "The Bosnian web corpus bsWaC was built by crawling the .ba top-level domain in 2014. The corpus was near-deduplicated on paragraph level, normalised via diacritic restoration, morphosyntactically annotated and lemmatised. The corpus is shuffled by paragraphs. Each paragraph contains metadata on the URL, domain and language identification (Bosnian vs. Croatian vs. Serbian).\n\nVersion 1.0 of this corpus is described in http://www.aclweb.org/anthology/W14-0405. Version 1.1 contains newer and better linguistic annotations.\n",
            "url": "http://nlp.ffzg.hr/resources/corpora/bswac/",
            "license": "CC BY-SA 4.0",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "bs"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "c3",
        "data": {
            "description": "Machine reading comprehension tasks require a machine reader to answer questions relevant to the given document. In this paper, we present the first free-form multiple-Choice Chinese machine reading Comprehension dataset (C^3), containing 13,369 documents (dialogues or more formally written mixed-genre texts) and their associated 19,577 multiple-choice free-form questions collected from Chinese-as-a-second-language examinations.\nWe present a comprehensive analysis of the prior knowledge (i.e., linguistic, domain-specific, and general world knowledge) needed for these real-world problems. We implement rule-based and popular neural methods and find that there is still a significant performance gap between the best performing model (68.5%) and human readers (96.0%), especially on problems that require prior knowledge. We further study the effects of distractor plausibility and data augmentation based on translated relevant datasets for English on model performance. We expect C^3 to present great challenges to existing systems as answering 86.8% of questions requires both knowledge within and beyond the accompanying document, and we hope that C^3 can serve as a platform to study how to leverage various kinds of prior knowledge to better understand a given written or orally oriented text.\n",
            "url": "https://github.com/nlpdata/c3",
            "license": "",
            "givenLicense": "other",
            "language": [
                "zh"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "c4",
        "data": {
            "description": "A colossal, cleaned version of Common Crawl's web crawl corpus.\n\nBased on Common Crawl dataset: \"https://commoncrawl.org\".\n\nThis is the processed version of Google's C4 dataset by AllenAI.\n\n",
            "url": "https://github.com/allenai/allennlp/discussions/5056",
            "license": "",
            "givenLicense": "odc-by",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "cail2018",
        "data": {
            "description": "In this paper, we introduce Chinese AI and Law challenge dataset (CAIL2018),\nthe first large-scale Chinese legal dataset for judgment prediction. CAIL contains more than 2.6 million\ncriminal cases published by the Supreme People's Court of China, which are several times larger than other\ndatasets in existing works on judgment prediction. Moreover, the annotations of judgment results are more\ndetailed and rich. It consists of applicable law articles, charges, and prison terms, which are expected\nto be inferred according to the fact descriptions of cases. For comparison, we implement several conventional\ntext classification baselines for judgment prediction and experimental results show that it is still a\nchallenge for current models to predict the judgment results of legal cases, especially on prison terms.\nTo help the researchers make improvements on legal judgment prediction.\n",
            "url": "https://arxiv.org/abs/1807.02478",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "zh"
            ],
            "categories": [
                "other"
            ],
            "tasks": [
                "other-other-judgement-prediction"
            ]
        }
    },
    {
        "id": "caner",
        "data": {
            "description": "Classical Arabic Named Entity Recognition corpus as a new corpus of tagged data that can be useful for handling the issues in recognition of Arabic named entities.\n",
            "url": "https://github.com/RamziSalah/Classical-Arabic-Named-Entity-Recognition-Corpus",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "capes",
        "data": {
            "description": "A parallel corpus of theses and dissertations abstracts in English and Portuguese were collected from the CAPES website (Coordenação de Aperfeiçoamento de Pessoal de Nível Superior) - Brazil. The corpus is sentence aligned for all language pairs. Approximately 240,000 documents were collected and aligned using the Hunalign algorithm.\n",
            "url": "https://sites.google.com/view/felipe-soares/datasets#h.p_kxOR6EhHm2a6",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "pt"
            ],
            "categories": [
                "translation"
            ],
            "tasks": [
                "translaiton-other-theses-translation",
                "translaiton-other-dissertation-abstracts-translation"
            ]
        }
    },
    {
        "id": "casino",
        "data": {
            "description": "We provide a novel dataset (referred to as CaSiNo) of 1030 negotiation dialogues. Two participants take the role of campsite neighbors and negotiate for Food, Water, and Firewood packages, based on their individual preferences and requirements. This design keeps the task tractable, while still facilitating linguistically rich and personal conversations. This helps to overcome the limitations of prior negotiation datasets such as Deal or No Deal and Craigslist Bargain. Each dialogue consists of rich meta-data including participant demographics, personality, and their subjective evaluation of the negotiation in terms of satisfaction and opponent likeness.\n",
            "url": "https://github.com/kushalchawla/CaSiNo",
            "license": "The project is licensed under CC-BY-4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "catalonia_independence",
        "data": {
            "description": "This dataset contains two corpora in Spanish and Catalan that consist of annotated Twitter messages for automatic stance detection. The data was collected over 12 days during February and March of 2019 from tweets posted in Barcelona, and during September of 2018 from tweets posted in the town of Terrassa, Catalonia.\n\nEach corpus is annotated with three classes: AGAINST, FAVOR and NEUTRAL, which express the stance towards the target - independence of Catalonia.\n",
            "url": "https://github.com/ixa-ehu/catalonia-independence-corpus",
            "license": "CC BY-NC-SA 4.0",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "ca",
                "es"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-stance-detection"
            ]
        }
    },
    {
        "id": "cats_vs_dogs",
        "data": {
            "description": "A large set of images of cats and dogs. There are 1738 corrupted images that are dropped.",
            "url": "https://www.microsoft.com/en-us/download/details.aspx?id=54765",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification"
            ],
            "tasks": [
                "multi-class-image-classification"
            ]
        }
    },
    {
        "id": "cawac",
        "data": {
            "description": "caWaC is a 780-million-token web corpus of Catalan built from the .cat top-level-domain in late 2013. \n",
            "url": "http://nlp.ffzg.hr/resources/corpora/cawac/",
            "license": "CC BY-SA 3.0",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "ca"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "cbt",
        "data": {
            "description": "The Children’s Book Test (CBT) is designed to measure directly\nhow well language models can exploit wider linguistic context.\nThe CBT is built from books that are freely available.\n",
            "url": "https://research.fb.com/downloads/babi/",
            "license": "GNU Free Documentation License v1.3",
            "givenLicense": "gfdl",
            "language": [
                "en"
            ],
            "categories": [
                "other",
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa",
                "other-other-raw-dataset"
            ]
        }
    },
    {
        "id": "cc100",
        "data": {
            "description": "This corpus is an attempt to recreate the dataset used for training XLM-R. This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages (indicated by *_rom). This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Each file comprises of documents separated by double-newlines and paragraphs within the same document separated by a newline. The data is generated using the open source CC-Net repository. No claims of intellectual property are made on the work of preparation of the corpus.\n",
            "url": "https://data.statmt.org/cc-100/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "af",
                "am",
                "ar",
                "as",
                "az",
                "be",
                "bg",
                "bn",
                "bn-Latn",
                "br",
                "bs",
                "ca",
                "cs",
                "cy",
                "da",
                "de",
                "el",
                "en",
                "eo",
                "es",
                "et",
                "eu",
                "fa",
                "ff",
                "fi",
                "fr",
                "fy",
                "ga",
                "gd",
                "gl",
                "gn",
                "gu",
                "ha",
                "he",
                "hi",
                "hi-Latn",
                "hr",
                "ht",
                "hu",
                "hy",
                "id",
                "ig",
                "is",
                "it",
                "ja",
                "jv",
                "ka",
                "kk",
                "km",
                "kn",
                "ko",
                "ku",
                "ky",
                "la",
                "lg",
                "li",
                "ln",
                "lo",
                "lt",
                "lv",
                "mg",
                "mk",
                "ml",
                "mn",
                "mr",
                "ms",
                "my",
                "my-x-zawgyi",
                "ne",
                "nl",
                "no",
                "ns",
                "om",
                "or",
                "pa",
                "pl",
                "ps",
                "pt",
                "qu",
                "rm",
                "ro",
                "ru",
                "sa",
                "si",
                "sc",
                "sd",
                "sk",
                "sl",
                "so",
                "sq",
                "sr",
                "ss",
                "su",
                "sv",
                "sw",
                "ta",
                "ta-Latn",
                "te",
                "te-Latn",
                "th",
                "tl",
                "tn",
                "tr",
                "ug",
                "uk",
                "ur",
                "ur-Latn",
                "uz",
                "vi",
                "wo",
                "xh",
                "yi",
                "yo",
                "zh-Hans",
                "zh-Hant",
                "zu"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "ccaligned_multilingual",
        "data": {
            "description": "CCAligned consists of parallel or comparable web-document pairs in 137 languages aligned with English. These web-document pairs were constructed by performing language identification on raw web-documents, and ensuring corresponding language codes were corresponding in the URLs of web documents. This pattern matching approach yielded more than 100 million aligned documents paired with English. Recognizing that each English document was often aligned to mulitple documents in different target language, we can join on English documents to obtain aligned documents that directly pair two non-English documents (e.g., Arabic-French).\n",
            "url": "https://data.statmt.org/cc-aligned/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "af",
                "ak",
                "am",
                "ar",
                "as",
                "ay",
                "az",
                "be",
                "bg",
                "bm",
                "bn",
                "br",
                "bs",
                "ca",
                "ckb",
                "cs",
                "ceb",
                "cy",
                "de",
                "dv",
                "el",
                "eo",
                "es",
                "fa",
                "ff",
                "fi",
                "fo",
                "fr",
                "fy",
                "ga",
                "gl",
                "gn",
                "gu",
                "he",
                "hi",
                "hr",
                "hu",
                "id",
                "ig",
                "is",
                "it",
                "iu",
                "ja",
                "ka",
                "kg",
                "kk",
                "km",
                "kn",
                "ko",
                "ku",
                "ky",
                "la",
                "lg",
                "li",
                "ln",
                "lo",
                "lt",
                "lv",
                "mg",
                "mi",
                "mk",
                "ml",
                "mn",
                "mr",
                "ms",
                "mt",
                "my",
                "my",
                "ne",
                "nl",
                "no",
                "nso",
                "ny",
                "om",
                "or",
                "pa",
                "pl",
                "ps",
                "pt",
                "shn",
                "kac",
                "rm",
                "ro",
                "ru",
                "rw",
                "sc",
                "sd",
                "se",
                "si",
                "sk",
                "sl",
                "sn",
                "so",
                "sq",
                "sr",
                "ss",
                "st",
                "su",
                "sv",
                "sw",
                "syc",
                "szl",
                "ta",
                "te",
                "tg",
                "th",
                "ti",
                "tl",
                "tn",
                "tr",
                "ts",
                "tt",
                "zgh",
                "ug",
                "uk",
                "ur",
                "uz",
                "ve",
                "vi",
                "wo",
                "war",
                "xh",
                "yi",
                "yo",
                "zh",
                "zh",
                "zu",
                "zza"
            ],
            "categories": [
                "other",
                "synthetic-language"
            ],
            "tasks": [
                "translation"
            ]
        }
    },
    {
        "id": "cc_news",
        "data": {
            "description": "CC-News containing news articles from news sites all over the world The data is available on AWS S3 in the Common Crawl bucket at /crawl-data/CC-NEWS/. This version of the dataset has 708241 articles. It represents a small portion of English  language subset of the CC-News dataset created using news-please(Hamborg et al.,2017) to collect and extract English language portion of CC-News.\n",
            "url": "https://commoncrawl.org/2016/10/news-dataset-available/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "cdsc",
        "data": {
            "description": "Polish CDSCorpus consists of 10K Polish sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for the evaluation of compositional distributional semantics models of Polish. The dataset was presented at ACL 2017. Please refer to the Wróblewska and Krasnowska-Kieraś (2017) for a detailed description of the resource.\n",
            "url": "http://zil.ipipan.waw.pl/Scwad/CDSCorpus",
            "license": "CC BY-NC-SA 4.0",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "pl"
            ],
            "categories": [
                "other"
            ],
            "tasks": [
                "other-other-sentences"
            ]
        }
    },
    {
        "id": "cdt",
        "data": {
            "description": "The Cyberbullying Detection task was part of 2019 edition of PolEval competition. The goal is to predict if a given Twitter message contains a cyberbullying (harmful) content.\n",
            "url": "https://github.com/ptaszynski/cyberbullying-Polish",
            "license": "BSD 3-Clause",
            "givenLicense": "bsd-3-clause",
            "language": [
                "pl"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "cedr",
        "data": {
            "description": "This new dataset is designed to solve emotion recognition task for text data in Russian. The Corpus for Emotions Detecting in\nRussian-language text sentences of different social sources (CEDR) contains 9410 sentences in Russian labeled for 5 emotion\ncategories. The data collected from different sources: posts of the LiveJournal social network, texts of the online news\nagency Lenta.ru, and Twitter microblog posts. There are two variants of the corpus: main and enriched. The enriched variant\nis include tokenization and lemmatization. Dataset with predefined train/test splits.\n",
            "url": "https://github.com/sag111/CEDR",
            "license": "http://www.apache.org/licenses/LICENSE-2.0",
            "givenLicense": "apache-2.0",
            "language": [
                "ru"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification",
                "multi-label-classification",
                "text-classification-other-emotion-classification"
            ]
        }
    },
    {
        "id": "cfq",
        "data": {
            "description": "\nThe CFQ dataset (and it's splits) for measuring compositional generalization.\n\nSee https://arxiv.org/abs/1912.09713.pdf for background.\n\nExample usage:\ndata = datasets.load_dataset('cfq/mcd1')\n",
            "url": "https://github.com/google-research/google-research/tree/master/cfq",
            "license": "CC BY 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering",
                "other"
            ],
            "tasks": [
                "open-domain-qa",
                "closed-domain-qa",
                "other-compositionality"
            ]
        }
    },
    {
        "id": "cifar10",
        "data": {
            "description": "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images\nper class. There are 50000 training images and 10000 test images.\n",
            "url": "https://www.cs.toronto.edu/~kriz/cifar.html",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification"
            ],
            "tasks": []
        }
    },
    {
        "id": "cifar100",
        "data": {
            "description": "The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images\nper class. There are 500 training images and 100 testing images per class. There are 50000 training images and 10000 test images. The 100 classes are grouped into 20 superclasses.\nThere are two labels per image - fine label (actual class) and coarse label (superclass).\n",
            "url": "https://www.cs.toronto.edu/~kriz/cifar.html",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification"
            ],
            "tasks": []
        }
    },
    {
        "id": "circa",
        "data": {
            "description": "The Circa (meaning ‘approximately’) dataset aims to help machine learning systems\nto solve the problem of interpreting indirect answers to polar questions.\n\nThe dataset contains pairs of yes/no questions and indirect answers, together with\nannotations for the interpretation of the answer. The data is collected in 10\ndifferent social conversational situations (eg. food preferences of a friend).\n\nNOTE: There might be missing labels in the dataset and we have replaced them with -1.\nThe original dataset contains no train/dev/test splits.\n",
            "url": "https://github.com/google-research-datasets/circa",
            "license": "Creative Commons Attribution 4.0 License",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "text-classification-other-question-answer-pair-classification"
            ]
        }
    },
    {
        "id": "civil_comments",
        "data": {
            "description": "\nThe comments in this dataset come from an archive of the Civil Comments\nplatform, a commenting plugin for independent news sites. These public comments\nwere created from 2015 - 2017 and appeared on approximately 50 English-language\nnews sites across the world. When Civil Comments shut down in 2017, they chose\nto make the public comments available in a lasting open archive to enable future\nresearch. The original data, published on figshare, includes the public comment\ntext, some associated metadata such as article IDs, timestamps and\ncommenter-generated \"civility\" labels, but does not include user ids. Jigsaw\nextended this dataset by adding additional labels for toxicity and identity\nmentions. This data set is an exact replica of the data released for the\nJigsaw Unintended Bias in Toxicity Classification Kaggle challenge.  This\ndataset is released under CC0, as is the underlying comment text.\n",
            "url": "https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "clickbait_news_bg",
        "data": {
            "description": "Dataset with clickbait and fake news in Bulgarian. Introduced for the Hack the Fake News 2017.\n",
            "url": "https://gitlab.com/datasciencesociety/case_fake_news/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "bg"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "fact-checking"
            ]
        }
    },
    {
        "id": "climate_fever",
        "data": {
            "description": "A dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change collected on the internet. Each claim is accompanied by five manually annotated evidence sentences retrieved from the English Wikipedia that support, refute or do not give enough information to validate the claim totalling in 7,675 claim-evidence pairs. The dataset features challenging claims that relate multiple facets and disputed cases of claims where both supporting and refuting evidence are present.\n",
            "url": "http://climatefever.ai",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification",
                "text-retrieval"
            ],
            "tasks": [
                "text-scoring",
                "fact-checking",
                "fact-checking-retrieval",
                "semantic-similarity-scoring",
                "multi-input-text-classification"
            ]
        }
    },
    {
        "id": "clinc_oos",
        "data": {
            "description": "    This dataset is for evaluating the performance of intent classification systems in the\n    presence of \"out-of-scope\" queries. By \"out-of-scope\", we mean queries that do not fall\n    into any of the system-supported intent classes. Most datasets include only data that is\n    \"in-scope\". Our dataset includes both in-scope and out-of-scope data. You might also know\n    the term \"out-of-scope\" by other terms, including \"out-of-domain\" or \"out-of-distribution\".\n\nSmall, in which there are only 50 training queries per each in-scope intent\n",
            "url": "https://github.com/clinc/oos-eval/",
            "license": "",
            "givenLicense": "cc-by-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification"
            ]
        }
    },
    {
        "id": "clue",
        "data": {
            "description": "CLUE, A Chinese Language Understanding Evaluation Benchmark\n(https://www.cluebenchmarks.com/) is a collection of resources for training,\nevaluating, and analyzing Chinese language understanding systems.\n\n",
            "url": "https://dc.cloud.alipay.com/index#/topic/data?id=8",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "cmrc2018",
        "data": {
            "description": "A Span-Extraction dataset for Chinese machine reading comprehension to add language\ndiversities in this area. The dataset is composed by near 20,000 real questions annotated\non Wikipedia paragraphs by human experts. We also annotated a challenge set which\ncontains the questions that need comprehensive understanding and multi-sentence\ninference throughout the context.\n",
            "url": "https://github.com/ymcui/cmrc2018",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "zh"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "cmu_hinglish_dog",
        "data": {
            "description": "This is a collection of text conversations in Hinglish (code mixing between Hindi-English) and their corresponding English only versions. Can be used for Translating between the two.\n",
            "url": "http://festvox.org/cedar/data/notyet/",
            "license": "",
            "givenLicense": "cc-by-sa-3.0,gfdl",
            "language": [
                "en",
                "hi"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "cnn_dailymail",
        "data": {
            "description": "CNN/DailyMail non-anonymized summarization dataset.\n\nThere are two features:\n  - article: text of news article, used as the document to be summarized\n  - highlights: joined text of highlights with <s> and </s> around each\n    highlight, which is the target summary\n",
            "url": "https://github.com/abisee/cnn-dailymail",
            "license": "",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "news-articles-summarization"
            ]
        }
    },
    {
        "id": "coached_conv_pref",
        "data": {
            "description": "A dataset consisting of 502 English dialogs with 12,000 annotated utterances between a user and an assistant discussing\nmovie preferences in natural language. It was collected using a Wizard-of-Oz methodology between two paid crowd-workers,\nwhere one worker plays the role of an 'assistant', while the other plays the role of a 'user'. The 'assistant' elicits\nthe 'user’s' preferences about movies following a Coached Conversational Preference Elicitation (CCPE) method. The\nassistant asks questions designed to minimize the bias in the terminology the 'user' employs to convey his or her\npreferences as much as possible, and to obtain these preferences in natural language. Each dialog is annotated with\nentity mentions, preferences expressed about entities, descriptions of entities provided, and other statements of\nentities.",
            "url": "https://research.google/tools/datasets/coached-conversational-preference-elicitation/",
            "license": "https://creativecommons.org/licenses/by-sa/4.0/",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "other",
                "text-generation",
                "fill-mask",
                "token-classification"
            ],
            "tasks": [
                "other-other-Conversational"
            ]
        }
    },
    {
        "id": "coarse_discourse",
        "data": {
            "description": "dataset contains discourse annotation and relation on threads from reddit during 2016\n",
            "url": "https://github.com/google-research-datasets/coarse-discourse",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "codah",
        "data": {
            "description": "The COmmonsense Dataset Adversarially-authored by Humans (CODAH) is an evaluation set for commonsense question-answering in the sentence completion style of SWAG. As opposed to other automatically generated NLI datasets, CODAH is adversarially constructed by humans who can view feedback from a pre-trained model and use this information to design challenging commonsense questions. Our experimental results show that CODAH questions present a complementary extension to the SWAG dataset, testing additional modes of common sense.\n",
            "url": "https://github.com/Websail-NU/CODAH",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "code_search_net",
        "data": {
            "description": "CodeSearchNet corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation.\n",
            "url": "https://github.com/github/CodeSearchNet",
            "license": "Various",
            "givenLicense": "other",
            "language": [
                "code"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "code_x_glue_cc_clone_detection_big_clone_bench",
        "data": {
            "description": "CodeXGLUE Clone-detection-BigCloneBench dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Clone-detection-BigCloneBench\n\nGiven two codes as the input, the task is to do binary classification (0/1), where 1 stands for semantic equivalence and 0 for others. Models are evaluated by F1 score.\nThe dataset we use is BigCloneBench and filtered following the paper Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax Tree.",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Code-Code/Clone-detection-BigCloneBench",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "code"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "semantic-similarity-classification"
            ]
        }
    },
    {
        "id": "code_x_glue_cc_clone_detection_poj104",
        "data": {
            "description": "CodeXGLUE Clone-detection-POJ-104 dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Clone-detection-POJ-104\n\nGiven a code and a collection of candidates as the input, the task is to return Top K codes with the same semantic. Models are evaluated by MAP score.\nWe use POJ-104 dataset on this task.",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Code-Code/Clone-detection-POJ-104",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "code"
            ],
            "categories": [
                "text-retrieval"
            ],
            "tasks": [
                "document-retrieval"
            ]
        }
    },
    {
        "id": "code_x_glue_cc_cloze_testing_all",
        "data": {
            "description": "CodeXGLUE ClozeTesting-all dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/ClozeTesting-all\n\nCloze tests are widely adopted in Natural Languages Processing to evaluate the performance of the trained language models. The task is aimed to predict the answers for the blank with the context of the blank, which can be formulated as a multi-choice classification problem.\nHere we present the two cloze testing datasets in code domain with six different programming languages: ClozeTest-maxmin and ClozeTest-all. Each instance in the dataset contains a masked code function, its docstring and the target word.\nThe only difference between ClozeTest-maxmin and ClozeTest-all is their selected words sets, where ClozeTest-maxmin only contains two words while ClozeTest-all contains 930 words.",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Code-Code/ClozeTesting-all",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "code"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "slot-filling"
            ]
        }
    },
    {
        "id": "code_x_glue_cc_cloze_testing_maxmin",
        "data": {
            "description": "CodeXGLUE ClozeTesting-maxmin dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/ClozeTesting-maxmin\n\nCloze tests are widely adopted in Natural Languages Processing to evaluate the performance of the trained language models. The task is aimed to predict the answers for the blank with the context of the blank, which can be formulated as a multi-choice classification problem.\nHere we present the two cloze testing datasets in code domain with six different programming languages: ClozeTest-maxmin and ClozeTest-all. Each instance in the dataset contains a masked code function, its docstring and the target word.\nThe only difference between ClozeTest-maxmin and ClozeTest-all is their selected words sets, where ClozeTest-maxmin only contains two words while ClozeTest-all contains 930 words.",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Code-Code/ClozeTesting-maxmin",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "code"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "slot-filling"
            ]
        }
    },
    {
        "id": "code_x_glue_cc_code_completion_line",
        "data": {
            "description": "CodeXGLUE CodeCompletion-line dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/CodeCompletion-line\n\nComplete the unfinished line given previous context. Models are evaluated by exact match and edit similarity.\nWe propose line completion task to test model's ability to autocomplete a line. Majority code completion systems behave well in token level completion, but fail in completing an unfinished line like a method call with specific parameters, a function signature, a loop condition, a variable definition and so on. When a software develop finish one or more tokens of the current line, the line level completion model is expected to generate the entire line of syntactically correct code.\nLine level code completion task shares the train/dev dataset with token level completion. After training a model on CodeCompletion-token, you could directly use it to test on line-level completion.",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Code-Code/CodeCompletion-line",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "code"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "slot-filling"
            ]
        }
    },
    {
        "id": "code_x_glue_cc_code_completion_token",
        "data": {
            "description": "CodeXGLUE CodeCompletion-token dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/CodeCompletion-token\n\nPredict next code token given context of previous tokens. Models are evaluated by token level accuracy.\nCode completion is a one of the most widely used features in software development through IDEs. An effective code completion tool could improve software developers' productivity. We provide code completion evaluation tasks in two granularities -- token level and line level. Here we introduce token level code completion. Token level task is analogous to language modeling. Models should have be able to predict the next token in arbitary types.\n",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Code-Code/CodeCompletion-token",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "code"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "code_x_glue_cc_code_refinement",
        "data": {
            "description": "CodeXGLUE code-refinement dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/code-refinement\n\nWe use the dataset released by this paper(https://arxiv.org/pdf/1812.08693.pdf). The source side is a Java function with bugs and the target side is the refined one. All the function and variable names are normalized. Their dataset contains two subsets ( i.e.small and medium) based on the function length.",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Code-Code/code-refinement",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "code"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-debugging"
            ]
        }
    },
    {
        "id": "code_x_glue_cc_code_to_code_trans",
        "data": {
            "description": "CodeXGLUE code-to-code-trans dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/code-to-code-trans\n\nThe dataset is collected from several public repos, including Lucene(http://lucene.apache.org/), POI(http://poi.apache.org/), JGit(https://github.com/eclipse/jgit/) and Antlr(https://github.com/antlr/).\n        We collect both the Java and C# versions of the codes and find the parallel functions. After removing duplicates and functions with the empty body, we split the whole dataset into training, validation and test sets.",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Code-Code/code-to-code-trans",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "code"
            ],
            "categories": [
                "translation"
            ],
            "tasks": [
                "translation-other-code-to-code"
            ]
        }
    },
    {
        "id": "code_x_glue_cc_defect_detection",
        "data": {
            "description": "CodeXGLUE Defect-detection dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection\n\nGiven a source code, the task is to identify whether it is an insecure code that may attack software systems, such as resource leaks, use-after-free vulnerabilities and DoS attack. We treat the task as binary classification (0/1), where 1 stands for insecure code and 0 for secure code.\nThe dataset we use comes from the paper Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks. We combine all projects and split 80%/10%/10% for training/dev/test.",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Code-Code/Defect-detection",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "code"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "code_x_glue_ct_code_to_text",
        "data": {
            "description": "CodeXGLUE code-to-text dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text\n\nThe dataset we use comes from CodeSearchNet and we filter the dataset as the following:\n- Remove examples that codes cannot be parsed into an abstract syntax tree.\n- Remove examples that #tokens of documents is < 3 or >256\n- Remove examples that documents contain special tokens (e.g. <img ...> or https:...)\n- Remove examples that documents are not English.\n",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Code-Text/code-to-text",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "code",
                "en"
            ],
            "categories": [
                "translation"
            ],
            "tasks": [
                "translation-other-code-to-text"
            ]
        }
    },
    {
        "id": "code_x_glue_tc_nl_code_search_adv",
        "data": {
            "description": "CodeXGLUE NL-code-search-Adv dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Text-Code/NL-code-search-Adv\n\nThe dataset we use comes from CodeSearchNet and we filter the dataset as the following:\n- Remove examples that codes cannot be parsed into an abstract syntax tree.\n- Remove examples that #tokens of documents is < 3 or >256\n- Remove examples that documents contain special tokens (e.g. <img ...> or https:...)\n- Remove examples that documents are not English.\n",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Text-Code/NL-code-search-Adv",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "code",
                "en"
            ],
            "categories": [
                "text-retrieval"
            ],
            "tasks": [
                "document-retrieval"
            ]
        }
    },
    {
        "id": "code_x_glue_tc_text_to_code",
        "data": {
            "description": "CodeXGLUE text-to-code dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Text-Code/text-to-code\n\nWe use concode dataset which is a widely used code generation dataset from Iyer's EMNLP 2018 paper Mapping Language to Code in Programmatic Context. See paper for details.",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Text-Code/text-to-code",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "en",
                "code"
            ],
            "categories": [
                "translation"
            ],
            "tasks": [
                "translation-other-text-to-code"
            ]
        }
    },
    {
        "id": "code_x_glue_tt_text_to_text",
        "data": {
            "description": "CodeXGLUE text-to-text dataset, available at https://github.com/microsoft/CodeXGLUE/tree/main/Text-Text/text-to-text\n\nThe dataset we use is crawled and filtered from Microsoft Documentation, whose document located at https://github.com/MicrosoftDocs/.",
            "url": "https://github.com/madlag/CodeXGLUE/tree/main/Text-Text/text-to-text",
            "license": "",
            "givenLicense": "c-uda",
            "language": [
                "da",
                "nb",
                "lv",
                "zh",
                "en"
            ],
            "categories": [
                "translation"
            ],
            "tasks": [
                "translation-other-code-documentation-translation"
            ]
        }
    },
    {
        "id": "common_gen",
        "data": {
            "description": "CommonGen is a constrained text generation task, associated with a benchmark dataset, \nto explicitly test machines for the ability of generative commonsense reasoning. Given \na set of common concepts; the task is to generate a coherent sentence describing an \neveryday scenario using these concepts.\n\nCommonGen is challenging because it inherently requires 1) relational reasoning using \nbackground commonsense knowledge, and 2) compositional generalization ability to work \non unseen concept combinations. Our dataset, constructed through a combination of \ncrowd-sourcing from AMT and existing caption corpora, consists of 30k concept-sets and \n50k sentences in total.\n",
            "url": "https://inklab.usc.edu/CommonGen/index.html",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-concepts-to-text"
            ]
        }
    },
    {
        "id": "common_language",
        "data": {
            "description": "This dataset is composed of speech recordings from languages that were carefully selected from the CommonVoice database.\nThe total duration of audio recordings is 45.1 hours (i.e., 1 hour of material for each language).\nThe dataset has been extracted from CommonVoice to train language-id systems.\n",
            "url": "https://zenodo.org/record/5036977",
            "license": "https://creativecommons.org/licenses/by/4.0/legalcode",
            "givenLicense": "cc-by-4.0",
            "language": [
                "ar",
                "br",
                "ca",
                "cnh",
                "cs",
                "cv",
                "cy",
                "de",
                "dv",
                "el",
                "en",
                "eo",
                "es",
                "et",
                "eu",
                "fa",
                "fr",
                "fy-NL",
                "ia",
                "id",
                "it",
                "ja",
                "ka",
                "kab",
                "ky",
                "lv",
                "mn",
                "mt",
                "nl",
                "pl",
                "pt",
                "rm-sursilv",
                "ro",
                "ru",
                "rw",
                "sah",
                "sl",
                "sv-SE",
                "ta",
                "tr",
                "tt",
                "uk",
                "zh-CN",
                "zh-HK",
                "zh-TW"
            ],
            "categories": [
                "audio-classification"
            ],
            "tasks": [
                "speaker-language-identification"
            ]
        }
    },
    {
        "id": "commonsense_qa",
        "data": {
            "description": "CommonsenseQA is a new multiple-choice question answering dataset that requires different types of commonsense knowledge\nto predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers.\nThe dataset is provided in two major training/validation/testing set splits: \"Random split\" which is the main evaluation\nsplit, and \"Question token split\", see paper for details.\n",
            "url": "https://www.tau-nlp.org/commonsenseqa",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "common_voice",
        "data": {
            "description": "Common Voice is Mozilla's initiative to help teach machines how real people speak.\nThe dataset currently consists of 7,335 validated hours of speech in 60 languages, but we’re always adding more voices and languages.\n",
            "url": "https://commonvoice.mozilla.org/en/datasets",
            "license": "https://github.com/common-voice/common-voice/blob/main/LICENSE",
            "givenLicense": "cc0-1.0",
            "language": [
                "ab",
                "ar",
                "as",
                "br",
                "ca",
                "cnh",
                "cs",
                "cv",
                "cy",
                "de",
                "dv",
                "el",
                "en",
                "eo",
                "es",
                "et",
                "eu",
                "fa",
                "fi",
                "fr",
                "fy-NL",
                "ga-IE",
                "hi",
                "hsb",
                "hu",
                "ia",
                "id",
                "it",
                "ja",
                "ka",
                "kab",
                "ky",
                "lg",
                "lt",
                "lv",
                "mn",
                "mt",
                "nl",
                "or",
                "pa-IN",
                "pl",
                "pt",
                "rm-sursilv",
                "rm-vallader",
                "ro",
                "ru",
                "rw",
                "sah",
                "sl",
                "sv-SE",
                "ta",
                "th",
                "tr",
                "tt",
                "uk",
                "vi",
                "vot",
                "zh-CN",
                "zh-HK",
                "zh-TW"
            ],
            "categories": [
                "automatic-speech-recognition"
            ],
            "tasks": []
        }
    },
    {
        "id": "competition_math",
        "data": {
            "description": "The Mathematics Aptitude Test of Heuristics (MATH) dataset consists of problems\nfrom mathematics competitions, including the AMC 10, AMC 12, AIME, and more.\nEach problem in MATH has a full step-by-step solution, which can be used to teach\nmodels to generate answer derivations and explanations.\n",
            "url": "https://github.com/hendrycks/math",
            "license": "https://github.com/hendrycks/math/blob/main/LICENSE",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-explanation-generation"
            ]
        }
    },
    {
        "id": "compguesswhat",
        "data": {
            "description": "\n        CompGuessWhat?! is an instance of a multi-task framework for evaluating the quality of learned neural representations,\n        in particular concerning attribute grounding. Use this dataset if you want to use the set of games whose reference\n        scene is an image in VisualGenome. Visit the website for more details: https://compguesswhat.github.io\n    ",
            "url": "https://compguesswhat.github.io/",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "com_qa",
        "data": {
            "description": "ComQA is a dataset of 11,214 questions, which were collected from WikiAnswers, a community question answering website. \nBy collecting questions from such a site we ensure that the information needs are ones of interest to actual users. \nMoreover, questions posed there are often cannot be answered by commercial search engines or QA technology, making them \nmore interesting for driving future research compared to those collected from an engine's query log. The dataset contains \nquestions with various challenging phenomena such as the need for temporal reasoning, comparison (e.g., comparatives, \nsuperlatives, ordinals), compositionality (multiple, possibly nested, subquestions with multiple entities), and \nunanswerable questions (e.g., Who was the first human being on Mars?). Through a large crowdsourcing effort, questions \nin ComQA are grouped into 4,834 paraphrase clusters that express the same information need. Each cluster is annotated \nwith its answer(s). ComQA answers come in the form of Wikipedia entities wherever possible. Wherever the answers are \ntemporal or measurable quantities, TIMEX3 and the International System of Units (SI) are used for normalization.\n",
            "url": "http://qa.mpi-inf.mpg.de/comqa/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "conceptnet5",
        "data": {
            "description": "\\ This dataset is designed to provide training data\nfor common sense relationships pulls together from various\nsources. \n\nThe dataset is multi-lingual. See langauge codes and language info\nhere: https://github.com/commonsense/conceptnet5/wiki/Languages\n\n\nThis dataset provides an interface for the conceptnet5 csv file, and\nsome (but not all) of the raw text data used to build conceptnet5:\nomcsnet_sentences_free.txt, and omcsnet_sentences_more.txt.\n\nOne use of this dataset would be to learn to extract the conceptnet\nrelationship from the omcsnet sentences.\n\nConceptnet5 has 34,074,917 relationships. Of those relationships,\nthere are 2,176,099 surface text sentences related to those 2M\nentries.\n\nomcsnet_sentences_free has 898,161 lines. omcsnet_sentences_more has\n2,001,736 lines.\n\nOriginal downloads are available here\nhttps://github.com/commonsense/conceptnet5/wiki/Downloads. For more\ninformation, see: https://github.com/commonsense/conceptnet5/wiki\n\nThe omcsnet data comes with the following warning from the authors of\nthe above site: \n\nRemember: this data comes from various forms of\ncrowdsourcing. Sentences in these files are not necessarily true,\nuseful, or appropriate.\n\n",
            "url": "https://github.com/commonsense/conceptnet5/wiki",
            "license": "\nThis work includes data from ConceptNet 5, which was compiled by the\nCommonsense Computing Initiative. ConceptNet 5 is freely available under\nthe Creative Commons Attribution-ShareAlike license (CC BY SA 3.0) from\nhttp://conceptnet.io.\n\nThe included data was created by contributors to Commonsense Computing\nprojects, contributors to Wikimedia projects, DBPedia, OpenCyc, Games\nwith a Purpose, Princeton University's WordNet, Francis Bond's Open\nMultilingual WordNet, and Jim Breen's JMDict.\n\nThere are various othe licenses. See:\nhttps://github.com/commonsense/conceptnet5/wiki/Copying-and-sharing-ConceptNet\n",
            "givenLicense": "cc-by-4.0",
            "language": [
                "de",
                "en",
                "es",
                "fr",
                "it",
                "ja",
                "nl",
                "pt",
                "ru",
                "zh"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "conceptual_12m",
        "data": {
            "description": "Conceptual 12M is a large-scale dataset of 12 million\nimage-text pairs specifically meant to be used for visionand-language pre-training.\nIts data collection pipeline is a relaxed version of the one used in Conceptual Captions 3M.\n",
            "url": "https://github.com/google-research-datasets/conceptual-12m",
            "license": "The dataset may be freely used for any purpose, although acknowledgement of\nGoogle LLC (\"Google\") as the data source would be appreciated. The dataset is\nprovided \"AS IS\" without any warranty, express or implied. Google disclaims all\nliability for any damages, direct or indirect, resulting from the use of the\ndataset.\n",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "image-to-text"
            ],
            "tasks": [
                "image-captioning"
            ]
        }
    },
    {
        "id": "conceptual_captions",
        "data": {
            "description": "Image captioning dataset\nThe resulting dataset (version 1.1) has been split into Training, Validation, and Test splits. The Training split consists of 3,318,333 image-URL/caption pairs, with a total number of 51,201 total token types in the captions (i.e., total vocabulary). The average number of tokens per captions is 10.3 (standard deviation of 4.5), while the median is 9.0 tokens per caption. The Validation split consists of 15,840 image-URL/caption pairs, with similar statistics.\n",
            "url": "http://data.statmt.org/cc-100/",
            "license": "",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "image-to-text"
            ],
            "tasks": [
                "image-captioning"
            ]
        }
    },
    {
        "id": "conll2000",
        "data": {
            "description": " Text chunking consists of dividing a text in syntactically correlated parts of words. For example, the sentence\n He reckons the current account deficit will narrow to only # 1.8 billion in September . can be divided as follows:\n[NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP to ] [NP only # 1.8 billion ]\n[PP in ] [NP September ] .\n\nText chunking is an intermediate step towards full parsing. It was the shared task for CoNLL-2000. Training and test\ndata for this task is available. This data consists of the same partitions of the Wall Street Journal corpus (WSJ)\nas the widely used data for noun phrase chunking: sections 15-18 as training data (211727 tokens) and section 20 as\ntest data (47377 tokens). The annotation of the data has been derived from the WSJ corpus by a program written by\nSabine Buchholz from Tilburg University, The Netherlands.\n",
            "url": "https://www.clips.uantwerpen.be/conll2000/chunking/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "conll2002",
        "data": {
            "description": "Named entities are phrases that contain the names of persons, organizations, locations, times and quantities.\n\nExample:\n[PER Wolff] , currently a journalist in [LOC Argentina] , played with [PER Del Bosque] in the final years of the seventies in [ORG Real Madrid] .\n\nThe shared task of CoNLL-2002 concerns language-independent named entity recognition.\nWe will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.\nThe participants of the shared task will be offered training and test data for at least two languages.\nThey will use the data for developing a named-entity recognition system that includes a machine learning component.\nInformation sources other than the training data may be used in this shared task.\nWe are especially interested in methods that can use additional unannotated data for improving their performance (for example co-training).\n\nThe train/validation/test sets are available in Spanish and Dutch.\n\nFor more details see https://www.clips.uantwerpen.be/conll2002/ner/ and https://www.aclweb.org/anthology/W02-2024/\n",
            "url": "https://www.aclweb.org/anthology/W02-2024/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "es",
                "nl"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition",
                "part-of-speech-tagging"
            ]
        }
    },
    {
        "id": "conll2003",
        "data": {
            "description": "The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\nnot belong to the previous three groups.\n\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\ntagging scheme, whereas the original dataset uses IOB1.\n\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\n",
            "url": "https://www.aclweb.org/anthology/W03-0419/",
            "license": "",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition",
                "part-of-speech"
            ]
        }
    },
    {
        "id": "conll2012_ontonotesv5",
        "data": {
            "description": "OntoNotes v5.0 is the final version of OntoNotes corpus, and is a large-scale, multi-genre,\nmultilingual corpus manually annotated with syntactic, semantic and discourse information.\n\nThis dataset is the version of OntoNotes v5.0 extended and is used in the CoNLL-2012 shared task.\nIt includes v4 train/dev and v9 test data for English/Chinese/Arabic and corrected version v12 train/dev/test data (English only).\n\nThe source of data is the Mendeley Data repo [ontonotes-conll2012](https://data.mendeley.com/datasets/zmycy7t9h9), which seems to be as the same as the official data, but users should use this dataset on their own responsibility.\n\nSee also summaries from paperwithcode, [OntoNotes 5.0](https://paperswithcode.com/dataset/ontonotes-5-0) and [CoNLL-2012](https://paperswithcode.com/dataset/conll-2012-1)\n\nFor more detailed info of the dataset like annotation, tag set, etc., you can refer to the documents in the Mendeley repo mentioned above.\n",
            "url": "https://conll.cemantix.org/2012/introduction.html",
            "license": "",
            "givenLicense": "cc-by-nc-nd-4.0",
            "language": [
                "ar",
                "en",
                "zh"
            ],
            "categories": [
                "token-classification",
                "natural-language-processing"
            ],
            "tasks": [
                "named-entity-recognition",
                "part-of-speech-tagging",
                "semantic-role-labeling",
                "coreference-resolution",
                "parsing",
                "lemmatization",
                "word-sense-disambiguation"
            ]
        }
    },
    {
        "id": "conllpp",
        "data": {
            "description": "CoNLLpp is a corrected version of the CoNLL2003 NER dataset where labels of 5.38% of the sentences in the test set\nhave been manually corrected. The training set and development set are included for completeness.\nFor more details see https://www.aclweb.org/anthology/D19-1519/ and https://github.com/ZihanWangKi/CrossWeigh\n",
            "url": "https://github.com/ZihanWangKi/CrossWeigh",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "consumer-finance-complaints",
        "data": {
            "description": "The Consumer Complaint Database is a collection of complaints about consumer financial products and services that we sent to companies for response. Complaints are published after the company responds, confirming a commercial relationship with the consumer, or after 15 days, whichever comes first. Complaints referred to other regulators, such as complaints about depository institutions with less than $10 billion in assets, are not published in the Consumer Complaint Database. The database generally updates daily.\nThere are multiple Text Classification problems that can be solved with this dataset:\n- Complaint Type Identification\n- Complaint Sub-Product Identification\n- Complaint from vulnerable or Service-person\n",
            "url": "https://www.consumerfinance.gov/data-research/consumer-complaints/",
            "license": "https://cfpb.github.io/source-code-policy/",
            "givenLicense": "cc0-1.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "topic-classification"
            ]
        }
    },
    {
        "id": "conv_ai",
        "data": {
            "description": "ConvAI is a dataset of human-to-bot conversations labelled for quality. This data can be used to train a metric for evaluating dialogue systems. Moreover, it can be used in the development of chatbots themselves: it contains the information on the quality of utterances and entire dialogues, that can guide a dialogue system in search of better answers.\n",
            "url": "https://github.com/DeepPavlov/convai/tree/master/2017",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "conversational",
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "text-classification-other-evaluating-dialogue-systems"
            ]
        }
    },
    {
        "id": "conv_ai_2",
        "data": {
            "description": "ConvAI is a dataset of human-to-bot conversations labelled for quality. This data can be used to train a metric for evaluating dialogue systems. Moreover, it can be used in the development of chatbots themselves: it contains the information on the quality of utterances and entire dialogues, that can guide a dialogue system in search of better answers.\n",
            "url": "https://github.com/DeepPavlov/convai/tree/master/2018",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "conversational",
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "text-scoring-other-evaluating-dialogue-systems"
            ]
        }
    },
    {
        "id": "conv_ai_3",
        "data": {
            "description": "The Conv AI 3 challenge is organized as part of the Search-oriented Conversational AI (SCAI) EMNLP workshop in 2020. The main aim of the conversational systems is to return an appropriate answer in response to the user requests. However, some user requests might be ambiguous. In Information Retrieval (IR) settings such a situation is handled mainly through the diversification of search result page. It is however much more challenging in dialogue settings. Hence, we aim to study the following situation for dialogue settings: \n- a user is asking an ambiguous question (where ambiguous question is a question to which one can return > 1 possible answers)\n- the system must identify that the question is ambiguous, and, instead of trying to answer it directly, ask a good clarifying question.\n",
            "url": "https://github.com/aliannejadi/ClariQ",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "conversational",
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "text-scoring-other-evaluating-dialogue-systems"
            ]
        }
    },
    {
        "id": "conv_questions",
        "data": {
            "description": "ConvQuestions is the first realistic benchmark for conversational question answering over knowledge graphs.\nIt contains 11,200 conversations which can be evaluated over Wikidata. The questions feature a variety of complex\nquestion phenomena like comparisons, aggregations, compositionality, and temporal reasoning.",
            "url": "https://convex.mpi-inf.mpg.de",
            "license": "CC BY 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en-US"
            ],
            "categories": [
                "question-answering",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "open-domain-qa",
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "coqa",
        "data": {
            "description": "CoQA: A Conversational Question Answering Challenge\n",
            "url": "https://stanfordnlp.github.io/coqa/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "cord19",
        "data": {
            "description": "The Covid-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on Covid-19 and related\nhistorical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information\nretrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19\nhas been downloaded over 75K times and has served as the basis of many Covid-19 text mining and discovery systems.\n\nThe dataset itself isn't defining a specific task, but there is a Kaggle challenge that define 17 open research\nquestions to be solved with the dataset: https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks\n",
            "url": "https://www.semanticscholar.org/cord19/download",
            "license": "",
            "givenLicense": "cc-by-nd-4.0,cc-by-sa-4.0,other",
            "language": [
                "en"
            ],
            "categories": [
                "other"
            ],
            "tasks": [
                "knowledge-extraction"
            ]
        }
    },
    {
        "id": "cornell_movie_dialog",
        "data": {
            "description": "     \nThis corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts:\n- 220,579 conversational exchanges between 10,292 pairs of movie characters\n- involves 9,035 characters from 617 movies\n- in total 304,713 utterances\n- movie metadata included:\n    - genres\n    - release year\n    - IMDB rating\n    - number of IMDB votes\n    - IMDB rating\n- character metadata included:\n    - gender (for 3,774 characters)\n    - position on movie credits (3,321 characters)\n",
            "url": "http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "cos_e",
        "data": {
            "description": "\nCommon Sense Explanations (CoS-E) allows for training language models to\nautomatically generate explanations that can be used during training and\ninference in a novel Commonsense Auto-Generated Explanation (CAGE) framework.\n",
            "url": "https://github.com/salesforce/cos-e",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "cosmos_qa",
        "data": {
            "description": "Cosmos QA is a large-scale dataset of 35.6K problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. It focuses on reading between the lines over a diverse collection of people's everyday narratives, asking questions concerning on the likely causes or effects of events that require reasoning beyond the exact text spans in the context\n",
            "url": "https://wilburone.github.io/cosmos/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "counter",
        "data": {
            "description": " The COrpus of Urdu News TExt Reuse (COUNTER) corpus contains 1200 documents with real examples of text reuse from the field of journalism. It has been manually annotated at document level with three levels of reuse: wholly derived, partially derived and non derived.\n",
            "url": "http://ucrel.lancs.ac.uk/textreuse/counter.php",
            "license": "The corpus is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. ",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "ur"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "semantic-similarity-scoring",
                "topic-classification"
            ]
        }
    },
    {
        "id": "covid_qa_castorini",
        "data": {
            "description": "COVID-QA is a Question Answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19.\n",
            "url": "https://github.com/deepset-ai/COVID-QA",
            "license": "Apache License 2.0",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa",
                "extractive-qa"
            ]
        }
    },
    {
        "id": "covid_qa_deepset",
        "data": {
            "description": "COVID-QA is a Question Answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19.\n",
            "url": "https://github.com/deepset-ai/COVID-QA",
            "license": "Apache License 2.0",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "closed-domain-qa",
                "extractive-qa"
            ]
        }
    },
    {
        "id": "covid_qa_ucsd",
        "data": {
            "description": "\nCOVID-Dialogue-Dataset is amedical dialogue dataset about COVID-19 and other types of pneumonia.\nPatients who are concerned that they may be infected by COVID-19 or other pneumonia consult doctors and doctors provide advice.\nThere are 603 consultations in English and 1393 consultations in Chinese.\n",
            "url": "https://github.com/UCSD-AI4H/COVID-Dialogue",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "zh"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "closed-domain-qa"
            ]
        }
    },
    {
        "id": "covid_tweets_japanese",
        "data": {
            "description": "53,640 Japanese tweets with annotation if a tweet is related to COVID-19 or not. The annotation is by majority decision by 5 - 10 crowd workers. Target tweets include \"COVID\" or \"コロナ\". The period of the tweets is from around January 2020 to around June 2020. The original tweets are not contained. Please use Twitter API to get them, for example.\n",
            "url": "http://www.db.info.gifu-u.ac.jp/data/Data_5f02db873363f976fce930d1",
            "license": "CC-BY-ND 4.0",
            "givenLicense": "cc-by-nd-4.0",
            "language": [
                "ja"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "fact-checking"
            ]
        }
    },
    {
        "id": "covost2",
        "data": {
            "description": "\nCoVoST 2, a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. The dataset is created using Mozilla’s open source Common Voice database of crowdsourced voice recordings.\n\nNote that in order to limit the required storage for preparing this dataset, the audio\nis stored in the .mp3 format and is not converted to a float32 array. To convert, the audio\nfile to a float32 array, please make use of the `.map()` function as follows:\n\n\n```python\nimport torchaudio\n\ndef map_to_array(batch):\n    speech_array, _ = torchaudio.load(batch[\"file\"])\n    batch[\"speech\"] = speech_array.numpy()\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n",
            "url": "https://github.com/facebookresearch/covost",
            "license": "",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "fr",
                "de",
                "es",
                "ca",
                "it",
                "ru",
                "zh-CN",
                "pt",
                "fa",
                "et",
                "mn",
                "nl",
                "tr",
                "ar",
                "sv-SE",
                "lv",
                "sl",
                "ta",
                "ja",
                "id",
                "cy"
            ],
            "categories": [
                "automatic-speech-recognition"
            ],
            "tasks": []
        }
    },
    {
        "id": "cppe-5",
        "data": {
            "description": "CPPE - 5 (Medical Personal Protective Equipment) is a new challenging dataset with the goal\nto allow the study of subordinate categorization of medical personal protective equipments,\nwhich is not possible with other popular data sets that focus on broad level categories.\n",
            "url": "https://sites.google.com/view/cppe5",
            "license": "Unknown",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "object-detection"
            ],
            "tasks": [
                "other"
            ]
        }
    },
    {
        "id": "craigslist_bargains",
        "data": {
            "description": "We study negotiation dialogues where two agents, a buyer and a seller,\nnegotiate over the price of an time for sale. We collected a dataset of more\nthan 6K negotiation dialogues over multiple categories of products scraped from Craigslist.\nOur goal is to develop an agent that negotiates with humans through such conversations.\nThe challenge is to handle both the negotiation strategy and the rich language for bargaining.\n",
            "url": "https://stanfordnlp.github.io/cocoa/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "crawl_domain",
        "data": {
            "description": "Corpus of domain names scraped from Common Crawl and manually annotated to add word boundaries (e.g. \"commoncrawl\" to \"common crawl\"). Breaking domain names such as \"openresearch\" into component words \"open\" and \"research\" is important for applications such as Text-to-Speech synthesis and web search. Common Crawl is an open repository of web crawl data that can be accessed and analyzed by anyone. Specifically, we scraped the plaintext (WET) extracts for domain names from URLs that contained diverse letter casing (e.g. \"OpenBSD\"). Although in the previous example, segmentation is trivial using letter casing, this was not always the case (e.g. \"NASA\"), so we had to manually annotate the data. The dataset is stored as plaintext file where each line is an example of space separated segments of a domain name. The examples are stored in their original letter casing, but harder and more interesting examples can be generated by lowercasing the input first.",
            "url": "https://github.com/google-research-datasets/common-crawl-domain-names",
            "license": "MIT License",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "other"
            ],
            "tasks": [
                "other-other-text-to-speech",
                "other-other-web-search"
            ]
        }
    },
    {
        "id": "crd3",
        "data": {
            "description": "\nStorytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset.\nCritical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons, an open-ended role-playing game.\nThe dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding\nabstractive summaries collected from the Fandom wiki. The dataset is linguistically unique in that the narratives are generated entirely through player\ncollaboration and spoken interaction. For each dialogue, there are a large number of turns, multiple abstractive summaries with varying levels of detail,\nand semantic ties to the previous dialogues.\n",
            "url": "https://github.com/RevanthRameshkumar/CRD3",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "summarization",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "crime_and_punish",
        "data": {
            "description": "\n",
            "url": "https://www.gutenberg.org/files/2554/2554-h/2554-h.htm",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "crows_pairs",
        "data": {
            "description": "CrowS-Pairs, a challenge dataset for measuring the degree to which U.S. stereotypical biases present in the masked language models (MLMs).\n",
            "url": "https://github.com/nyu-mll/crows-pairs",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "text-classification-other-bias-evaluation"
            ]
        }
    },
    {
        "id": "cryptonite",
        "data": {
            "description": "We study negotiation dialogues where two agents, a buyer and a seller,\nnegotiate over the price of an time for sale. We collected a dataset of more\nthan 6K negotiation dialogues over multiple categories of products scraped from Craigslist.\nOur goal is to develop an agent that negotiates with humans through such conversations.\nThe challenge is to handle both the negotiation strategy and the rich language for bargaining.\n",
            "url": "https://stanfordnlp.github.io/cocoa/",
            "license": "",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "cs_restaurants",
        "data": {
            "description": "This is a dataset for NLG in task-oriented spoken dialogue systems with Czech as the target language. It originated as \na translation of the English San Francisco Restaurants dataset by Wen et al. (2015).\n",
            "url": "https://github.com/UFAL-DSG/cs_restaurant_dataset",
            "license": "Creative Commons 4.0 BY-SA",
            "givenLicense": "cc-by-4.0",
            "language": [
                "cs"
            ],
            "categories": [
                "text2text-generation",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling",
                "language-modeling",
                "masked-language-modeling",
                "text2text-generation-other-intent-to-text"
            ]
        }
    },
    {
        "id": "cuad",
        "data": {
            "description": "Contract Understanding Atticus Dataset (CUAD) v1 is a corpus of more than 13,000 labels in 510\ncommercial legal contracts that have been manually labeled to identify 41 categories of important\nclauses that lawyers look for when reviewing contracts in connection with corporate transactions.\n",
            "url": "https://www.atticusprojectai.org/cuad",
            "license": "CUAD is licensed under the Creative Commons Attribution 4.0 (CC BY 4.0) license.",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "closed-domain-qa",
                "extractive-qa"
            ]
        }
    },
    {
        "id": "curiosity_dialogs",
        "data": {
            "description": "This dataset contains 14K dialogs (181K utterances) where users and assistants converse about geographic topics like\ngeopolitical entities and locations. This dataset is annotated with pre-existing user knowledge, message-level dialog\nacts, grounding to Wikipedia, and user reactions to messages.\n",
            "url": "https://www.pedro.ai/curiosity",
            "license": "https://github.com/facebookresearch/curiosity/blob/master/LICENSE",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling",
                "text-generation",
                "fill-mask-other-conversational-curiosity"
            ]
        }
    },
    {
        "id": "daily_dialog",
        "data": {
            "description": "We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects. \nThe language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way \nand cover various topics about our daily life. We also manually label the developed dataset with communication \nintention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it \nbenefit the research field of dialog systems.\n",
            "url": "http://yanran.li/dailydialog",
            "license": "cc-by-nc-sa-4.0",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification",
                "text-classification-other-emotion-classification",
                "text-classification-other-dialog-act-classification"
            ]
        }
    },
    {
        "id": "dane",
        "data": {
            "description": "The DaNE dataset has been annotated with Named Entities for PER, ORG and LOC\nby the Alexandra Institute.\nIt is a reannotation of the UD-DDT (Universal Dependency - Danish Dependency Treebank)\nwhich has annotations for dependency parsing and part-of-speech (POS) tagging.\nThe Danish UD treebank (Johannsen et al., 2015, UD-DDT) is a conversion of\nthe Danish Dependency Treebank (Buch-Kromann et al. 2003) based on texts\nfrom Parole (Britt, 1998).\n",
            "url": "https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane",
            "license": "CC BY-SA 4.0",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "da"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition",
                "part-of-speech-tagging"
            ]
        }
    },
    {
        "id": "danish_political_comments",
        "data": {
            "description": "The dataset consists of 9008 sentences that are labelled with fine-grained polarity in the range from -2 to 2 (negative to postive). The quality of the fine-grained is not cross validated and is therefore subject to uncertainties; however, the simple polarity has been cross validated and therefore is considered to be more correct.\n",
            "url": "https://github.com/steffan267/Sentiment-Analysis-on-Danish-Social-Media",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "da"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "dart",
        "data": {
            "description": "DART is a large and open-domain structured DAta Record to Text generation corpus with high-quality\nsentence annotations with each input being a set of entity-relation triples following a tree-structured ontology.\nIt consists of 82191 examples across different domains with each input being a semantic RDF triple set derived\nfrom data records in tables and the tree ontology of table schema, annotated with sentence description that\ncovers all facts in the triple set.\n\nDART is released in the following paper where you can find more details and baseline results:\nhttps://arxiv.org/abs/2007.02871\n",
            "url": "https://github.com/Yale-LILY/dart",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "tabular-to-text"
            ],
            "tasks": [
                "rdf-to-text"
            ]
        }
    },
    {
        "id": "datacommons_factcheck",
        "data": {
            "description": "A dataset of fact checked claims by news media maintained by datacommons.org\n",
            "url": "https://datacommons.org/factcheck/faq",
            "license": "CC-BY-NC-4.0",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "fact-checking"
            ]
        }
    },
    {
        "id": "dbpedia_14",
        "data": {
            "description": "The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping classes\nfrom DBpedia 2014. They are listed in classes.txt. From each of thse 14 ontology classes, we\nrandomly choose 40,000 training samples and 5,000 testing samples. Therefore, the total size\nof the training dataset is 560,000 and testing dataset 70,000.\nThere are 3 columns in the dataset (same for train and test splits), corresponding to class index\n(1 to 14), title and content. The title and content are escaped using double quotes (\"), and any\ninternal double quote is escaped by 2 double quotes (\"\"). There are no new lines in title or content.\n",
            "url": "https://wiki.dbpedia.org/develop/datasets",
            "license": "Creative Commons Attribution-ShareAlike 3.0 and the GNU Free Documentation License",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "topic-classification"
            ]
        }
    },
    {
        "id": "dbrd",
        "data": {
            "description": "The Dutch Book Review Dataset (DBRD) contains over 110k book reviews of which 22k have associated binary sentiment polarity labels. It is intended as a benchmark for sentiment classification in Dutch and created due to a lack of annotated datasets in Dutch that are suitable for this task.\n",
            "url": "https://github.com/benjaminvdb/DBRD",
            "license": "",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "nl"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "text-classification"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling",
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "deal_or_no_dialog",
        "data": {
            "description": "A large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other’s reward functions must reach anagreement (o a deal) via natural language dialogue.\n",
            "url": "https://github.com/facebookresearch/end-to-end-negotiator",
            "license": "The project is licenced under CC-by-NC",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "conversational"
            ],
            "tasks": []
        }
    },
    {
        "id": "definite_pronoun_resolution",
        "data": {
            "description": "Composed by 30 students from one of the author's undergraduate classes. These\nsentence pairs cover topics ranging from real events (e.g., Iran's plan to\nattack the Saudi ambassador to the U.S.) to events/characters in movies (e.g.,\nBatman) and purely imaginary situations, largely reflecting the pop culture as\nperceived by the American kids born in the early 90s. Each annotated example\nspans four lines: the first line contains the sentence, the second line contains\nthe target pronoun, the third line contains the two candidate antecedents, and\nthe fourth line contains the correct antecedent. If the target pronoun appears\nmore than once in the sentence, its first occurrence is the one to be resolved.\n",
            "url": "http://www.hlt.utdallas.edu/~vince/data/emnlp12/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "dengue_filipino",
        "data": {
            "description": "    Benchmark dataset for low-resource multiclass classification, with 4,015 training, 500 testing, and 500 validation examples, each labeled as part of five classes. Each sample can be a part of multiple classes. Collected as tweets.\n",
            "url": "https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "tl"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "dialog_re",
        "data": {
            "description": "DialogRE is the first human-annotated dialogue based relation extraction (RE) dataset aiming\nto support the prediction of relation(s) between two arguments that appear in a dialogue.\nThe dataset annotates all occurrences of 36 possible relation types that exist between pairs\nof arguments in the 1,788 dialogues originating from the complete transcripts of Friends.\n",
            "url": "https://github.com/nlpdata/dialogre",
            "license": "https://github.com/nlpdata/dialogre/blob/master/license.txt",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "other",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "relation-extraction",
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "diplomacy_detection",
        "data": {
            "description": "The Diplomacy dataset contains pairwise conversations annotated by the sender and the receiver for deception (and conversely truthfulness).   The 17,289 messages are gathered from 12 games. ",
            "url": "https://sites.google.com/view/qanta/projects/diplomacy",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification"
            ]
        }
    },
    {
        "id": "disaster_response_messages",
        "data": {
            "description": "This dataset contains 30,000 messages drawn from events including an earthquake in Haiti in 2010, an earthquake in Chile in 2010, floods in Pakistan in 2010, super-storm Sandy in the U.S.A. in 2012, and news articles spanning a large number of years and 100s of different disasters.\nThe data has been encoded with 36 different categories related to disaster response and has been stripped of messages with sensitive information in their entirety.\nUpon release, this is the featured dataset of a new Udacity course on Data Science and the AI4ALL summer school and is especially utile for text analytics and natural language processing (NLP) tasks and models.\nThe input data in this job contains thousands of untranslated disaster-related messages and their English translations.\n",
            "url": "https://appen.com/datasets/combined-disaster-response-data/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "es",
                "fr",
                "ht",
                "ur"
            ],
            "categories": [
                "text2text-generation",
                "text-classification"
            ],
            "tasks": [
                "intent-classification",
                "sentiment-classification",
                "text-simplification"
            ]
        }
    },
    {
        "id": "discofuse",
        "data": {
            "description": " DISCOFUSE is a large scale dataset for discourse-based sentence fusion.\n",
            "url": "https://github.com/google-research-datasets/discofuse",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "discovery",
        "data": {
            "description": "Discourse marker prediction with 174 different markers\n",
            "url": "",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-discourse-marker-prediction"
            ]
        }
    },
    {
        "id": "disfl_qa",
        "data": {
            "description": "Disfl-QA is a targeted dataset for contextual disfluencies in an information seeking setting,\nnamely question answering over Wikipedia passages. Disfl-QA builds upon the SQuAD-v2 (Rajpurkar et al., 2018)\ndataset, where each question in the dev set is annotated to add a contextual disfluency using the paragraph as\na source of distractors.\n\nThe final dataset consists of ~12k (disfluent question, answer) pairs. Over 90% of the disfluencies are\ncorrections or restarts, making it a much harder test set for disfluency correction. Disfl-QA aims to fill a\nmajor gap between speech and NLP research community. We hope the dataset can serve as a benchmark dataset for\ntesting robustness of models against disfluent inputs.\n\nOur expriments reveal that the state-of-the-art models are brittle when subjected to disfluent inputs from\nDisfl-QA. Detailed experiments and analyses can be found in our paper.\n",
            "url": "https://github.com/google-research-datasets/disfl-qa",
            "license": "Disfl-QA dataset is licensed under CC BY 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "doc2dial",
        "data": {
            "description": "Doc2dial is dataset of goal-oriented dialogues that are grounded in the associated documents. It includes over 4500 annotated conversations with an average of 14 turns that are grounded in over 450 documents from four domains. Compared to the prior document-grounded dialogue datasets this dataset covers a variety of dialogue scenes in information-seeking conversations.\n",
            "url": "https://doc2dial.github.io",
            "license": "",
            "givenLicense": "cc-by-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "closed-domain-qa"
            ]
        }
    },
    {
        "id": "docred",
        "data": {
            "description": "Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features:\n    - DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text.\n    - DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document.\n    - Along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios.\n",
            "url": "https://github.com/thunlp/DocRED",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text-retrieval"
            ],
            "tasks": [
                "entity-linking-retrieval"
            ]
        }
    },
    {
        "id": "doqa",
        "data": {
            "description": "\nDoQA is a dataset for accessing Domain Specific FAQs via conversational QA that contains 2,437 information-seeking question/answer dialogues \n(10,917 questions in total) on three different domains: cooking, travel and movies. Note that we include in the generic concept of FAQs also \nCommunity Question Answering sites, as well as corporate information in intranets which is maintained in textual form similar to FAQs, often \nreferred to as internal “knowledge bases”.\n\nThese dialogues are created by crowd workers that play the following two roles: the user who asks questions about a given topic posted in Stack \nExchange (https://stackexchange.com/), and the domain expert who replies to the questions by selecting a short span of text from the long textual \nreply in the original post. The expert can rephrase the selected span, in order to make it look more natural. The dataset covers unanswerable \nquestions and some relevant dialogue acts.\n\nDoQA enables the development and evaluation of conversational QA systems that help users access the knowledge buried in domain specific FAQs.\n",
            "url": "https://github.com/RevanthRameshkumar/CRD3",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "dream",
        "data": {
            "description": "DREAM is a multiple-choice Dialogue-based REAding comprehension exaMination dataset. In contrast to existing reading comprehension datasets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding.\n",
            "url": "https://dataset.org/dream/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "drop",
        "data": {
            "description": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs.\n. DROP is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a\nquestion, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or\n sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was\n necessary for prior datasets.\n",
            "url": "https://allennlp.org/drop",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering",
                "text2text-generation"
            ],
            "tasks": [
                "extractive-qa",
                "abstractive-qa"
            ]
        }
    },
    {
        "id": "duorc",
        "data": {
            "description": "DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\n",
            "url": "https://duorc.github.io/",
            "license": "https://raw.githubusercontent.com/duorc/duorc/master/LICENSE",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering",
                "text2text-generation"
            ],
            "tasks": [
                "abstractive-qa",
                "extractive-qa"
            ]
        }
    },
    {
        "id": "dutch_social",
        "data": {
            "description": "The dataset contains around 271,342 tweets. The tweets are filtered via the official Twitter API to\ncontain tweets in Dutch language or by users who have specified their location information within Netherlands\ngeographical boundaries. Using natural language processing we have classified the tweets for their HISCO codes.\nIf the user has provided their location within Dutch boundaries, we have also classified them to their respective\nprovinces The objective of this dataset is to make research data available publicly in a FAIR (Findable, Accessible,\nInteroperable, Reusable) way. Twitter's Terms of Service Licensed under Attribution-NonCommercial 4.0 International\n(CC BY-NC 4.0) (2020-10-27)\n",
            "url": "http://datasets.coronawhy.org/dataset.xhtml?persistentId=doi:10.5072/FK2/MTPTL7",
            "license": "CC BY-NC 4.0",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "en",
                "nl"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification",
                "multi-label-classification"
            ]
        }
    },
    {
        "id": "dyk",
        "data": {
            "description": "The Did You Know (pol. Czy wiesz?) dataset consists of human-annotated question-answer pairs. The task is to predict if the answer is correct. We chose the negatives which have the largest token overlap with a question.\n",
            "url": "http://nlp.pwr.wroc.pl/en/tools-and-resources/resources/czy-wiesz-question-answering-dataset",
            "license": "CC BY-SA 3.0",
            "givenLicense": "bsd-3-clause",
            "language": [
                "pl"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "e2e_nlg",
        "data": {
            "description": "The E2E dataset is used for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area.\nThe E2E dataset poses new challenges:\n(1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena;\n(2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances.\n\n\nE2E is released in the following paper where you can find more details and baseline results:\nhttps://arxiv.org/abs/1706.09254\n",
            "url": "http://www.macs.hw.ac.uk/InteractionLab/E2E/#data",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-meaning-representation-to-text"
            ]
        }
    },
    {
        "id": "e2e_nlg_cleaned",
        "data": {
            "description": "An update release of E2E NLG Challenge data with cleaned MRs and scripts, accompanying the following paper:\n\nOndřej Dušek, David M. Howcroft, and Verena Rieser (2019): Semantic Noise Matters for Neural Natural Language Generation. In INLG, Tokyo, Japan.\n",
            "url": "https://github.com/tuetschek/e2e-cleaning",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-meaning-representtion-to-text"
            ]
        }
    },
    {
        "id": "ecb",
        "data": {
            "description": "Original source: Website and documentatuion from the European Central Bank, compiled and made available by Alberto Simoes (thank you very much!)\n19 languages, 170 bitexts\ntotal number of files: 340\ntotal number of tokens: 757.37M\ntotal number of sentence fragments: 30.55M\n",
            "url": "",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "cs",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "fi",
                "fr",
                "hu",
                "it",
                "lt",
                "lv",
                "mt",
                "nl",
                "pl",
                "pt",
                "sk",
                "sl"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "ecthr_cases",
        "data": {
            "description": "The ECtHR Cases dataset is designed for experimentation of neural judgment prediction and rationale extraction considering ECtHR cases.\n",
            "url": "http://archive.org/details/ECtHR-NAACL2021/",
            "license": "CC BY-NC-SA (Creative Commons / Attribution-NonCommercial-ShareAlike)",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification",
                "text-classification-other-rationale-extraction",
                "text-classification-other-legal-judgment-prediction"
            ]
        }
    },
    {
        "id": "eduge",
        "data": {
            "description": "Eduge news classification dataset is provided by Bolorsoft LLC. It is used for training the Eduge.mn production news classifier\n75K news articles in 9 categories: урлаг соёл, эдийн засаг, эрүүл мэнд, хууль, улс төр, спорт, технологи, боловсрол and байгал орчин\n",
            "url": "http://eduge.mn",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "mn"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "ehealth_kd",
        "data": {
            "description": "Dataset of the eHealth Knowledge Discovery Challenge at IberLEF 2020. It is designed for\nthe identification of semantic entities and relations in Spanish health documents.\n",
            "url": "https://knowledge-learning.github.io/ehealthkd-2020/",
            "license": "https://creativecommons.org/licenses/by-nc-sa/4.0/",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "es"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition",
                "token-classification-other-relation-prediction"
            ]
        }
    },
    {
        "id": "eitb_parcc",
        "data": {
            "description": "EiTB-ParCC: Parallel Corpus of Comparable News. A Basque-Spanish parallel corpus provided by Vicomtech (https://www.vicomtech.org), extracted from comparable news produced by the Basque public broadcasting group Euskal Irrati Telebista.\n",
            "url": "http://opus.nlpl.eu/EiTB-ParCC.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "es",
                "eu"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "electricity_load_diagrams",
        "data": {
            "description": "This new dataset contains hourly kW electricity consumption time series of 370 Portuguese clients from 2011 to 2014.\n",
            "url": "https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "unknown"
            ],
            "categories": [
                "time-series-forecasting"
            ],
            "tasks": [
                "univariate-time-series-forecasting"
            ]
        }
    },
    {
        "id": "eli5",
        "data": {
            "description": "Explain Like I'm 5 long form QA dataset\n",
            "url": "https://facebookresearch.github.io/ELI5/explore.html",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "abstractive-qa",
                "open-domain-abstrative-qa"
            ]
        }
    },
    {
        "id": "eli5_category",
        "data": {
            "description": "The ELI5-Category dataset is a smaller but newer and categorized version of the original ELI5 dataset. After 2017, a tagging system was introduced to this subreddit so that the questions can be categorized into different topics according to their tags. Since the training and validation set is built by questions in different topics, the dataset is expected to alleviate the train/validation overlapping issue in the original ELI5 dataset.\n",
            "url": "",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "abstractive-qa",
                "open-domain-abstractive-qa"
            ]
        }
    },
    {
        "id": "elkarhizketak",
        "data": {
            "description": "\nElkarHizketak is a low resource conversational Question Answering\n(QA) dataset in Basque created by Basque speaker volunteers. The\ndataset contains close to 400 dialogues and more than 1600 question\nand answers, and its small size presents a realistic low-resource\nscenario for conversational QA systems. The dataset is built on top of\nWikipedia sections about popular people and organizations. The\ndialogues involve two crowd workers: (1) a student ask questions after\nreading a small introduction about the person, but without seeing the\nsection text; and (2) a teacher answers the questions selecting a span\nof text of the section.  ",
            "url": "http://ixa.si.ehu.es/node/12934",
            "license": "Creative Commons Attribution-ShareAlike 4.0 International Public License (CC BY-SA 4.0)",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "eu"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "question-ansering-other-dialogue-qa"
            ]
        }
    },
    {
        "id": "emea",
        "data": {
            "description": "This is a parallel corpus made out of PDF documents from the European Medicines Agency. All files are automatically converted from PDF to plain text using pdftotext with the command line arguments -layout -nopgbrk -eol unix. There are some known problems with tables and multi-column layouts - some of them are fixed in the current version.\n\nsource: http://www.emea.europa.eu/\n\n22 languages, 231 bitexts\ntotal number of files: 41,957\ntotal number of tokens: 311.65M\ntotal number of sentence fragments: 26.51M\n",
            "url": "http://opus.nlpl.eu/EMEA.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "bg",
                "cs",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "fi",
                "fr",
                "hu",
                "it",
                "lt",
                "lv",
                "mt",
                "nl",
                "pl",
                "pt",
                "ro",
                "sk",
                "sl",
                "sv"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "emo",
        "data": {
            "description": "In this dataset, given a textual dialogue i.e. an utterance along with two previous turns of context, the goal was to infer the underlying emotion of the utterance by choosing from four emotion classes - Happy, Sad, Angry and Others.\n",
            "url": "https://www.aclweb.org/anthology/S19-2005/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "emotion",
        "data": {
            "description": "Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. For more detailed information please refer to the paper.\n",
            "url": "https://github.com/dair-ai/emotion_dataset",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "text-classification-other-emotion-classification"
            ]
        }
    },
    {
        "id": "emotone_ar",
        "data": {
            "description": "Dataset of 10065 tweets in Arabic for Emotion detection in Arabic text",
            "url": "https://github.com/AmrMehasseb/Emotional-Tone",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "empathetic_dialogues",
        "data": {
            "description": "PyTorch original implementation of Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset\n",
            "url": "https://github.com/facebookresearch/EmpatheticDialogues",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "enriched_web_nlg",
        "data": {
            "description": "WebNLG is a valuable resource and benchmark for the Natural Language Generation (NLG) community. However, as other NLG benchmarks, it only consists of a collection of parallel raw representations and their corresponding textual realizations. This work aimed to provide intermediate representations of the data for the development and evaluation of popular tasks in the NLG pipeline architecture (Reiter and Dale, 2000), such as Discourse Ordering, Lexicalization, Aggregation and Referring Expression Generation.\n",
            "url": "https://github.com/ThiagoCF05/webnlg",
            "license": "CC Attribution-Noncommercial-Share Alike 4.0 International",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "de",
                "en"
            ],
            "categories": [
                "tabular-to-text"
            ],
            "tasks": [
                "rdf-to-text"
            ]
        }
    },
    {
        "id": "enwik8",
        "data": {
            "description": "The dataset is based on the Hutter Prize (http://prize.hutter1.net) and contains the first 10^8 byte of Wikipedia\n",
            "url": "https://cs.fit.edu/~mmahoney/compression/textdata.html",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "fill-mask",
                "text-generation"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "eraser_multi_rc",
        "data": {
            "description": "\nEraser Multi RC is a dataset for queries over multi-line passages, along with\nanswers and a rationalte. Each example in this dataset has the following 5 parts\n1. A Mutli-line Passage\n2. A Query about the passage\n3. An Answer to the query\n4. A Classification as to whether the answer is right or wrong\n5. An Explanation justifying the classification\n",
            "url": "https://cogcomp.seas.upenn.edu/multirc/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "esnli",
        "data": {
            "description": "\nThe e-SNLI dataset extends the Stanford Natural Language Inference Dataset to\ninclude human-annotated natural language explanations of the entailment\nrelations.\n",
            "url": "https://github.com/OanaMariaCamburu/e-SNLI",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "ethos",
        "data": {
            "description": "",
            "url": "https://github.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset/tree/masterethos/ethos_data",
            "license": "",
            "givenLicense": "agpl-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification",
                "sentiment-classification",
                "text-classification-other-Hate"
            ]
        }
    },
    {
        "id": "eth_py150_open",
        "data": {
            "description": "A redistributable subset of the ETH Py150 corpus, introduced in the ICML 2020 paper 'Learning and Evaluating Contextual Embedding of Source Code'\n",
            "url": "https://github.com/google-research-datasets/eth_py150_open",
            "license": "Apache License, Version 2.0",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "other"
            ],
            "tasks": [
                "other-other-contextual-embeddings"
            ]
        }
    },
    {
        "id": "ett",
        "data": {
            "description": "The data of Electricity Transformers from two separated counties\nin China collected for two years at hourly and 15-min frequencies.\nEach data point consists of the target value \"oil temperature\" and\n6 power load features. The train/val/test is 12/4/4 months.\n",
            "url": "https://github.com/zhouhaoyi/ETDataset",
            "license": "The Creative Commons Attribution 4.0 International License. https://creativecommons.org/licenses/by/4.0/",
            "givenLicense": "cc-by-4.0",
            "language": [
                "unknown"
            ],
            "categories": [
                "time-series-forecasting"
            ],
            "tasks": [
                "univariate-time-series-forecasting",
                "multivariate-time-series-forecasting"
            ]
        }
    },
    {
        "id": "eu_regulatory_ir",
        "data": {
            "description": "EURegIR: Regulatory Compliance IR (EU/UK)\n",
            "url": "https://archive.org/details/eacl2021_regir_dataset",
            "license": "CC BY-SA (Creative Commons / Attribution-ShareAlike)",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-retrieval"
            ],
            "tasks": [
                "document-retrieval",
                "text-retrieval-other-document-to-document-retrieval"
            ]
        }
    },
    {
        "id": "eurlex",
        "data": {
            "description": "EURLEX57K contains 57k legislative documents in English from EUR-Lex portal, annotated with EUROVOC concepts.\n",
            "url": "http://nlp.cs.aueb.gr/software_and_datasets/EURLEX57K/",
            "license": "CC BY-SA (Creative Commons / Attribution-ShareAlike)",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification",
                "text-classification-legal-topic-classification"
            ]
        }
    },
    {
        "id": "euronews",
        "data": {
            "description": "The corpora comprise of files per data provider that are encoded in the IOB format (Ramshaw & Marcus, 1995). The IOB format is a simple text chunking format that divides texts into single tokens per line, and, separated by a whitespace, tags to mark named entities. The most commonly used categories for tags are PER (person), LOC (location) and ORG (organization). To mark named entities that span multiple tokens, the tags have a prefix of either B- (beginning of named entity) or I- (inside of named entity). O (outside of named entity) tags are used to mark tokens that are not a named entity.\n",
            "url": "https://github.com/EuropeanaNewspapers/ner-corpora",
            "license": "",
            "givenLicense": "cc0-1.0",
            "language": [
                "de",
                "fr",
                "nl"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "europa_eac_tm",
        "data": {
            "description": "In October 2012, the European Union's (EU) Directorate General for Education and Culture ( DG EAC) released a translation memory (TM), i.e. a collection of sentences and their professionally produced translations, in twenty-six languages. This resource bears the name EAC Translation Memory, short EAC-TM.\n\nEAC-TM covers up to 26 languages: 22 official languages of the EU (all except Irish) plus Icelandic, Croatian, Norwegian and Turkish. EAC-TM thus contains translations from English into the following 25 languages: Bulgarian, Czech, Danish, Dutch, Estonian, German, Greek, Finnish, French, Croatian, Hungarian, Icelandic, Italian, Latvian, Lithuanian, Maltese, Norwegian, Polish, Portuguese, Romanian, Slovak, Slovenian, Spanish, Swedish and Turkish.\n\nAll documents and sentences were originally written in English (source language is English) and then translated into the other languages. The texts were translated by staff of the National Agencies of the Lifelong Learning and Youth in Action programmes. They are typically professionals in the field of education/youth and EU programmes. They are thus not professional translators, but they are normally native speakers of the target language.\n",
            "url": "https://ec.europa.eu/jrc/en/language-technologies/eac-translation-memory",
            "license": "Creative Commons Attribution 4.0 International(CC BY 4.0) licence © European Union, 1995-2020",
            "givenLicense": "cc-by-4.0",
            "language": [
                "bg",
                "cs",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "fi",
                "fr",
                "hr",
                "hu",
                "is",
                "it",
                "lt",
                "lv",
                "mt",
                "nl",
                "no",
                "pl",
                "pt",
                "ro",
                "sk",
                "sl",
                "sv",
                "tr"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "europa_ecdc_tm",
        "data": {
            "description": "In October 2012, the European Union (EU) agency 'European Centre for Disease Prevention and Control' (ECDC) released a translation memory (TM), i.e. a collection of sentences and their professionally produced translations, in twenty-five languages. This resource bears the name EAC Translation Memory, short EAC-TM.\nECDC-TM covers 25 languages: the 23 official languages of the EU plus Norwegian (Norsk) and Icelandic. ECDC-TM was created by translating from English into the following 24 languages: Bulgarian, Czech, Danish, Dutch, English, Estonian, Gaelige (Irish), German, Greek, Finnish, French, Hungarian, Icelandic, Italian, Latvian, Lithuanian, Maltese, Norwegian (NOrsk), Polish, Portuguese, Romanian, Slovak, Slovenian, Spanish and Swedish.\nAll documents and sentences were thus originally written in English. They were then translated into the other languages by professional translators from the Translation Centre CdT in Luxembourg.",
            "url": "https://ec.europa.eu/jrc/en/language-technologies/ecdc-translation-memory",
            "license": "Creative Commons Attribution 4.0 International(CC BY 4.0) licence Copyright © EU/ECDC, 1995-2020",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "bg",
                "cs",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "fi",
                "fr",
                "ga",
                "hu",
                "is",
                "it",
                "lt",
                "lv",
                "mt",
                "nl",
                "no",
                "pl",
                "pt",
                "ro",
                "sk",
                "sl",
                "sv"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "europarl_bilingual",
        "data": {
            "description": "A parallel corpus extracted from the European Parliament web site by Philipp Koehn (University of Edinburgh). The main intended use is to aid statistical machine translation research.\n",
            "url": "https://opus.nlpl.eu/Europarl.php",
            "license": "The data set comes with the same license\nas the original sources.\nPlease, check the information about the source\nthat is given on\nhttp://opus.nlpl.eu/Europarl-v8.php\n",
            "givenLicense": "unknown",
            "language": [
                "bg",
                "cs",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "fi",
                "fr",
                "hu",
                "it",
                "lt",
                "lv",
                "nl",
                "pl",
                "pt",
                "ro",
                "sk",
                "sl",
                "sv"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "event2Mind",
        "data": {
            "description": "In Event2Mind, we explore the task of understanding stereotypical intents and reactions to events. Through crowdsourcing, we create a large corpus with 25,000 events and free-form descriptions of their intents and reactions, both of the event's subject and (potentially implied) other participants.\n",
            "url": "https://uwnlp.github.io/event2mind/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "evidence_infer_treatment",
        "data": {
            "description": "Data and code from our \"Inferring Which Medical Treatments Work from Reports of Clinical Trials\", NAACL 2019. This work concerns inferring the results reported in clinical trials from text.\n\nThe dataset consists of biomedical articles describing randomized control trials (RCTs) that compare multiple treatments. Each of these articles will have multiple questions, or 'prompts' associated with them. These prompts will ask about the relationship between an intervention and comparator with respect to an outcome, as reported in the trial. For example, a prompt may ask about the reported effects of aspirin as compared to placebo on the duration of headaches. For the sake of this task, we assume that a particular article will report that the intervention of interest either significantly increased, significantly decreased or had significant effect on the outcome, relative to the comparator.\n\nThe dataset could be used for automatic data extraction of the results of a given RCT. This would enable readers to discover the effectiveness of different treatments without needing to read the paper.\n",
            "url": "https://github.com/jayded/evidence-inference",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text-retrieval"
            ],
            "tasks": [
                "fact-checking-retrieval"
            ]
        }
    },
    {
        "id": "exams",
        "data": {
            "description": "EXAMS is a benchmark dataset for multilingual and cross-lingual question answering from high school examinations.\nIt consists of more than 24,000 high-quality high school exam questions in 16 languages,\ncovering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others.\n",
            "url": "https://github.com/mhardalov/exams-qa",
            "license": "CC-BY-SA-4.0",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "ar",
                "bg",
                "de",
                "es",
                "fr",
                "hr",
                "hu",
                "it",
                "lt",
                "mk",
                "pl",
                "pt",
                "sq",
                "sr",
                "tr",
                "vi"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "factckbr",
        "data": {
            "description": "A dataset to study Fake News in Portuguese, presenting a supposedly false News along with their respective fact check and classification.\nThe data is collected from the ClaimReview, a structured data schema used by fact check agencies to share their results in search engines, enabling data collect in real time.\nThe FACTCK.BR dataset contains 1309 claims with its corresponding label.\n",
            "url": "https://github.com/jghm-f/FACTCK.BR",
            "license": "MIT",
            "givenLicense": "mit",
            "language": [
                "pt"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "fact-checking"
            ]
        }
    },
    {
        "id": "fake_news_english",
        "data": {
            "description": "\nFake news has become a major societal issue and a technical challenge for social media companies to identify. This content is difficult to identify because the term \"fake news\" covers intentionally false, deceptive stories as well as factual errors, satire, and sometimes, stories that a person just does not like. Addressing the problem requires clear definitions and examples. In this work, we present a dataset of fake news and satire stories that are hand coded, verified, and, in the case of fake news, include rebutting stories. We also include a thematic content analysis of the articles, identifying major themes that include hyperbolic support or condemnation of a gure, conspiracy theories, racist themes, and discrediting of reliable sources. In addition to releasing this dataset for research use, we analyze it and show results based on language that are promising for classification purposes. Overall, our contribution of a dataset and initial analysis are designed to support future work by fake news researchers.\n",
            "url": "https://dl.acm.org/doi/10.1145/3201064.3201100",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification"
            ]
        }
    },
    {
        "id": "fake_news_filipino",
        "data": {
            "description": "    Low-Resource Fake News Detection Corpora in Filipino. The first of its kind. Contains 3,206 expertly-labeled news samples, half of which are real and half of which are fake.\n",
            "url": "https://github.com/jcblaisecruz02/Tagalog-fake-news",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "tl"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "fact-checking"
            ]
        }
    },
    {
        "id": "farsi_news",
        "data": {
            "description": "\n",
            "url": "https://github.com/sci2lab/Farsi-datasets",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "fa"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "fashion_mnist",
        "data": {
            "description": "Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of\n60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image,\nassociated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in\nreplacement for the original MNIST dataset for benchmarking machine learning algorithms.\nIt shares the same image size and structure of training and testing splits.\n",
            "url": "https://github.com/zalandoresearch/fashion-mnist",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification"
            ],
            "tasks": [
                "multi-class-image-classification"
            ]
        }
    },
    {
        "id": "fever",
        "data": {
            "description": "FEVER  v1.0\nFEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment.",
            "url": "https://fever.ai/dataset/fever.html",
            "license": "",
            "givenLicense": "cc-by-sa-3.0,gpl-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-knowledge-verification"
            ]
        }
    },
    {
        "id": "few_rel",
        "data": {
            "description": "FewRel is a large-scale few-shot relation extraction dataset, which contains more than one hundred relations and tens of thousands of annotated instances cross different domains.\n",
            "url": "https://thunlp.github.io/",
            "license": "https://raw.githubusercontent.com/thunlp/FewRel/master/LICENSE",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "other"
            ],
            "tasks": [
                "relation-extraction"
            ]
        }
    },
    {
        "id": "financial_phrasebank",
        "data": {
            "description": "The key arguments for the low utilization of statistical techniques in\nfinancial sentiment analysis have been the difficulty of implementation for\npractical applications and the lack of high quality training data for building\nsuch models. Especially in the case of finance and economic texts, annotated\ncollections are a scarce resource and many are reserved for proprietary use\nonly. To resolve the missing training data problem, we present a collection of\n∼ 5000 sentences to establish human-annotated standards for benchmarking\nalternative modeling techniques.\n\nThe objective of the phrase level annotation task was to classify each example\nsentence into a positive, negative or neutral category by considering only the\ninformation explicitly available in the given sentence. Since the study is\nfocused only on financial and economic domains, the annotators were asked to\nconsider the sentences from the view point of an investor only; i.e. whether\nthe news may have positive, negative or neutral influence on the stock price.\nAs a result, sentences which have a sentiment that is not relevant from an\neconomic or financial perspective are considered neutral.\n\nThis release of the financial phrase bank covers a collection of 4840\nsentences. The selected collection of phrases was annotated by 16 people with\nadequate background knowledge on financial markets. Three of the annotators\nwere researchers and the remaining 13 annotators were master’s students at\nAalto University School of Business with majors primarily in finance,\naccounting, and economics.\n\nGiven the large number of overlapping annotations (5 to 8 annotations per\nsentence), there are several ways to define a majority vote based gold\nstandard. To provide an objective comparison, we have formed 4 alternative\nreference datasets based on the strength of majority agreement: all annotators\nagree, >=75% of annotators agree, >=66% of annotators agree and >=50% of\nannotators agree.\n",
            "url": "https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news",
            "license": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License",
            "givenLicense": "cc-by-nc-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "finer",
        "data": {
            "description": "The directory data contains a corpus of Finnish technology related news articles with a manually prepared\nnamed entity annotation (digitoday.2014.csv). The text material was extracted from the archives of Digitoday,\na Finnish online technology news source (www.digitoday.fi). The corpus consists of 953 articles\n(193,742 word tokens) with six named entity classes (organization, location, person, product, event, and date).\nThe corpus is available for research purposes and can be readily used for development of NER systems for Finnish.\n",
            "url": "https://github.com/mpsilfve/finer-data",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "fi"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "flores",
        "data": {
            "description": "Evaluation datasets for low-resource machine translation: Nepali-English and Sinhala-English.\n",
            "url": "https://github.com/facebookresearch/flores/",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en",
                "ne",
                "si"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "flue",
        "data": {
            "description": "FLUE is an evaluation setup for French NLP systems similar to the popular GLUE benchmark. The goal is to enable further reproducible experiments in the future and to share models and progress on the French language.\n",
            "url": "",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "fr"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification",
                "semantic-similarity-classification",
                "sentiment-classification",
                "text-classification-other-Word"
            ]
        }
    },
    {
        "id": "food101",
        "data": {
            "description": "This dataset consists of 101 food categories, with 101'000 images. For each class, 250 manually reviewed test images are provided as well as 750 training images. On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.",
            "url": "https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/",
            "license": "LICENSE AGREEMENT\n=================\n - The Food-101 data set consists of images from Foodspotting [1] which are not\n   property of the Federal Institute of Technology Zurich (ETHZ). Any use beyond\n   scientific fair use must be negociated with the respective picture owners\n   according to the Foodspotting terms of use [2].\n\n[1] http://www.foodspotting.com/\n[2] http://www.foodspotting.com/terms/\n",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification"
            ],
            "tasks": [
                "multi-class-image-classification"
            ]
        }
    },
    {
        "id": "fquad",
        "data": {
            "description": "FQuAD: French Question Answering Dataset\nWe introduce FQuAD, a native French Question Answering Dataset. FQuAD contains 25,000+ question and answer pairs.\nFinetuning CamemBERT on FQuAD yields a F1 score of 88% and an exact match of 77.9%.\n\n",
            "url": "https://fquad.illuin.tech/",
            "license": "",
            "givenLicense": "cc-by-nc-sa-3.0",
            "language": [
                "fr"
            ],
            "categories": [
                "question-answering",
                "text-retrieval"
            ],
            "tasks": [
                "extractive-qa",
                "closed-domain-qa"
            ]
        }
    },
    {
        "id": "freebase_qa",
        "data": {
            "description": "FreebaseQA is for open-domain factoid question answering (QA) tasks over structured knowledge bases, like Freebase The data set is generated by matching trivia-type question-answer pairs with subject-predicateobject triples in Freebase.\n",
            "url": "https://github.com/kelvin-jiang/FreebaseQA",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "gap",
        "data": {
            "description": "\nGAP is a gender-balanced dataset containing 8,908 coreference-labeled pairs of\n(ambiguous pronoun, antecedent name), sampled from Wikipedia and released by\nGoogle AI Language for the evaluation of coreference resolution in practical\napplications.\n",
            "url": "https://github.com/google-research-datasets/gap-coreference",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "gem",
        "data": {
            "description": "GEM is a benchmark environment for Natural Language Generation with a focus on its Evaluation,\nboth through human annotations and automated Metrics.\n\nGEM aims to:\n- measure NLG progress across 13 datasets spanning many NLG tasks and languages.\n- provide an in-depth analysis of data and models presented via data statements and challenge sets.\n- develop standards for evaluation of generated text using both automated and human metrics.\n\nIt is our goal to regularly update GEM and to encourage toward more inclusive practices in dataset development\nby extending existing data or developing datasets for additional languages.\n",
            "url": "https://gem-benchmark.github.io/",
            "license": "CC-BY-SA-4.0",
            "givenLicense": "other",
            "language": [
                "cs",
                "de",
                "en",
                "es",
                "ru",
                "tr",
                "vi"
            ],
            "categories": [
                "fill-mask",
                "summarization",
                "table-to-text",
                "tabular-to-text",
                "text-generation",
                "text2text-generation"
            ],
            "tasks": [
                "dialogue-modeling",
                "other-concepts-to-text",
                "other-intent-to-text",
                "rdf-to-text",
                "news-articles-summarization",
                "text-simplification",
                "text2text-generation-other-meaning-representation-to-text"
            ]
        }
    },
    {
        "id": "generated_reviews_enth",
        "data": {
            "description": " `generated_reviews_enth`\n Generated product reviews dataset for machine translation quality prediction, part of [scb-mt-en-th-2020](https://arxiv.org/pdf/2007.03541.pdf)\n `generated_reviews_enth` is created as part of [scb-mt-en-th-2020](https://arxiv.org/pdf/2007.03541.pdf) for machine translation task.\n This dataset (referred to as `generated_reviews_yn` in [scb-mt-en-th-2020](https://arxiv.org/pdf/2007.03541.pdf)) are English product reviews\n generated by [CTRL](https://arxiv.org/abs/1909.05858), translated by Google Translate API and annotated as accepted or rejected (`correct`)\n based on fluency and adequacy of the translation by human annotators.\n This allows it to be used for English-to-Thai translation quality esitmation (binary label), machine translation, and sentiment analysis.\n",
            "url": "https://github.com/vistec-AI/generated_reviews_enth",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en",
                "th"
            ],
            "categories": [
                "transkation",
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "semantic-similarity-classification"
            ]
        }
    },
    {
        "id": "generics_kb",
        "data": {
            "description": "The GenericsKB contains 3.4M+ generic sentences about the world, i.e., sentences expressing general truths such as \"Dogs bark,\" and \"Trees remove carbon dioxide from the atmosphere.\" Generics are potentially useful as a knowledge source for AI systems requiring general world knowledge. The GenericsKB is the first large-scale resource containing naturally occurring generic sentences (as opposed to extracted or crowdsourced triples), and is rich in high-quality, general, semantically complete statements. Generics were primarily extracted from three large text sources, namely the Waterloo Corpus, selected parts of Simple Wikipedia, and the ARC Corpus. A filtered, high-quality subset is also available in GenericsKB-Best, containing 1,020,868 sentences. We recommend you start with GenericsKB-Best.\n",
            "url": "https://allenai.org/data/genericskb",
            "license": "cc-by-4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "other",
                "conversational"
            ],
            "tasks": [
                "knowledge-base"
            ]
        }
    },
    {
        "id": "germaner",
        "data": {
            "description": "GermaNER is a freely available statistical German Named Entity Tagger based on conditional random fields(CRF). The tagger is trained and evaluated on the NoSta-D Named Entity dataset, which was used in the GermEval 2014 for named entity recognition. The tagger comes close to the performance of the best (proprietary) system in the competition with 77% F-measure (this is the latest result; the one reported in the paper is 76%) test set performance on the four standard NER classes (PERson, LOCation, ORGanisation and OTHer).\n\nWe describe a range of features and their influence on German NER classification and provide a comparative evaluation and some analysis of the results. The software components, the training data and all data used for feature generation are distributed under permissive licenses, thus this tagger can be used in academic and commercial settings without restrictions or fees. The tagger is available as a command-line tool and as an Apache UIMA component.\n",
            "url": "https://github.com/tudarmstadt-lt/GermaNER",
            "license": "",
            "givenLicense": "apache-2.0",
            "language": [
                "de"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "german_legal_entity_recognition",
        "data": {
            "description": "",
            "url": "https://github.com/elenanereiss/Legal-Entity-Recognition",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "de"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "germeval_14",
        "data": {
            "description": "The GermEval 2014 NER Shared Task builds on a new dataset with German Named Entity annotation with the following properties:    - The data was sampled from German Wikipedia and News Corpora as a collection of citations.    - The dataset covers over 31,000 sentences corresponding to over 590,000 tokens.    - The NER annotation uses the NoSta-D guidelines, which extend the Tübingen Treebank guidelines,      using four main NER categories with sub-structure, and annotating embeddings among NEs      such as [ORG FC Kickers [LOC Darmstadt]].\n",
            "url": "https://sites.google.com/site/germeval2014ner/",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "giga_fren",
        "data": {
            "description": "Giga-word corpus for French-English from WMT2010 collected by Chris Callison-Burch\n2 languages, total number of files: 452\ntotal number of tokens: 1.43G\ntotal number of sentence fragments: 47.55M\n",
            "url": "http://opus.nlpl.eu/giga-fren.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "fr"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "gigaword",
        "data": {
            "description": "\nHeadline-generation on a corpus of article pairs from Gigaword consisting of\naround 4 million articles. Use the 'org_data' provided by\nhttps://github.com/microsoft/unilm/ which is identical to\nhttps://github.com/harvardnlp/sent-summary but with better format.\n\nThere are two features:\n  - document: article.\n  - summary: headline.\n\n",
            "url": "https://github.com/harvardnlp/sent-summary",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "summarization--other-headline-generation"
            ]
        }
    },
    {
        "id": "glucose",
        "data": {
            "description": "When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context.\n",
            "url": "https://github.com/ElementalCognition/glucose",
            "license": "Creative Commons Attribution-NonCommercial 4.0 International Public License",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "text-generation",
                "fill-mask",
                "text-generation-other-common-sense-inference"
            ],
            "tasks": []
        }
    },
    {
        "id": "glue",
        "data": {
            "description": "GLUE, the General Language Understanding Evaluation benchmark\n(https://gluebenchmark.com/) is a collection of resources for training,\nevaluating, and analyzing natural language understanding systems.\n\n",
            "url": "https://nyu-mll.github.io/CoLA/",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "acceptability-classification",
                "natural-language-inference",
                "semantic-similarity-scoring",
                "sentiment-classification",
                "text-classification-other-coreference-nli",
                "text-classification-other-paraphrase-identification",
                "text-classification-other-qa-nli",
                "text-scoring"
            ]
        }
    },
    {
        "id": "gnad10",
        "data": {
            "description": "This dataset is intended to advance topic classification for German texts. A classifier that is efffective in\nEnglish may not be effective in German dataset because it has a higher inflection and longer compound words.\nThe 10kGNAD dataset contains 10273 German news articles from an Austrian online newspaper categorized into\n9 categories. Article titles and text are concatenated together and authors are removed to avoid a keyword-like\nclassification on authors that write frequently about one category. This dataset can be used as a benchmark\nfor German topic classification.\n",
            "url": "https://tblock.github.io/10kGNAD/",
            "license": "",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "de"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "topic-classification"
            ]
        }
    },
    {
        "id": "go_emotions",
        "data": {
            "description": "The GoEmotions dataset contains 58k carefully curated Reddit comments labeled for 27 emotion categories or Neutral.\nThe emotion categories are admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire,\ndisappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness,\noptimism, pride, realization, relief, remorse, sadness, surprise.\n",
            "url": "https://github.com/google-research/google-research/tree/master/goemotions",
            "license": "",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "multi-label-classification",
                "text-classification-other-emotion"
            ]
        }
    },
    {
        "id": "gooaq",
        "data": {
            "description": "GooAQ is a large-scale dataset with a variety of answer types. This dataset contains over\n5 million questions and 3 million answers collected from Google. GooAQ questions are collected\nsemi-automatically from the Google search engine using its autocomplete feature. This results in\nnaturalistic questions of practical interest that are nonetheless short and expressed using simple\nlanguage. GooAQ answers are mined from Google's responses to our collected questions, specifically from\nthe answer boxes in the search results. This yields a rich space of answer types, containing both\ntextual answers (short and long) as well as more structured ones such as collections.\n",
            "url": "https://github.com/allenai/gooaq",
            "license": "Licensed under the Apache License, Version 2.0",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "google_wellformed_query",
        "data": {
            "description": "Google's query wellformedness dataset was created by crowdsourcing well-formedness annotations for 25,100 queries from the Paralex corpus. Every query was annotated by five raters each with 1/0 rating of whether or not the query is well-formed.\n",
            "url": "https://github.com/google-research-datasets/query-wellformedness",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring"
            ]
        }
    },
    {
        "id": "grail_qa",
        "data": {
            "description": "Strongly Generalizable Question Answering (GrailQA) is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). It can be used to test three levels of generalization in KBQA: i.i.d., compositional, and zero-shot.\n",
            "url": "https://rajpurkar.github.io/SQuAD-explorer/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "question-answering-other-knowledge-base-qa"
            ]
        }
    },
    {
        "id": "great_code",
        "data": {
            "description": "The dataset for the variable-misuse task, described in the ICLR 2020 paper 'Global Relational Models of Source Code' [https://openreview.net/forum?id=B1lnbRNtwr]\n\nThis is the public version of the dataset used in that paper. The original, used to produce the graphs in the paper, could not be open-sourced due to licensing issues. See the public associated code repository [https://github.com/VHellendoorn/ICLR20-Great] for results produced from this dataset.\n\nThis dataset was generated synthetically from the corpus of Python code in the ETH Py150 Open dataset [https://github.com/google-research-datasets/eth_py150_open].\n",
            "url": "",
            "license": "",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "table-to-text"
            ],
            "tasks": []
        }
    },
    {
        "id": "greek_legal_code",
        "data": {
            "description": "Greek_Legal_Code contains 47k classified legal resources from Greek Legislation. Its origin is “Permanent Greek Legislation Code - Raptarchis”,\na collection of Greek legislative documents classified into multi-level (from broader to more specialized) categories.\n",
            "url": "https://doi.org/10.5281/zenodo.5528002",
            "license": "CC BY-SA (Creative Commons / Attribution-ShareAlike)",
            "givenLicense": "cc-by-4.0",
            "language": [
                "el"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "topic-classification"
            ]
        }
    },
    {
        "id": "gsm8k",
        "data": {
            "description": "GSM8K (Grade School Math 8K) is a dataset of 8.5K high quality\nlinguistically diverse grade school math word problems. The\ndataset was created to support the task of question answering\non basic mathematical problems that require multi-step reasoning.\n",
            "url": "https://openai.com/blog/grade-school-math",
            "license": "MIT",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-math-word-problems"
            ]
        }
    },
    {
        "id": "guardian_authorship",
        "data": {
            "description": "A dataset cross-topic authorship attribution. The dataset is provided by Stamatatos 2013. \n1- The cross-topic scenarios are based on Table-4 in Stamatatos 2017 (Ex. cross_topic_1 => row 1:P S U&W ).\n2- The cross-genre scenarios are based on Table-5 in the same paper. (Ex. cross_genre_1 => row 1:B P S&U&W).\n\n3- The same-topic/genre scenario is created by grouping all the datasts as follows. \nFor ex., to use same_topic and split the data 60-40 use:\ntrain_ds = load_dataset('guardian_authorship', name=\"cross_topic_<<#>>\", \n                        split='train[:60%]+validation[:60%]+test[:60%]')\ntests_ds = load_dataset('guardian_authorship', name=\"cross_topic_<<#>>\", \n                        split='train[-40%:]+validation[-40%:]+test[-40%:]')            \n\nIMPORTANT: train+validation+test[:60%] will generate the wrong splits becasue the data is imbalanced\n\n* See https://huggingface.co/docs/datasets/splits.html for detailed/more examples \n",
            "url": "http://www.icsd.aegean.gr/lecturers/stamatatos/papers/JLP2013.pdf",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "gutenberg_time",
        "data": {
            "description": "A clean data resource containing all explicit time references in a dataset of 52,183 novels whose full text is available via Project Gutenberg.\n",
            "url": "https://github.com/allenkim/what-time-is-it",
            "license": "[More Information needed]",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "hans",
        "data": {
            "description": "The HANS dataset is an NLI evaluation set that tests specific hypotheses about invalid heuristics that NLI models are likely to learn.\n",
            "url": "https://github.com/tommccoy1/hans",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference"
            ]
        }
    },
    {
        "id": "hansards",
        "data": {
            "description": "\nThis release contains 1.3 million pairs of aligned text chunks (sentences or smaller fragments)\nfrom the official records (Hansards) of the 36th Canadian Parliament.\n\nThe complete Hansards of the debates in the House and Senate of the 36th Canadian Parliament,\nas far as available, were aligned. The corpus was then split into 5 sets of sentence pairs:\ntraining (80% of the sentence pairs), two sets of sentence pairs for testing (5% each), and\ntwo sets of sentence pairs for final evaluation (5% each). The current release consists of the\ntraining and testing sets. The evaluation sets are reserved for future MT evaluation purposes\nand currently not available.\n\nCaveats\n1. This release contains only sentence pairs. Even though the order of the sentences is the same\nas in the original, there may be gaps resulting from many-to-one, many-to-many, or one-to-many\nalignments that were filtered out. Therefore, this release may not be suitable for\ndiscourse-related research. \n2. Neither the sentence splitting nor the alignments are perfect. In particular, watch out for\npairs that differ considerably in length. You may want to filter these out before you do\nany statistical training.\n\nThe alignment of the Hansards was performed as part of the ReWrite project under funding\nfrom the DARPA TIDES program.\n",
            "url": "https://www.isi.edu/natural-language/download/hansard/",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "hard",
        "data": {
            "description": "This dataset contains 93700 hotel reviews in Arabic language.The hotel reviews were collected from Booking.com website during June/July 2016.The reviews are expressed in Modern Standard Arabic as well as dialectal Arabic.The following table summarize some tatistics on the HARD Dataset.\n",
            "url": "https://github.com/elnagara/HARD-Arabic-Dataset",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "harem",
        "data": {
            "description": "\nThe HAREM is a Portuguese language corpus commonly used for Named Entity Recognition tasks. It includes about 93k words, from 129 different texts,\nfrom several genres, and language varieties. The split of this dataset version follows the division made by [1], where 7% HAREM\ndocuments are the validation set and the miniHAREM corpus (with about 65k words) is the test set. There are two versions of the dataset set,\na version that has a total of 10 different named entity classes (Person, Organization, Location, Value, Date, Title, Thing, Event,\nAbstraction, and Other) and a \"selective\" version with only 5 classes (Person, Organization, Location, Value, and Date).\n\nIt's important to note that the original version of the HAREM dataset has 2 levels of NER details, namely \"Category\" and \"Sub-type\".\nThe dataset version processed here ONLY USE the \"Category\" level of the original dataset.\n\n[1] Souza, Fábio, Rodrigo Nogueira, and Roberto Lotufo. \"BERTimbau: Pretrained BERT Models for Brazilian Portuguese.\" Brazilian Conference on Intelligent Systems. Springer, Cham, 2020.\n",
            "url": "https://www.linguateca.pt/primeiroHAREM/harem_coleccaodourada_en.html",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "pt"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "has_part",
        "data": {
            "description": "This dataset is a new knowledge-base (KB) of hasPart relationships, extracted from a large corpus of generic statements. Complementary to other resources available, it is the first which is all three of: accurate (90% precision), salient (covers relationships a person may mention), and has high coverage of common terms (approximated as within a 10 year old’s vocabulary), as well as having several times more hasPart entries than in the popular ontologies ConceptNet and WordNet. In addition, it contains information about quantifiers, argument modifiers, and links the entities to appropriate concepts in Wikipedia and WordNet.\n",
            "url": "https://allenai.org/data/haspartkb",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "text-classification-other-Meronym-Prediction"
            ]
        }
    },
    {
        "id": "hate_offensive",
        "data": {
            "description": "This dataset contains annotated tweets for automated hate-speech recognition",
            "url": "https://arxiv.org/abs/1905.12516",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "text-classification-other-hate-speech-detection"
            ]
        }
    },
    {
        "id": "hate_speech18",
        "data": {
            "description": "These files contain text extracted from Stormfront, a white supremacist forum. A random set of \nforums posts have been sampled from several subforums and split into sentences. Those sentences \nhave been manually labelled as containing hate speech or not, according to certain annotation guidelines.\n",
            "url": "https://github.com/Vicomtech/hate-speech-dataset",
            "license": "",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification"
            ]
        }
    },
    {
        "id": "hate_speech_filipino",
        "data": {
            "description": "    Contains 10k tweets (training set) that are labeled as hate speech or non-hate speech. Released with 4,232 validation and 4,232 testing samples. Collected during the 2016 Philippine Presidential Elections.\n",
            "url": "https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "tl"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-analysis"
            ]
        }
    },
    {
        "id": "hate_speech_offensive",
        "data": {
            "description": "An annotated dataset for hate speech and offensive language detection on tweets.\n",
            "url": "https://github.com/t-davidson/hate-speech-and-offensive-language",
            "license": "MIT",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-hate-speech-detection"
            ]
        }
    },
    {
        "id": "hate_speech_pl",
        "data": {
            "description": "HateSpeech corpus in the current version contains over 2000 posts crawled from public Polish web. They represent various types and degrees of offensive language, expressed toward minorities (eg. ethnical, racial). The data were annotated manually.\n",
            "url": "",
            "license": "CC BY-NC-SA",
            "givenLicense": "cc-by-nc-sa-3.0",
            "language": [
                "pl"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "multi-class-classification",
                "multi-label-classification",
                "sentiment-classification",
                "sentiment-scoring",
                "topic-classification"
            ]
        }
    },
    {
        "id": "hate_speech_portuguese",
        "data": {
            "description": "Portuguese dataset for hate speech detection composed of 5,668 tweets with binary annotations (i.e. 'hate' vs. 'no-hate').\n",
            "url": "https://github.com/paulafortuna/Portuguese-Hate-Speech-Dataset",
            "license": "Unknown",
            "givenLicense": "unknown",
            "language": [
                "pt"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-hate-speech-detection"
            ]
        }
    },
    {
        "id": "hatexplain",
        "data": {
            "description": "Hatexplain is the first benchmark hate speech dataset covering multiple aspects of the issue. Each post in the dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based.\n",
            "url": "",
            "license": "cc-by-4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-hate-speech-detection"
            ]
        }
    },
    {
        "id": "hausa_voa_ner",
        "data": {
            "description": "The Hausa VOA NER dataset is a labeled dataset for named entity recognition in Hausa. The texts were obtained from\nHausa Voice of America News articles https://www.voahausa.com/ . We concentrate on\nfour types of named entities: persons [PER], locations [LOC], organizations [ORG], and dates & time [DATE].\n\nThe Hausa VOA NER data files contain 2 columns separated by a tab ('\t'). Each word has been put on a separate line and\nthere is an empty line after each sentences i.e the CoNLL format. The first item on each line is a word, the second\nis the named entity tag. The named entity tags have the format I-TYPE which means that the word is inside a phrase\nof type TYPE. For every multi-word expression like 'New York', the first word gets a tag B-TYPE and the subsequent words\nhave tags I-TYPE, a word with tag O is not part of a phrase. The dataset is in the BIO tagging scheme.\n\nFor more details, see https://www.aclweb.org/anthology/2020.emnlp-main.204/\n",
            "url": "https://www.aclweb.org/anthology/2020.emnlp-main.204/",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "ha"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "hausa_voa_topics",
        "data": {
            "description": "A collection of news article headlines in Hausa from VOA Hausa.\nEach headline is labeled with one of the following classes: Nigeria,\nAfrica, World, Health or Politics.\n\nThe dataset was presented in the paper:\nHedderich, Adelani, Zhu, Alabi, Markus, Klakow: Transfer Learning and\nDistant Supervision for Multilingual Transformer Models: A Study on\nAfrican Languages (EMNLP 2020).\n",
            "url": "https://github.com/uds-lsv/transfer-distant-transformer-african",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ha"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "topic-classification"
            ]
        }
    },
    {
        "id": "hda_nli_hindi",
        "data": {
            "description": "This dataset is a recasted version of the Hindi Discourse Analysis Dataset used to train models for Natural Language Inference Tasks in Low-Resource Languages like Hindi.\n",
            "url": "https://github.com/midas-research/hindi-nli-data",
            "license": "\nMIT License\n\nCopyright (c) 2019 MIDAS, IIIT Delhi\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
            "givenLicense": "mit",
            "language": [
                "hi"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference"
            ]
        }
    },
    {
        "id": "head_qa",
        "data": {
            "description": "HEAD-QA is a multi-choice HEAlthcare Dataset. The questions come from exams to access a specialized position in the\nSpanish healthcare system, and are challenging even for highly specialized humans. They are designed by the Ministerio\nde Sanidad, Consumo y Bienestar Social.\n\nThe dataset contains questions about the following topics: medicine, nursing, psychology, chemistry, pharmacology and biology.\n",
            "url": "https://aghie.github.io/head-qa/",
            "license": "MIT License",
            "givenLicense": "mit",
            "language": [
                "en",
                "es"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "health_fact",
        "data": {
            "description": "PUBHEALTH is a comprehensive dataset for explainable automated fact-checking of\npublic health claims. Each instance in the PUBHEALTH dataset has an associated\nveracity label (true, false, unproven, mixture). Furthermore each instance in the\ndataset has an explanation text field. The explanation is a justification for which\nthe claim has been assigned a particular veracity label.\n\nThe dataset was created to explore fact-checking of difficult to verify claims i.e.,\nthose which require expertise from outside of the journalistics domain, in this case\nbiomedical and public health expertise.\n\nIt was also created in response to the lack of fact-checking datasets which provide\ngold standard natural language explanations for verdicts/labels.\n\nNOTE: There are missing labels in the dataset and we have replaced them with -1.\n",
            "url": "https://github.com/neemakot/Health-Fact-Checking/blob/master/data/DATASHEET.md",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "fact-checking",
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "hebrew_projectbenyehuda",
        "data": {
            "description": "This repository contains a dump of thousands of public domain works in Hebrew, from Project Ben-Yehuda, in plaintext UTF-8 files, with and without diacritics (nikkud). The metadata (pseudocatalogue.csv) file is a list of titles, authors, genres, and file paths, to help you process the dump.\nAll these works are in the public domain, so you are free to make any use of them, and do not need to ask for permission.\nThere are 10078 files, 3181136 lines\n",
            "url": "https://github.com/projectbenyehuda/public_domain_dump",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "he"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "hebrew_sentiment",
        "data": {
            "description": "HebrewSentiment is a data set consists of 12,804 user comments to posts on the official Facebook page of Israel’s\npresident, Mr. Reuven Rivlin. In October 2015, we used the open software application Netvizz (Rieder,\n2013) to scrape all the comments to all of the president’s posts in the period of June – August 2014,\nthe first three months of Rivlin’s presidency.2 While the president’s posts aimed at reconciling tensions\nand called for tolerance and empathy, the sentiment expressed in the comments to the president’s posts\nwas polarized between citizens who warmly thanked the president, and citizens that fiercely critiqued his\npolicy. Of the 12,804 comments, 370 are neutral; 8,512 are positive, 3,922 negative.\n\nData Annotation: A trained researcher examined each comment and determined its sentiment value,\nwhere comments with an overall positive sentiment were assigned the value 1, comments with an overall\nnegative sentiment were assigned the value -1, and comments that are off-topic to the post’s content\nwere assigned the value 0. We validated the coding scheme by asking a second trained researcher to\ncode the same data. There was substantial agreement between raters (N of agreements: 10623, N of\ndisagreements: 2105, Coehn’s Kappa = 0.697, p = 0).\n",
            "url": "https://github.com/omilab/Neural-Sentiment-Analyzer-for-Modern-Hebrew",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "he"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "hebrew_this_world",
        "data": {
            "description": "HebrewThisWorld is a data set consists of 2028 issues of the newspaper 'This World' edited by Uri Avnery and were published between 1950 and 1989. Released under the AGPLv3 license.",
            "url": "https://github.com/thisworld1/thisworld.online/",
            "license": "",
            "givenLicense": "agpl-3.0",
            "language": [
                "he"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "hellaswag",
        "data": {
            "description": "\n",
            "url": "https://rowanzellers.com/hellaswag/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "hendrycks_test",
        "data": {
            "description": "This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge, covering 57 tasks including elementary mathematics, US history, computer science, law, and more.\n",
            "url": "https://github.com/hendrycks/test",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en-US"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "hind_encorp",
        "data": {
            "description": "HindEnCorp parallel texts (sentence-aligned) come from the following sources:\nTides, which contains 50K sentence pairs taken mainly from news articles. This dataset was originally col- lected for the DARPA-TIDES surprise-language con- test in 2002, later refined at IIIT Hyderabad and provided for the NLP Tools Contest at ICON 2008 (Venkatapathy, 2008).\n\nCommentaries by Daniel Pipes contain 322 articles in English written by a journalist Daniel Pipes and translated into Hindi.\n\nEMILLE. This corpus (Baker et al., 2002) consists of three components: monolingual, parallel and annotated corpora. There are fourteen monolingual sub- corpora, including both written and (for some lan- guages) spoken data for fourteen South Asian lan- guages. The EMILLE monolingual corpora contain in total 92,799,000 words (including 2,627,000 words of transcribed spoken data for Bengali, Gujarati, Hindi, Punjabi and Urdu). The parallel corpus consists of 200,000 words of text in English and its accompanying translations into Hindi and other languages.\n\nSmaller datasets as collected by Bojar et al. (2010) include the corpus used at ACL 2005 (a subcorpus of EMILLE), a corpus of named entities from Wikipedia (crawled in 2009), and Agriculture domain parallel corpus.\n￼\nFor the current release, we are extending the parallel corpus using these sources:\nIntercorp (Čermák and Rosen,2012) is a large multilingual parallel corpus of 32 languages including Hindi. The central language used for alignment is Czech. Intercorp’s core texts amount to 202 million words. These core texts are most suitable for us because their sentence alignment is manually checked and therefore very reliable. They cover predominately short sto- ries and novels. There are seven Hindi texts in Inter- corp. Unfortunately, only for three of them the English translation is available; the other four are aligned only with Czech texts. The Hindi subcorpus of Intercorp contains 118,000 words in Hindi.\n\nTED talks 3 held in various languages, primarily English, are equipped with transcripts and these are translated into 102 languages. There are 179 talks for which Hindi translation is available.\n\nThe Indic multi-parallel corpus (Birch et al., 2011; Post et al., 2012) is a corpus of texts from Wikipedia translated from the respective Indian language into English by non-expert translators hired over Mechanical Turk. The quality is thus somewhat mixed in many respects starting from typesetting and punctuation over capi- talization, spelling, word choice to sentence structure. A little bit of control could be in principle obtained from the fact that every input sentence was translated 4 times. We used the 2012 release of the corpus.\n\nLaunchpad.net is a software collaboration platform that hosts many open-source projects and facilitates also collaborative localization of the tools. We downloaded all revisions of all the hosted projects and extracted the localization (.po) files.\n\nOther smaller datasets. This time, we added Wikipedia entities as crawled in 2013 (including any morphological variants of the named entitity that appears on the Hindi variant of the Wikipedia page) and words, word examples and quotes from the Shabdkosh online dictionary.\n",
            "url": "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0023-625F-0",
            "license": "CC BY-NC-SA 3.0",
            "givenLicense": "cc-by-nc-sa-3.0",
            "language": [
                "en",
                "hi"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "hindi_discourse",
        "data": {
            "description": "The Hindi Discourse Analysis dataset is a corpus for analyzing discourse modes present in its sentences. \nIt contains sentences from stories written by 11 famous authors from the 20th Century. \n4-5 stories by each author have been selected which were available in the public domain resulting \nin a collection of 53 stories. Most of these short stories were originally written in Hindi \nbut some of them were written in other Indian languages and later translated to Hindi.\n",
            "url": "https://github.com/midas-research/hindi-discourse",
            "license": "",
            "givenLicense": "other",
            "language": [
                "hi"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "text-generation",
                "fill-mask",
                "text-generation-other-discourse-analysis"
            ],
            "tasks": []
        }
    },
    {
        "id": "hippocorpus",
        "data": {
            "description": "To examine the cognitive processes of remembering and imagining and their traces in language, we introduce Hippocorpus, a dataset of 6,854 English diary-like short stories about recalled and imagined events. Using a crowdsourcing framework, we first collect recalled stories and summaries from workers, then provide these summaries to other workers who write imagined stories. Finally, months later, we collect a retold version of the recalled stories from a subset of recalled authors. Our dataset comes paired with author demographics (age, gender, race), their openness to experience, as well as some variables regarding the author's relationship to the event (e.g., how personal the event is, how often they tell its story, etc.).\n",
            "url": "https://msropendata.com/datasets/0a83fb6f-a759-4a17-aaa2-fbac84577318",
            "license": "",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "text-classification-other-narrative-flow"
            ]
        }
    },
    {
        "id": "hkcancor",
        "data": {
            "description": "The Hong Kong Cantonese Corpus (HKCanCor) comprise transcribed conversations\nrecorded between March 1997 and August 1998. It contains recordings of\nspontaneous speech (51 texts) and radio programmes (42 texts),\nwhich involve 2 to 4 speakers, with 1 text of monologue.\n\nIn total, the corpus contains around 230,000 Chinese words.\nThe text is word-segmented, annotated with part-of-speech (POS) tags and\nromanised Cantonese pronunciation.\n\nRomanisation scheme - Linguistic Society of Hong Kong (LSHK)\nPOS scheme - Peita-Fujitsu-Renmin Ribao (PRF) corpus (Duan et al., 2000),\n             with extended tags for Cantonese-specific phenomena added by\n             Luke and Wang (see original paper for details).\n",
            "url": "http://compling.hss.ntu.edu.sg/hkcancor/",
            "license": "CC BY 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "yue"
            ],
            "categories": [
                "translation",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "hlgd",
        "data": {
            "description": "HLGD is a binary classification dataset consisting of 20,056 labeled news headlines pairs indicating\nwhether the two headlines describe the same underlying world event or not.\n",
            "url": "https://github.com/tingofurro/headline_grouping",
            "license": "Apache-2.0 License",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-headline-grouping"
            ]
        }
    },
    {
        "id": "hope_edi",
        "data": {
            "description": "A Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not.\n",
            "url": "https://competitions.codalab.org/competitions/27653#learn_the_details",
            "license": "Creative Commons Attribution 4.0 International Licence",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en",
                "ml",
                "ta"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-hope-speech-classification"
            ]
        }
    },
    {
        "id": "hotpot_qa",
        "data": {
            "description": "HotpotQA is a new dataset with 113k  Wikipedia-based question-answer  pairs with  four  key  features:  (1)  the  questions  require finding and reasoning over multiple supporting  documents  to  answer;  (2)  the  questions  are  diverse  and  not  constrained  to  any pre-existing  knowledge  bases  or  knowledge schemas;  (3)  we  provide  sentence-level  supporting facts required for reasoning, allowingQA systems to reason with strong supervisionand explain the predictions; (4) we offer a new type  of  factoid  comparison  questions  to  testQA  systems’  ability  to  extract  relevant  facts and perform necessary comparison.\n",
            "url": "https://hotpotqa.github.io/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "hover",
        "data": {
            "description": "HoVer is an open-domain, many-hop fact extraction and claim verification dataset built upon the Wikipedia corpus. The original 2-hop claims are adapted from question-answer pairs from HotpotQA. It is collected by a team of NLP researchers at UNC Chapel Hill and Verisk Analytics.\n",
            "url": "https://hover-nlp.github.io/",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-retrieval"
            ],
            "tasks": [
                "fact-checking-retrieval"
            ]
        }
    },
    {
        "id": "hrenwac_para",
        "data": {
            "description": "The hrenWaC corpus version 2.0 consists of parallel Croatian-English texts crawled from the .hr top-level domain for Croatia. The corpus was built with Spidextor (https://github.com/abumatran/spidextor), a tool that glues together the output of SpiderLing used for crawling and Bitextor used for bitext extraction. The accuracy of the extracted bitext on the segment level is around 80% and on the word level around 84%.\n",
            "url": "http://nlp.ffzg.hr/resources/corpora/hrenwac/",
            "license": "CC BY-SA 3.0",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en",
                "hr"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "hrwac",
        "data": {
            "description": "The Croatian web corpus hrWaC was built by crawling the .hr top-level domain in 2011 and again in 2014. The corpus was near-deduplicated on paragraph level, normalised via diacritic restoration, morphosyntactically annotated and lemmatised. The corpus is shuffled by paragraphs. Each paragraph contains metadata on the URL, domain and language identification (Croatian vs. Serbian).\n\nVersion 2.0 of this corpus is described in http://www.aclweb.org/anthology/W14-0405. Version 2.1 contains newer and better linguistic annotations.\n",
            "url": "http://nlp.ffzg.hr/resources/corpora/hrwac/",
            "license": "CC BY-SA 4.0",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "hr"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "humicroedit",
        "data": {
            "description": "This new dataset is designed to assess the funniness of edited news headlines.\n",
            "url": "https://www.cs.rochester.edu/u/nhossain/humicroedit.html",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-funnier-headline-identification",
                "text-classification-other-funniness-score-prediction",
                "text-scoring"
            ]
        }
    },
    {
        "id": "hybrid_qa",
        "data": {
            "description": "Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA, a new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable.\n",
            "url": "https://github.com/wenhuchen/HybridQA",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "question-answering-other-multihop-tabular-text-qa"
            ]
        }
    },
    {
        "id": "hyperpartisan_news_detection",
        "data": {
            "description": "Hyperpartisan News Detection was a dataset created for PAN @ SemEval 2019 Task 4.\nGiven a news article text, decide whether it follows a hyperpartisan argumentation, i.e., whether it exhibits blind, prejudiced, or unreasoning allegiance to one party, faction, cause, or person.\n\nThere are 2 parts:\n- byarticle: Labeled through crowdsourcing on an article basis. The data contains only articles for which a consensus among the crowdsourcing workers existed.\n- bypublisher: Labeled by the overall bias of the publisher as provided by BuzzFeed journalists or MediaBiasFactCheck.com.\n",
            "url": "https://pan.webis.de/semeval19/semeval19-web/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "iapp_wiki_qa_squad",
        "data": {
            "description": "`iapp_wiki_qa_squad` is an extractive question answering dataset from Thai Wikipedia articles.\nIt is adapted from [the original iapp-wiki-qa-dataset](https://github.com/iapp-technology/iapp-wiki-qa-dataset)\nto [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) format, resulting in\n5761/742/739 questions from 1529/191/192 articles.\n",
            "url": "https://github.com/iapp-technology/iapp-wiki-qa-dataset/",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "th"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "id_clickbait",
        "data": {
            "description": "The CLICK-ID dataset is a collection of Indonesian news headlines that was collected from 12 local online news\npublishers; detikNews, Fimela, Kapanlagi, Kompas, Liputan6, Okezone, Posmetro-Medan, Republika, Sindonews, Tempo,\nTribunnews, and Wowkeren. This dataset is comprised of mainly two parts; (i) 46,119 raw article data, and (ii)\n15,000 clickbait annotated sample headlines. Annotation was conducted with 3 annotator examining each headline.\nJudgment were based only on the headline. The majority then is considered as the ground truth. In the annotated\nsample, our annotation shows 6,290 clickbait and 8,710 non-clickbait.\n",
            "url": "https://github.com/feryandi/Dataset-Artikel",
            "license": "Creative Commons Attribution 4.0 International license",
            "givenLicense": "cc-by-4.0",
            "language": [
                "id"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "fact-checking"
            ]
        }
    },
    {
        "id": "id_liputan6",
        "data": {
            "description": "In this paper, we introduce a large-scale Indonesian summarization dataset. We harvest articles from this http URL,\nan online news portal, and obtain 215,827 document-summary pairs. We leverage pre-trained language models to develop\nbenchmark extractive and abstractive summarization methods over the dataset with multilingual and monolingual\nBERT-based models. We include a thorough error analysis by examining machine-generated summaries that have\nlow ROUGE scores, and expose both issues with ROUGE it-self, as well as with extractive and abstractive\nsummarization models.\n",
            "url": "https://arxiv.org/abs/2011.00679",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "id"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "summarization-other-extractive-summarization",
                "news-articles-summarization"
            ]
        }
    },
    {
        "id": "id_nergrit_corpus",
        "data": {
            "description": "Nergrit Corpus is a dataset collection for Indonesian Named Entity Recognition, Statement Extraction, and Sentiment\nAnalysis. id_nergrit_corpus is the Named Entity Recognition of this dataset collection which contains 18 entities as\nfollow:\n    'CRD': Cardinal\n    'DAT': Date\n    'EVT': Event\n    'FAC': Facility\n    'GPE': Geopolitical Entity\n    'LAW': Law Entity (such as Undang-Undang)\n    'LOC': Location\n    'MON': Money\n    'NOR': Political Organization\n    'ORD': Ordinal\n    'ORG': Organization\n    'PER': Person\n    'PRC': Percent\n    'PRD': Product\n    'QTY': Quantity\n    'REG': Religion\n    'TIM': Time\n    'WOA': Work of Art\n    'LAN': Language\n",
            "url": "https://github.com/grit-id/nergrit-corpus",
            "license": "",
            "givenLicense": "other",
            "language": [
                "id"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "id_newspapers_2018",
        "data": {
            "description": "The dataset contains around 500K articles (136M of words) from 7 Indonesian newspapers: Detik, Kompas, Tempo,\nCNN Indonesia, Sindo, Republika and Poskota. The articles are dated between 1st January 2018 and 20th August 2018\n(with few exceptions dated earlier). The size of uncompressed 500K json files (newspapers-json.tgz) is around 2.2GB,\nand the cleaned uncompressed in a big text file (newspapers.txt.gz) is about 1GB. The original source in Google Drive\ncontains also a dataset in html format which include raw data (pictures, css, javascript, ...)\nfrom the online news website\n",
            "url": "https://github.com/feryandi/Dataset-Artikel",
            "license": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
            "givenLicense": "cc-by-4.0",
            "language": [
                "id"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "id_panl_bppt",
        "data": {
            "description": "Parallel Text Corpora for Multi-Domain Translation System created by BPPT (Indonesian Agency for the Assessment and\nApplication of Technology) for PAN Localization Project (A Regional Initiative to Develop Local Language Computing\nCapacity in Asia). The dataset contains around 24K sentences divided in 4 difference topics (Economic, international,\nScience and Technology and Sport).\n",
            "url": "http://digilib.bppt.go.id/sampul/p92-budiono.pdf",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "id"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "id_puisi",
        "data": {
            "description": "Puisi (poem) is an Indonesian poetic form. The dataset contains 7223 Indonesian puisi with its title and author.\n",
            "url": "https://github.com/ilhamfp/puisi-pantun-generator",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "id"
            ],
            "categories": [
                "text2text-generation",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "text2text-generation-other-poem-generation"
            ]
        }
    },
    {
        "id": "igbo_english_machine_translation",
        "data": {
            "description": "Parallel Igbo-English Dataset\n",
            "url": "https://github.com/IgnatiusEzeani/IGBONLP/tree/master/ig_en_mt",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "ig"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "igbo_monolingual",
        "data": {
            "description": "A dataset is a collection of Monolingual Igbo sentences.\n",
            "url": "https://github.com/IgnatiusEzeani/IGBONLP/tree/master/ig_monoling",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ig"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "igbo_ner",
        "data": {
            "description": "Igbo Named Entity Recognition Dataset\n",
            "url": "https://github.com/IgnatiusEzeani/IGBONLP/tree/master/ig_ner",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ig"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "ilist",
        "data": {
            "description": "This dataset is introduced in a task which aimed at identifying 5 closely-related languages of Indo-Aryan language family –\nHindi (also known as Khari Boli), Braj Bhasha, Awadhi, Bhojpuri, and Magahi.\n",
            "url": "https://github.com/kmi-linguistics/vardial2018",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "hi",
                "awa",
                "bho",
                "mag",
                "bra"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-language-identification"
            ]
        }
    },
    {
        "id": "imagenet-1k",
        "data": {
            "description": "ILSVRC 2012, commonly known as 'ImageNet' is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a \"synonym set\" or \"synset\". There are more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). ImageNet aims to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated. In its completion, ImageNet hopes to offer tens of millions of cleanly sorted images for most of the concepts in the WordNet hierarchy. ImageNet 2012 is the most commonly used subset of ImageNet. This dataset spans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and 100,000 test images\n",
            "url": "https://image-net.org/index.php",
            "license": "",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification"
            ],
            "tasks": [
                "multi-class-image-classification"
            ]
        }
    },
    {
        "id": "imagenet_sketch",
        "data": {
            "description": "ImageNet-Sketch data set consists of 50000 images, 50 images for each of the 1000 ImageNet classes.\nWe construct the data set with Google Image queries \"sketch of __\", where __ is the standard class name.\nWe only search within the \"black and white\" color scheme. We initially query 100 images for every class,\nand then manually clean the pulled images by deleting the irrelevant images and images that are for similar\nbut different classes. For some classes, there are less than 50 images after manually cleaning, and then we\naugment the data set by flipping and rotating the images.\n",
            "url": "https://github.com/HaohanWang/ImageNet-Sketch",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification"
            ],
            "tasks": [
                "multi-class-image-classification"
            ]
        }
    },
    {
        "id": "imdb",
        "data": {
            "description": "Large Movie Review Dataset.\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.",
            "url": "http://ai.stanford.edu/~amaas/data/sentiment/",
            "license": "",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "imdb_urdu_reviews",
        "data": {
            "description": "\nLarge Movie translated Urdu Reviews Dataset.\nThis is a dataset for binary sentiment classification containing substantially more data than previous\nbenchmark datasets. We provide a set of 40,000 highly polar movie reviews for training, and 10,000 for testing.\nTo increase the availability of sentiment analysis dataset for a low recourse language like Urdu,\nwe opted to use the already available IMDB Dataset. we have translated this dataset using google translator.\nThis is a binary classification dataset having two classes as positive and negative.\nThe reason behind using this dataset is high polarity for each class.\nIt contains 50k samples equally divided in two classes.\n",
            "url": "https://github.com/mirfan899/Urdu",
            "license": "",
            "givenLicense": "odbl",
            "language": [
                "ur"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "imppres",
        "data": {
            "description": "Over >25k semiautomatically generated sentence pairs illustrating well-studied pragmatic inference types. IMPPRES is an NLI dataset following the format of SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018) and XNLI (Conneau et al., 2018), which was created to evaluate how well trained NLI models recognize several classes of presuppositions and scalar implicatures.",
            "url": "https://github.com/facebookresearch/Imppres",
            "license": "Creative Commons Attribution-NonCommercial 4.0 International Public License",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference"
            ]
        }
    },
    {
        "id": "indic_glue",
        "data": {
            "description": "    IndicGLUE is a natural language understanding benchmark for Indian languages. It contains a wide\n    variety of tasks and covers 11 major Indian languages - as, bn, gu, hi, kn, ml, mr, or, pa, ta, te.\n\n\nThe Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task\nin which a system must read a sentence with a pronoun and select the referent of that pronoun from\na list of choices. The examples are manually constructed to foil simple statistical methods: Each\none is contingent on contextual information provided by a single word or phrase in the sentence.\nTo convert the problem into sentence pair classification, we construct sentence pairs by replacing\nthe ambiguous pronoun with each possible referent. The task is to predict if the sentence with the\npronoun substituted is entailed by the original sentence. We use a small evaluation set consisting of\nnew examples derived from fiction books that was shared privately by the authors of the original\ncorpus. While the included training set is balanced between two classes, the test set is imbalanced\nbetween them (65% not entailment). Also, due to a data quirk, the development set is adversarial:\nhypotheses are sometimes shared between training and development examples, so if a model memorizes the\ntraining examples, they will predict the wrong label on corresponding development set\nexample. As with QNLI, each example is evaluated separately, so there is not a systematic correspondence\nbetween a model's score on this task and its score on the unconverted original task. We\ncall converted dataset WNLI (Winograd NLI). This dataset is translated and publicly released for 3\nIndian languages by AI4Bharat.\n",
            "url": "https://indicnlp.ai4bharat.org/indic-glue/#natural-language-inference",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "indonli",
        "data": {
            "description": "  IndoNLI is the first human-elicited Natural Language Inference (NLI) dataset for Indonesian.\n  IndoNLI is annotated by both crowd workers and experts. The expert-annotated data is used exclusively as a test set.\n  It is designed to provide a challenging test-bed for Indonesian NLI by explicitly incorporating various linguistic phenomena such as numerical reasoning, structural changes, idioms, or temporal and spatial reasoning.\n",
            "url": "https://github.com/ir-nlp-csui/indonli",
            "license": "\n  CC BY-SA 4.0\n\n  Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\n  ShareAlike — If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\n\n  No additional restrictions — You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.\n\n",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "id"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference"
            ]
        }
    },
    {
        "id": "indonlu",
        "data": {
            "description": "An emotion classification dataset collected from the social media\nplatform Twitter (Saputri et al., 2018). The dataset consists of\naround 4000 Indonesian colloquial language tweets, covering five\ndifferent emotion labels: sadness, anger, love, fear, and happy.",
            "url": "https://www.indobenchmark.com/",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "id"
            ],
            "categories": [
                "question-answering",
                "text-classification",
                "token-classification"
            ],
            "tasks": [
                "closed-domain-qa",
                "multi-class-classification",
                "named-entity-recognition",
                "part-of-speech-tagging",
                "semantic-similarity-classification",
                "sentiment-classification",
                "text-classification-other-aspect-based-sentiment-analysis",
                "token-classification-other-keyphrase-extraction",
                "token-classification-other-span-extraction"
            ]
        }
    },
    {
        "id": "inquisitive_qg",
        "data": {
            "description": "A dataset of about 20k questions that are elicited from readers as they naturally read through a document sentence by sentence. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. Because these questions are generated while the readers are pro-cessing the information, the questions directly communicate gaps between the reader’s and writer’s knowledge about the events described in the text, and are not necessarily answered in the document itself. This type of question reflects a real-world scenario: if one has questions during reading, some of them are answered by the text later on, the rest are not, but any of them would help further the reader’s understanding at the particular point when they asked it. This resource could enable question generation models to simulate human-like curiosity and cognitive processing, which may open up a new realm of applications.\n",
            "url": "https://github.com/wjko2/INQUISITIVE",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-question-generation"
            ]
        }
    },
    {
        "id": "interpress_news_category_tr",
        "data": {
            "description": "It is a Turkish news data set consisting of 273601 news in 17 categories, compiled from print media and news websites between 2010 and 2017 by the Interpress (https://www.interpress.com/) media monitoring company.\n",
            "url": "https://www.interpress.com/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "tr"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-news-category-classification"
            ]
        }
    },
    {
        "id": "interpress_news_category_tr_lite",
        "data": {
            "description": "It is a Turkish news data set consisting of 273601 news in 10 categories, compiled from print media and news websites between 2010 and 2017 by the Interpress (https://www.interpress.com/) media monitoring company. It has been rearranged as easily separable and with fewer classes.\n",
            "url": "https://www.interpress.com/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "tr"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-news-category-classification"
            ]
        }
    },
    {
        "id": "irc_disentangle",
        "data": {
            "description": "Disentangling conversations mixed together in a single stream of messages is\na difficult task, made harder by the lack of large manually annotated\ndatasets. This new dataset of 77,563 messages manually annotated with\nreply-structure graphs that both disentangle conversations and define\ninternal conversation structure. The dataset is 16 times larger than all\npreviously released datasets combined, the first to include adjudication of\nannotation disagreements, and the first to include context.\n",
            "url": "https://jkk.name/irc-disentanglement/",
            "license": "Creative Commons Attribution 4.0 International Public License",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "token-classification-other-conversation-disentanglement"
            ]
        }
    },
    {
        "id": "isixhosa_ner_corpus",
        "data": {
            "description": "Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.\n",
            "url": "https://repo.sadilar.org/handle/20.500.12185/312",
            "license": "",
            "givenLicense": "other",
            "language": [
                "xh"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "isizulu_ner_corpus",
        "data": {
            "description": "Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.\n",
            "url": "https://repo.sadilar.org/handle/20.500.12185/319",
            "license": "",
            "givenLicense": "other",
            "language": [
                "zu"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "iwslt2017",
        "data": {
            "description": "The IWSLT 2017 Evaluation Campaign includes a multilingual TED Talks MT task. The languages involved are five:\n\n  German, English, Italian, Dutch, Romanian.\n\nFor each language pair, training and development sets are available through the entry of the table below: by clicking, an archive will be downloaded which contains the sets and a README file. Numbers in the table refer to millions of units (untokenized words) of the target side of all parallel training sets.\n",
            "url": "https://sites.google.com/site/iwsltevaluation2017/TED-tasks",
            "license": "",
            "givenLicense": "cc-by-nc-nd-4.0",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "jeopardy",
        "data": {
            "description": "\nDataset containing 216,930 Jeopardy questions, answers and other data.\n\nThe json file is an unordered list of questions where each question has\n'category' : the question category, e.g. \"HISTORY\"\n'value' : integer $ value of the question as string, e.g. \"200\"\nNote: This is \"None\" for Final Jeopardy! and Tiebreaker questions\n'question' : text of question\nNote: This sometimes contains hyperlinks and other things messy text such as when there's a picture or video question\n'answer' : text of answer\n'round' : one of \"Jeopardy!\",\"Double Jeopardy!\",\"Final Jeopardy!\" or \"Tiebreaker\"\nNote: Tiebreaker questions do happen but they're very rare (like once every 20 years)\n'show_number' : int of show number, e.g '4680'\n'air_date' : string of the show air date in format YYYY-MM-DD\n",
            "url": "https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "jfleg",
        "data": {
            "description": "JFLEG (JHU FLuency-Extended GUG) is an English grammatical error correction (GEC) corpus.\nIt is a gold standard benchmark for developing and evaluating GEC systems with respect to\nfluency (extent to which a text is native-sounding) as well as grammaticality.\n\nFor each source document, there are four human-written corrections (ref0 to ref3).\n",
            "url": "https://github.com/keisks/jfleg",
            "license": "CC BY-NC-SA 4.0",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-grammatical-error-correction"
            ]
        }
    },
    {
        "id": "jigsaw_toxicity_pred",
        "data": {
            "description": "This dataset consists of a large number of Wikipedia comments which have been labeled by human raters for toxic behavior.\n",
            "url": "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data",
            "license": "The \"Toxic Comment Classification\" dataset is released under CC0, with the underlying comment text being governed by Wikipedia's CC-SA-3.0.",
            "givenLicense": "cc0-1.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification"
            ]
        }
    },
    {
        "id": "jigsaw_unintended_bias",
        "data": {
            "description": "A collection of comments from the defunct Civil Comments platform that have been annotated for their toxicity.\n",
            "url": "https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/",
            "license": "CC0 (both the dataset and underlying text)",
            "givenLicense": "cc0-1.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "text-classification-other-toxicity-prediction"
            ]
        }
    },
    {
        "id": "jnlpba",
        "data": {
            "description": "The data came from the GENIA version 3.02 corpus (Kim et al., 2003). This was formed from a controlled search\non MEDLINE using the MeSH terms \u0018human\u0019, \u0018blood cells\u0019 and \u0018transcription factors\u0019. From this search 2,000 abstracts\nwere selected and hand annotated according to a small taxonomy of 48 classes based on a chemical classification.\nAmong the classes, 36 terminal classes were used to annotate the GENIA corpus.\n",
            "url": "http://www.geniaproject.org/shared-tasks/bionlp-jnlpba-shared-task-2004",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "journalists_questions",
        "data": {
            "description": "The journalists_questions corpus (version 1.0) is a collection of 10K human-written Arabic\ntweets manually labeled for question identification over Arabic tweets posted by journalists.\n",
            "url": "http://qufaculty.qu.edu.qa/telsayed/datasets/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-question-identification"
            ]
        }
    },
    {
        "id": "kan_hope",
        "data": {
            "description": "Numerous methods have been developed to monitor the spread of negativity in modern years by\neliminating vulgar, offensive, and fierce comments from social media platforms. However, there are relatively\nlesser amounts of study that converges on embracing positivity, reinforcing supportive and reassuring content in online forums.\nConsequently, we propose creating an English Kannada Hope speech dataset, KanHope and comparing several experiments to benchmark the dataset.\nThe dataset consists of 6,176 user generated comments in code mixed Kannada scraped from YouTube and manually annotated as bearing hope\nspeech or Not-hope speech.\nThis dataset was prepared for hope-speech text classification benchmark on code-mixed Kannada, an under-resourced language.\n",
            "url": "https://github.com/adeepH/kan_hope",
            "license": "Creative Commons Attribution 4.0 International Licence",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en-IN",
                "kn-IN"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification",
                "text-classification-other-Hope"
            ]
        }
    },
    {
        "id": "kannada_news",
        "data": {
            "description": "The Kannada news dataset contains only the headlines of news article in three categories:\nEntertainment, Tech, and Sports.\n\nThe data set contains around 6300 news article headlines which collected from Kannada news websites.\nThe data set has been cleaned and contains train and test set using which can be used to benchmark\nclassification models in Kannada.\n",
            "url": "",
            "license": "CC BY-SA 4.0",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "kn"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "topic-classification"
            ]
        }
    },
    {
        "id": "kd_conv",
        "data": {
            "description": "KdConv is a Chinese multi-domain Knowledge-driven Conversionsation dataset, grounding the topics in multi-turn conversations to knowledge graphs. KdConv contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics, while the corpus can also used for exploration of transfer learning and domain adaptation. \n",
            "url": "https://github.com/thu-coai/KdConv",
            "license": "Apache License 2.0",
            "givenLicense": "apache-2.0",
            "language": [
                "zh"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "kde4",
        "data": {
            "description": "A parallel corpus of KDE4 localization files (v.2).\n\n92 languages, 4,099 bitexts\ntotal number of files: 75,535\ntotal number of tokens: 60.75M\ntotal number of sentence fragments: 8.89M\n",
            "url": "http://opus.nlpl.eu/KDE4.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "af",
                "ar",
                "as",
                "ast",
                "be",
                "bg",
                "bn",
                "bn_IN",
                "br",
                "ca",
                "crh",
                "cs",
                "csb",
                "cy",
                "da",
                "de",
                "el",
                "en",
                "en_GB",
                "eo",
                "es",
                "et",
                "eu",
                "fa",
                "fi",
                "fr",
                "fy",
                "ga",
                "gl",
                "gu",
                "ha",
                "he",
                "hi",
                "hne",
                "hr",
                "hsb",
                "hu",
                "hy",
                "id",
                "is",
                "it",
                "ja",
                "ka",
                "kk",
                "km",
                "kn",
                "ko",
                "ku",
                "lb",
                "lt",
                "lv",
                "mai",
                "mk",
                "ml",
                "mr",
                "ms",
                "mt",
                "nb",
                "nds",
                "ne",
                "nl",
                "nn",
                "nso",
                "oc",
                "or",
                "pa",
                "pl",
                "ps",
                "pt",
                "pt_BR",
                "ro",
                "ru",
                "rw",
                "se",
                "si",
                "sk",
                "sl",
                "sr",
                "sv",
                "ta",
                "te",
                "tg",
                "th",
                "tr",
                "uk",
                "uz",
                "vi",
                "wa",
                "xh",
                "zh_CN",
                "zh_HK",
                "zh_TW"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "kelm",
        "data": {
            "description": "Data-To-Text Generation involves converting knowledge graph (KG) triples of the form (subject, relation, object) into\na natural language sentence(s). This dataset consists of English KG data converted into paired natural language text.\nThe generated corpus consists of ∼18M sentences spanning ∼45M triples with ∼1500 distinct relations.\n",
            "url": "https://github.com/google-research-datasets/KELM-corpus",
            "license": "",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "other"
            ],
            "tasks": [
                "other-other-data-to-text-generation"
            ]
        }
    },
    {
        "id": "kilt_tasks",
        "data": {
            "description": "KILT tasks training and evaluation data.\n- [FEVER](https://fever.ai) | Fact Checking | fever\n- [AIDA CoNLL-YAGO](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads) | Entity Linking | aidayago2\n- [WNED-WIKI](https://github.com/U-Alberta/wned) | Entity Linking | wned\n- [WNED-CWEB](https://github.com/U-Alberta/wned) | Entity Linking | cweb\n- [T-REx](https://hadyelsahar.github.io/t-rex) | Slot Filling | trex\n- [Zero-Shot RE](http://nlp.cs.washington.edu/zeroshot) | Slot Filling | structured_zeroshot\n- [Natural Questions](https://ai.google.com/research/NaturalQuestions) | Open Domain QA  | nq\n- [HotpotQA](https://hotpotqa.github.io) | Open Domain QA | hotpotqa\n- [TriviaQA](http://nlp.cs.washington.edu/triviaqa) | Open Domain QA | triviaqa\n- [ELI5](https://facebookresearch.github.io/ELI5/explore.html) | Open Domain QA | eli5\n- [Wizard of Wikipedia](https://parl.ai/projects/wizard_of_wikipedia) | Dialogue | wow\n\nTo finish linking TriviaQA questions to the IDs provided, follow the instructions [here](http://github.com/huggingface/datasets/datasets/kilt_tasks/README.md).\n",
            "url": "https://github.com/facebookresearch/KILT",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "fill-mask",
                "question-answering",
                "text-classification",
                "text-generation",
                "text-retrieval",
                "text2text-generation"
            ],
            "tasks": [
                "abstractive-qa",
                "dialogue-modeling",
                "document-retrieval",
                "entity-linking-retrieval",
                "extractive-qa",
                "fact-checking",
                "fact-checking-retrieval",
                "open-domain-abstractive-qa",
                "open-domain-qa",
                "slot-filling"
            ]
        }
    },
    {
        "id": "kilt_wikipedia",
        "data": {
            "description": "KILT-Wikipedia: Wikipedia pre-processed for KILT.\n",
            "url": "https://github.com/facebookresearch/KILT",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "kinnews_kirnews",
        "data": {
            "description": "Kinyarwanda and Kirundi news classification datasets\n",
            "url": "https://github.com/Andrews2017/KINNEWS-and-KIRNEWS-Corpus",
            "license": "MIT License",
            "givenLicense": "mit",
            "language": [
                "rn",
                "rw"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "topic-classification"
            ]
        }
    },
    {
        "id": "klue",
        "data": {
            "description": "KLUE (Korean Language Understanding Evaluation)\nKorean Language Understanding Evaluation (KLUE) benchmark is a series of datasets to evaluate natural language\nunderstanding capability of Korean language models. KLUE consists of 8 diverse and representative tasks, which are accessible\nto anyone without any restrictions. With ethical considerations in mind, we deliberately design annotation guidelines to obtain\nunambiguous annotations for all datasets. Futhermore, we build an evaluation system and carefully choose evaluations metrics\nfor every task, thus establishing fair comparison across Korean language models.\n",
            "url": "https://klue-benchmark.com/tasks/66/overview/description",
            "license": "CC-BY-SA-4.0",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "ko"
            ],
            "categories": [
                "fill-mask",
                "question-answering",
                "text-classification",
                "text-generation",
                "token-classification",
                "natural-language-processing"
            ],
            "tasks": [
                "extractive-qa",
                "named-entity-recognition",
                "natural-language-inference",
                "other-dialogue-state-tracking",
                "parsing",
                "semantic-similarity-scoring",
                "text-scoring",
                "token-classification-other-relation-extraction",
                "topic-classification"
            ]
        }
    },
    {
        "id": "kor_3i4k",
        "data": {
            "description": "This dataset is designed to identify speaker intention based on real-life spoken utterance in Korean into one of\n7 categories: fragment, description, question, command, rhetorical question, rhetorical command, utterances.\n",
            "url": "https://github.com/warnikchow/3i4k",
            "license": "CC BY-SA-4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "ko"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification"
            ]
        }
    },
    {
        "id": "kor_hate",
        "data": {
            "description": "Human-annotated Korean corpus collected from a popular domestic entertainment news aggregation platform\nfor toxic speech detection. Comments are annotated for gender bias, social bias and hate speech. \n",
            "url": "https://github.com/kocohub/korean-hate-speech",
            "license": "Creative Commons",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "ko"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification"
            ]
        }
    },
    {
        "id": "kor_ner",
        "data": {
            "description": "Korean named entity recognition dataset\n",
            "url": "https://github.com/kmounlp/NER",
            "license": "NER License, MIT License for non-commercial use",
            "givenLicense": "mit",
            "language": [
                "ko"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "kor_nli",
        "data": {
            "description": " Korean Natural  Language Inference datasets\n",
            "url": "https://github.com/kakaobrain/KorNLUDatasets",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "kor_nlu",
        "data": {
            "description": "    The dataset contains data for bechmarking korean models on NLI and STS\n",
            "url": "https://github.com/kakaobrain/KorNLUDatasets",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "ko"
            ],
            "categories": [
                "text-classification",
                "text-scoring"
            ],
            "tasks": [
                "natural-language-inference",
                "semantic-similarity-scoring"
            ]
        }
    },
    {
        "id": "kor_qpair",
        "data": {
            "description": "This is a Korean paired question dateset containing labels that denote whether two questions in a given pair are semantically identical.\n",
            "url": "https://github.com/songys/Question_pair",
            "license": "The MIT License (MIT)",
            "givenLicense": "mit",
            "language": [
                "ko"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "semantic-similarity-classification"
            ]
        }
    },
    {
        "id": "kor_sae",
        "data": {
            "description": "This new dataset is designed to extract intent from non-canonical directives which will help dialog managers\nextract intent from user dialog that may have no clear objective or are paraphrased forms of utterances.\n",
            "url": "https://github.com/warnikchow/sae4k",
            "license": "CC-BY-SA-4.0",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "ko"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification"
            ]
        }
    },
    {
        "id": "kor_sarcasm",
        "data": {
            "description": "This is a dataset designed to detect sarcasm in Korean because it distorts the literal meaning of a sentence\nand is highly related to sentiment classification.\n",
            "url": "https://github.com/SpellOnYou/korean-sarcasm",
            "license": "MIT License",
            "givenLicense": "mit",
            "language": [
                "ko"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sarcasm-detection"
            ]
        }
    },
    {
        "id": "labr",
        "data": {
            "description": "This dataset contains over 63,000 book reviews in Arabic.It is the largest sentiment analysis dataset for Arabic to-date.The book reviews were harvested from the website Goodreads during the month or March 2013.Each book review comes with the goodreads review id, the user id, the book id, the rating (1 to 5) and the text of the review.\n",
            "url": "https://github.com/mohamedadaly/LABR",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "lama",
        "data": {
            "description": "LAMA is a dataset used to probe and analyze the factual and commonsense knowledge contained in pretrained language models. See https://github.com/facebookresearch/LAMA.\n",
            "url": "https://github.com/facebookresearch/LAMA",
            "license": "The Creative Commons Attribution-Noncommercial 4.0 International License. see https://github.com/facebookresearch/LAMA/blob/master/LICENSE",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-retrieval",
                "text-classification"
            ],
            "tasks": [
                "fact-checking-retrieval",
                "text-classification-other-probing",
                "text-scoring"
            ]
        }
    },
    {
        "id": "lambada",
        "data": {
            "description": "\nThe LAMBADA evaluates the capabilities of computational models\nfor text understanding by means of a word prediction task.\nLAMBADA is a collection of narrative passages sharing the characteristic\nthat human subjects are able to guess their last word if\nthey are exposed to the whole passage, but not if they\nonly see the last sentence preceding the target word.\nTo succeed on LAMBADA, computational models cannot\nsimply rely on local context, but must be able to\nkeep track of information in the broader discourse.\n\nThe LAMBADA dataset is extracted from BookCorpus and\nconsists of 10'022 passages, divided into 4'869 development\nand 5'153 test passages. The training data for language\nmodels to be tested on LAMBADA include the full text\nof 2'662 novels (disjoint from those in dev+test),\ncomprising 203 million words.\n",
            "url": "https://zenodo.org/record/2630551#.X8UP76pKiIa",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-long-range-dependency"
            ]
        }
    },
    {
        "id": "large_spanish_corpus",
        "data": {
            "description": "The Large Spanish Corpus is a compilation of 15 unlabelled Spanish corpora spanning Wikipedia to European parliament notes. Each config contains the data corresponding to a different corpus. For example, \"all_wiki\" only includes examples from Spanish Wikipedia. By default, the config is set to \"combined\" which loads all the corpora; with this setting you can also specify the number of samples to return per corpus by configuring the \"split\" argument.\n",
            "url": "https://github.com/josecannete/spanish-corpora",
            "license": "MIT",
            "givenLicense": "mit",
            "language": [
                "es"
            ],
            "categories": [
                "other"
            ],
            "tasks": [
                "other-other-pretraining-language-models"
            ]
        }
    },
    {
        "id": "laroseda",
        "data": {
            "description": "        LaRoSeDa (A Large Romanian Sentiment Data Set) contains 15,000 reviews written in Romanian, of which 7,500 are positive and 7,500 negative.\n        Star ratings of 1 and 2 and of 4 and 5 are provided for negative and positive reviews respectively.\n        The current dataset uses star rating as the label for multi-class classification.\n",
            "url": "https://github.com/ancatache/LaRoSeDa",
            "license": "CC BY-SA 4.0 License",
            "givenLicense": "cc-by-4.0",
            "language": [
                "ro"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "lccc",
        "data": {
            "description": "LCCC: Large-scale Cleaned Chinese Conversation corpus (LCCC) is a large corpus of Chinese conversations.\nA rigorous data cleaning pipeline is designed to ensure the quality of the corpus.\nThis pipeline involves a set of rules and several classifier-based filters.\nNoises such as offensive or sensitive words, special symbols, emojis,\ngrammatically incorrect sentences, and incoherent conversations are filtered.\n",
            "url": "https://github.com/thu-coai/CDial-GPT",
            "license": "MIT",
            "givenLicense": "mit",
            "language": [
                "zh"
            ],
            "categories": [
                "conversational"
            ],
            "tasks": [
                "dialogue-generation"
            ]
        }
    },
    {
        "id": "lc_quad",
        "data": {
            "description": "LC-QuAD 2.0 is a Large Question Answering dataset with 30,000 pairs of question and its corresponding SPARQL query. The target knowledge base is Wikidata and DBpedia, specifically the 2018 version. Please see our paper for details about the dataset creation process and framework.\n",
            "url": "http://lc-quad.sda.tech/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "lener_br",
        "data": {
            "description": "\nLeNER-Br is a Portuguese language dataset for named entity recognition \napplied to legal documents. LeNER-Br consists entirely of manually annotated \nlegislation and legal cases texts and contains tags for persons, locations, \ntime entities, organizations, legislation and legal cases.\nTo compose the dataset, 66 legal documents from several Brazilian Courts were\ncollected. Courts of superior and state levels were considered, such as Supremo\nTribunal Federal, Superior Tribunal de Justiça, Tribunal de Justiça de Minas\nGerais and Tribunal de Contas da União. In addition, four legislation documents\nwere collected, such as \"Lei Maria da Penha\", giving a total of 70 documents\n",
            "url": "https://cic.unb.br/~teodecampos/LeNER-Br/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "pt"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "lex_glue",
        "data": {
            "description": "The European Court of Human Rights (ECtHR) hears allegations that a state has\nbreached human rights provisions of the European Convention of Human Rights (ECHR).\nFor each case, the dataset provides a list of factual paragraphs (facts) from the case description.\nEach case is mapped to articles of the ECHR that were violated (if any).",
            "url": "https://archive.org/details/ECtHR-NAACL2021",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering",
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "multi-label-classification",
                "multiple-choice-qa",
                "topic-classification"
            ]
        }
    },
    {
        "id": "liar",
        "data": {
            "description": "LIAR is a dataset for fake news detection with 12.8K human labeled short statements from politifact.com's API, and each statement is evaluated by a politifact.com editor for its truthfulness. The distribution of labels in the LIAR dataset is relatively well-balanced: except for 1,050 pants-fire cases, the instances for all other labels range from 2,063 to 2,638. In each case, the labeler provides a lengthy analysis report to ground each judgment.\n",
            "url": "https://www.aclweb.org/anthology/P17-2067",
            "license": "Unknown",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-fake-news-detection"
            ]
        }
    },
    {
        "id": "librispeech_asr",
        "data": {
            "description": "LibriSpeech is a corpus of approximately 1000 hours of read English speech with sampling rate of 16 kHz,\nprepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read\naudiobooks from the LibriVox project, and has been carefully segmented and aligned.87\n\nNote that in order to limit the required storage for preparing this dataset, the audio\nis stored in the .flac format and is not converted to a float32 array. To convert, the audio\nfile to a float32 array, please make use of the `.map()` function as follows:\n\n\n```python\nimport soundfile as sf\n\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n",
            "url": "http://www.openslr.org/12",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "automatic-speech-recognition",
                "audio-classification"
            ],
            "tasks": [
                "speaker-identification"
            ]
        }
    },
    {
        "id": "librispeech_lm",
        "data": {
            "description": "Language modeling resources to be used in conjunction with the LibriSpeech ASR corpus.\n",
            "url": "http://www.openslr.org/11",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "limit",
        "data": {
            "description": "Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. Literal-Motion-in-Text (LiMiT) dataset, is a large human-annotated collection of English text sentences describing physical occurrence of motion, with annotated physical entities in motion.\n",
            "url": "https://github.com/ilmgut/limit_dataset",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification",
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "lince",
        "data": {
            "description": "LinCE is a centralized Linguistic Code-switching Evaluation benchmark\n(https://ritual.uh.edu/lince/) that contains data for training and evaluating\nNLP systems on code-switching tasks.\n",
            "url": "http://ritual.uh.edu/lince",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "linnaeus",
        "data": {
            "description": "A novel corpus of full-text documents manually annotated for species mentions.\n",
            "url": "http://linnaeus.sourceforge.net/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "liveqa",
        "data": {
            "description": "This is LiveQA, a Chinese dataset constructed from play-by-play live broadcast.\nIt contains 117k multiple-choice questions written by human commentators for over 1,670 NBA games,\nwhich are collected from the Chinese Hupu website.\n",
            "url": "https://github.com/PKU-TANGENT/LiveQA",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "zh"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "lj_speech",
        "data": {
            "description": "This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading \npassages from 7 non-fiction books in English. A transcription is provided for each clip. Clips vary in length \nfrom 1 to 10 seconds and have a total length of approximately 24 hours.\n\nNote that in order to limit the required storage for preparing this dataset, the audio\nis stored in the .wav format and is not converted to a float32 array. To convert the audio\nfile to a float32 array, please make use of the `.map()` function as follows:\n\n\n```python\nimport soundfile as sf\n\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n",
            "url": "https://keithito.com/LJ-Speech-Dataset/",
            "license": "",
            "givenLicense": "unlicense",
            "language": [
                "en"
            ],
            "categories": [
                "automatic-speech-recognition"
            ],
            "tasks": []
        }
    },
    {
        "id": "lm1b",
        "data": {
            "description": "A benchmark corpus to be used for measuring progress in statistical language modeling. This has almost one billion words in the training data.\n",
            "url": "http://www.statmt.org/lm-benchmark/",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "lst20",
        "data": {
            "description": "LST20 Corpus is a dataset for Thai language processing developed by National Electronics and Computer Technology Center (NECTEC), Thailand.\nIt offers five layers of linguistic annotation: word boundaries, POS tagging, named entities, clause boundaries, and sentence boundaries.\nAt a large scale, it consists of 3,164,002 words, 288,020 named entities, 248,181 clauses, and 74,180 sentences, while it is annotated with\n16 distinct POS tags. All 3,745 documents are also annotated with one of 15 news genres. Regarding its sheer size, this dataset is\nconsidered large enough for developing joint neural models for NLP.\nManually download at https://aiforthai.in.th/corpus.php\n",
            "url": "https://aiforthai.in.th/",
            "license": "",
            "givenLicense": "other",
            "language": [
                "th"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition",
                "part-of-speech-tagging",
                "token-classification-other-clause-segmentation",
                "token-classification-other-sentence-segmentation",
                "token-classification-other-word-segmentation"
            ]
        }
    },
    {
        "id": "mac_morpho",
        "data": {
            "description": "\nMac-Morpho is a corpus of Brazilian Portuguese texts annotated with part-of-speech tags.\nIts first version was released in 2003 [1], and since then, two revisions have been made in order\nto improve the quality of the resource [2, 3].\nThe corpus is available for download split into train, development and test sections.\nThese are 76%, 4% and 20% of the corpus total, respectively (the reason for the unusual numbers\nis that the corpus was first split into 80%/20% train/test, and then 5% of the train section was\nset aside for development). This split was used in [3], and new POS tagging research with Mac-Morpho\nis encouraged to follow it in order to make consistent comparisons possible.\n\n\n[1] Aluísio, S., Pelizzoni, J., Marchi, A.R., de Oliveira, L., Manenti, R., Marquiafável, V. 2003.\nAn account of the challenge of tagging a reference corpus for brazilian portuguese.\nIn: Proceedings of the 6th International Conference on Computational Processing of the Portuguese Language. PROPOR 2003\n\n[2] Fonseca, E.R., Rosa, J.L.G. 2013. Mac-morpho revisited: Towards robust part-of-speech.\nIn: Proceedings of the 9th Brazilian Symposium in Information and Human Language Technology – STIL\n\n[3] Fonseca, E.R., Aluísio, Sandra Maria, Rosa, J.L.G. 2015.\nEvaluating word embeddings and a revised corpus for part-of-speech tagging in Portuguese.\nJournal of the Brazilian Computer Society.\n",
            "url": "http://www.nilc.icmc.usp.br/macmorpho/",
            "license": "Creative Commons Attribution 4.0 International License",
            "givenLicense": "cc-by-4.0",
            "language": [
                "pt"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "part-of-speech-tagging"
            ]
        }
    },
    {
        "id": "makhzan",
        "data": {
            "description": "An Urdu text corpus for machine learning, natural language processing and linguistic analysis.\n",
            "url": "https://matnsaz.net/en/makhzan",
            "license": "All files in the /text directory are covered under standard copyright. Each piece of text has been included in this repository with explicity permission of respective copyright holders, who are identified in the <meta> tag for each file. You are free to use this text for analysis, research and development, but you are not allowed to redistribute or republish this text. Some cases where a less restrictive license could apply to files in the /text directory are presented below. In some cases copyright free text has been digitally reproduced through the hard work of our collaborators. In such cases we have credited the appropriate people where possible in a notes field in the file's metadata, and we strongly encourage you to contact them before redistributing this text in any form. Where a separate license is provided along with the text, we have provided corresponding data in the publication field in a file's metadata.",
            "givenLicense": "other",
            "language": [
                "ur"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "masakhaner",
        "data": {
            "description": "MasakhaNER is the first large publicly available high-quality dataset for named entity recognition (NER) in ten African languages.\n\nNamed entities are phrases that contain the names of persons, organizations, locations, times and quantities.\n\nExample:\n[PER Wolff] , currently a journalist in [LOC Argentina] , played with [PER Del Bosque] in the final years of the seventies in [ORG Real Madrid] .\nMasakhaNER is a named entity dataset consisting of PER, ORG, LOC, and DATE entities annotated by Masakhane for ten African languages:\n- Amharic\n- Hausa\n- Igbo\n- Kinyarwanda\n- Luganda\n- Luo\n- Nigerian-Pidgin\n- Swahili\n- Wolof\n- Yoruba\n\nThe train/validation/test sets are available for all the ten languages.\n\nFor more details see https://arxiv.org/abs/2103.11811\n",
            "url": "https://arxiv.org/abs/2103.11811",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "am",
                "ha",
                "ig",
                "lg",
                "luo",
                "pcm",
                "rw",
                "sw",
                "wo",
                "yo"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "math_dataset",
        "data": {
            "description": "\nMathematics database.\n\nThis dataset code generates mathematical question and answer pairs,\nfrom a range of question types at roughly school-level difficulty.\nThis is designed to test the mathematical learning and algebraic\nreasoning skills of learning models.\n\nOriginal paper: Analysing Mathematical Reasoning Abilities of Neural Models\n(Saxton, Grefenstette, Hill, Kohli).\n\nExample usage:\ntrain_examples, val_examples = datasets.load_dataset(\n    'math_dataset/arithmetic__mul',\n    split=['train', 'test'],\n    as_supervised=True)\n",
            "url": "https://github.com/deepmind/mathematics_dataset",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "math_qa",
        "data": {
            "description": "\nOur dataset is gathered by using a new representation language to annotate over the AQuA-RAT dataset. AQuA-RAT has provided the questions, options, rationale, and the correct options.\n",
            "url": "https://math-qa.github.io/math-QA/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "matinf",
        "data": {
            "description": "MATINF is the first jointly labeled large-scale dataset for classification, question answering and summarization.\nMATINF contains 1.07 million question-answer pairs with human-labeled categories and user-generated question \ndescriptions. Based on such rich information, MATINF is applicable for three major NLP tasks, including classification, \nquestion answering, and summarization. We benchmark existing methods and a novel multi-task baseline over MATINF to \ninspire further research. Our comprehensive comparison and experiments over MATINF and other datasets demonstrate the \nmerits held by MATINF.\n",
            "url": "https://github.com/WHUIR/MATINF",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "mbpp",
        "data": {
            "description": "The MBPP (Mostly Basic Python Problems) dataset consists of around 1,000 crowd-sourced Python\nprogramming problems, designed to be solvable by entry level programmers, covering programming\nfundamentals, standard library functionality, and so on. Each problem consists of a task\ndescription, code solution and 3 automated test cases.\n",
            "url": "https://github.com/google-research/google-research/tree/master/mbpp",
            "license": "CC-BY-4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-code-generation"
            ]
        }
    },
    {
        "id": "mc_taco",
        "data": {
            "description": "MC-TACO (Multiple Choice TemporAl COmmonsense) is a dataset of 13k question-answer\npairs that require temporal commonsense comprehension. A system receives a sentence\nproviding context information, a question designed to require temporal commonsense\nknowledge, and multiple candidate answers. More than one candidate answer can be plausible.\n\nThe task is framed as binary classification: givent he context, the question,\nand the candidate answer, the task is to determine whether the candidate\nanswer is plausible (\"yes\") or not (\"no\").",
            "url": "https://cogcomp.seas.upenn.edu/page/resource_view/125",
            "license": "Unknown",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "mdd",
        "data": {
            "description": "The Movie Dialog dataset (MDD) is designed to measure how well\nmodels can perform at goal and non-goal orientated dialog\ncentered around the topic of movies (question answering,\nrecommendation and discussion).\n\n",
            "url": "https://research.fb.com/downloads/babi/",
            "license": "Creative Commons Attribution 3.0 License",
            "givenLicense": "cc-by-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "md_gender_bias",
        "data": {
            "description": "Machine learning models are trained to find patterns in data.\nNLP models can inadvertently learn socially undesirable patterns when training on gender biased text.\nIn this work, we propose a general framework that decomposes gender bias in text along several pragmatic and semantic dimensions:\nbias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker.\nUsing this fine-grained framework, we automatically annotate eight large scale datasets with gender information.\nIn addition, we collect a novel, crowdsourced evaluation benchmark of utterance-level gender rewrites.\nDistinguishing between gender bias along multiple dimensions is important, as it enables us to train finer-grained gender bias classifiers.\nWe show our classifiers prove valuable for a variety of important applications, such as controlling for gender bias in generative models,\ndetecting gender bias in arbitrary text, and shed light on offensive language in terms of genderedness.\n",
            "url": "https://parl.ai/projects/md_gender/",
            "license": "MIT License",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-gender-bias"
            ]
        }
    },
    {
        "id": "medal",
        "data": {
            "description": "A large medical text dataset (14Go) curated to 4Go for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. For example, DHF can be disambiguated to dihydrofolate, diastolic heart failure, dengue hemorragic fever or dihydroxyfumarate\n",
            "url": "https://github.com/BruceWen120/medal",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "other"
            ],
            "tasks": [
                "disambiguation"
            ]
        }
    },
    {
        "id": "med_hop",
        "data": {
            "description": "MedHop is based on research paper abstracts from PubMed, and the queries are about interactions between pairs of drugs. The correct answer has to be inferred by combining information from a chain of reactions of drugs and proteins.\n",
            "url": "http://qangaroo.cs.ucl.ac.uk/",
            "license": "",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "question-answering-other-multi-hop"
            ]
        }
    },
    {
        "id": "medical_dialog",
        "data": {
            "description": "The MedDialog dataset (English) contains conversations (in English) between doctors and patients.It has 0.26 million dialogues. The data is continuously growing and more dialogues will be added. The raw dialogues are from healthcaremagic.com and icliniq.com.\nAll copyrights of the data belong to healthcaremagic.com and icliniq.com.\n",
            "url": "https://github.com/UCSD-AI4H/Medical-Dialogue-System",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "zh"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "closed-domain-qa"
            ]
        }
    },
    {
        "id": "medical_questions_pairs",
        "data": {
            "description": "This dataset consists of 3048 similar and dissimilar medical question pairs hand-generated and labeled by Curai's doctors.\n",
            "url": "https://github.com/curai/medical-question-pair-dataset",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "semantic-similarity-classification"
            ]
        }
    },
    {
        "id": "medmcqa",
        "data": {
            "description": "MedMCQA is a large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. \nMedMCQA has more than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity.\nThe dataset contains questions about the following topics: Anesthesia, Anatomy, Biochemistry, Dental, ENT, Forensic Medicine (FM)\nObstetrics and Gynecology (O&G), Medicine, Microbiology, Ophthalmology, Orthopedics Pathology, Pediatrics, Pharmacology, Physiology, \nPsychiatry, Radiology Skin, Preventive & Social Medicine (PSM) and Surgery\n",
            "url": "https://medmcqa.github.io",
            "license": "Apache License 2.0",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering",
                "multiple-choice"
            ],
            "tasks": [
                "multiple-choice-qa",
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "menyo20k_mt",
        "data": {
            "description": "MENYO-20k is a multi-domain parallel dataset with texts obtained from news articles, ted talks, movie transcripts, radio transcripts, science and technology texts, and other short articles curated from the web and professional translators. The dataset has 20,100 parallel sentences split into 10,070 training sentences, 3,397 development sentences, and 6,633 test sentences (3,419 multi-domain, 1,714 news domain, and 1,500 ted talks speech transcript domain). The development and test sets are available upon request.\n",
            "url": "https://zenodo.org/record/4297448#.X81G7s0zZPY",
            "license": "For non-commercial use because some of the data sources like Ted talks and JW news requires permission for commercial use.",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en",
                "yo"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "metashift",
        "data": {
            "description": "The MetaShift is a dataset of datasets for evaluating distribution shifts and training conflicts.\nThe MetaShift dataset is a collection of 12,868 sets of natural images across 410 classes.\nIt was created for understanding the performance of a machine learning model across diverse data distributions.\n",
            "url": "https://metashift.readthedocs.io/",
            "license": "https://github.com/Weixin-Liang/MetaShift/blob/main/LICENSE",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification",
                "other"
            ],
            "tasks": [
                "multi-label-image-classification",
                "other-other-domain-generalization"
            ]
        }
    },
    {
        "id": "meta_woz",
        "data": {
            "description": "MetaLWOz: A Dataset of Multi-Domain Dialogues for the Fast Adaptation of Conversation Models. We introduce the Meta-Learning Wizard of Oz (MetaLWOz) dialogue dataset for developing fast adaptation methods for conversation models. This data can be used to train task-oriented dialogue models, specifically to develop methods to quickly simulate user responses with a small amount of data. Such fast-adaptation models fall into the research areas of transfer learning and meta learning. The dataset consists of 37,884 crowdsourced dialogues recorded between two human users in a Wizard of Oz setup, in which one was instructed to behave like a bot, and the other a true human user. The users are assigned a task belonging to a particular domain, for example booking a reservation at a particular restaurant, and work together to complete the task. Our dataset spans 47 domains having 227 tasks total. Dialogues are a minimum of 10 turns long.\n",
            "url": "https://www.microsoft.com/en-us/research/project/metalwoz/",
            "license": "Microsoft Research Data License Agreement",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "metooma",
        "data": {
            "description": "The dataset consists of tweets belonging to #MeToo movement on Twitter, labelled into different categories.\nDue to Twitter's development policies, we only provide the tweet ID's and corresponding labels,\nother data can be fetched via Twitter API.\nThe data has been labelled by experts, with the majority taken into the account for deciding the final label.\nWe provide these labels for each of the tweets. The labels provided for each data point\nincludes -- Relevance, Directed Hate, Generalized Hate,\nSarcasm, Allegation, Justification, Refutation, Support, Oppose\n",
            "url": "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JN4EYU",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification",
                "text-retrieval"
            ],
            "tasks": [
                "multi-class-classification",
                "multi-label-classification"
            ]
        }
    },
    {
        "id": "metrec",
        "data": {
            "description": "Arabic Poetry Metric Classification.\nThe dataset contains the verses and their corresponding meter classes.Meter classes are represented as numbers from 0 to 13. The dataset can be highly useful for further research in order to improve the field of Arabic poems’ meter classification.The train dataset contains 47,124 records and the test dataset contains 8316 records.\n",
            "url": "https://github.com/zaidalyafeai/MetRec",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-poetry-classification"
            ]
        }
    },
    {
        "id": "miam",
        "data": {
            "description": "Multilingual dIalogAct benchMark is a collection of resources for training, evaluating, and\nanalyzing natural language understanding systems specifically designed for spoken language. Datasets\nare in English, French, German, Italian and Spanish. They cover a variety of domains including\nspontaneous speech, scripted scenarios, and joint task completion. Some datasets additionally include\nemotion and/or sentimant labels.\n",
            "url": "",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "de",
                "en",
                "es",
                "fr",
                "it"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "text-classification"
            ],
            "tasks": [
                "dialogue-modeling",
                "language-modeling",
                "masked-language-modeling",
                "text-classification-other-dialogue-act-classification"
            ]
        }
    },
    {
        "id": "mkb",
        "data": {
            "description": "The Prime Minister's speeches - Mann Ki Baat, on All India Radio, translated into many languages.\n",
            "url": "http://preon.iiit.ac.in/~jerin/resources/datasets/mkb-v0.tar",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "hi",
                "te",
                "ta",
                "ml",
                "gu",
                "ur",
                "bn",
                "or",
                "mr",
                "pa",
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "mkqa",
        "data": {
            "description": "We introduce MKQA, an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). The goal of this dataset is to provide a challenging benchmark for question answering quality across a wide set of languages.\n",
            "url": "https://github.com/apple/ml-mkqa",
            "license": "CC BY-SA 3.0",
            "givenLicense": "cc-by-3.0",
            "language": [
                "ar",
                "da",
                "de",
                "en",
                "es",
                "fi",
                "fr",
                "he",
                "hu",
                "it",
                "ja",
                "ko",
                "km",
                "ms",
                "nl",
                "no",
                "pl",
                "pt",
                "ru",
                "sv",
                "th",
                "tr",
                "vi",
                "zh"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "m_lama",
        "data": {
            "description": "mLAMA: a multilingual version of the LAMA benchmark (T-REx and GoogleRE) covering 53 languages.",
            "url": "http://cistern.cis.lmu.de/mlama/",
            "license": "The Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0). https://creativecommons.org/licenses/by-nc-sa/4.0/",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "af",
                "ar",
                "az",
                "be",
                "bg",
                "bn",
                "ca",
                "ceb",
                "cs",
                "cy",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "eu",
                "fa",
                "fi",
                "fr",
                "ga",
                "gl",
                "he",
                "hi",
                "hr",
                "hu",
                "hy",
                "id",
                "it",
                "ja",
                "ka",
                "ko",
                "la",
                "lt",
                "lv",
                "ms",
                "nl",
                "pl",
                "pt",
                "ro",
                "ru",
                "sk",
                "sl",
                "sq",
                "sr",
                "sv",
                "ta",
                "th",
                "tr",
                "uk",
                "ur",
                "vi",
                "zh"
            ],
            "categories": [
                "question-answering",
                "text-classification"
            ],
            "tasks": [
                "open-domain-qa",
                "text-scoring",
                "text-classification-other-probing"
            ]
        }
    },
    {
        "id": "mlqa",
        "data": {
            "description": "    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.\n    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,\n    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between \n    4 different languages on average.\n",
            "url": "https://github.com/facebookresearch/MLQA",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "mlsum",
        "data": {
            "description": "We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. \nObtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. \nTogether with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. \nWe report cross-lingual comparative analyses based on state-of-the-art systems. \nThese highlight existing biases which motivate the use of a multi-lingual dataset. \n",
            "url": "",
            "license": "",
            "givenLicense": "other",
            "language": [
                "de",
                "es",
                "fr",
                "ru",
                "tr"
            ],
            "categories": [
                "translation",
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "multi-label-classification",
                "summarization",
                "topic-classification"
            ]
        }
    },
    {
        "id": "mnist",
        "data": {
            "description": "The MNIST dataset consists of 70,000 28x28 black-and-white images in 10 classes (one for each digits), with 7,000\nimages per class. There are 60,000 training images and 10,000 test images.\n",
            "url": "http://yann.lecun.com/exdb/mnist/",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification"
            ],
            "tasks": [
                "multi-class-image-classification"
            ]
        }
    },
    {
        "id": "mocha",
        "data": {
            "description": "Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. To address this, we introduce a benchmark for training and evaluating generative reading comprehension metrics: MOdeling Correctness with Human Annotations. MOCHA contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using MOCHA, we train an evaluation metric: LERC, a Learned Evaluation metric for Reading Comprehension, to mimic human judgement scores.\n",
            "url": "https://allennlp.org/mocha",
            "license": "https://creativecommons.org/licenses/by-sa/4.0/legalcode",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "question-answering-other-generative-reading-comprehension-metric"
            ]
        }
    },
    {
        "id": "monash_tsf",
        "data": {
            "description": "The first repository containing datasets of related time series for global forecasting.\n",
            "url": "https://forecastingdata.org/",
            "license": "The Creative Commons Attribution 4.0 International License. https://creativecommons.org/licenses/by/4.0/",
            "givenLicense": "cc-by-4.0",
            "language": [
                "unknown"
            ],
            "categories": [
                "time-series-forecasting"
            ],
            "tasks": [
                "univariate-time-series-forecasting",
                "multivariate-time-series-forecasting"
            ]
        }
    },
    {
        "id": "moroco",
        "data": {
            "description": "The MOROCO (Moldavian and Romanian Dialectal Corpus) dataset contains 33564 samples of text collected from the news domain.\nThe samples belong to one of the following six topics:\n    - culture\n    - finance\n    - politics\n    - science\n    - sports\n    - tech\n",
            "url": "https://github.com/butnaruandrei/MOROCO",
            "license": "CC BY-SA 4.0 License",
            "givenLicense": "cc-by-4.0",
            "language": [
                "ro",
                "ro-md"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "topic-classification"
            ]
        }
    },
    {
        "id": "movie_rationales",
        "data": {
            "description": "\nThe movie rationale dataset contains human annotated rationales for movie\nreviews.\n",
            "url": "http://www.cs.jhu.edu/~ozaidan/rationales/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "mrqa",
        "data": {
            "description": "The MRQA 2019 Shared Task focuses on generalization in question answering.\nAn effective question answering system should do more than merely\ninterpolate from the training set to answer test examples drawn\nfrom the same distribution: it should also be able to extrapolate\nto out-of-distribution examples — a significantly harder challenge.\n\nThe dataset is a collection of 18 existing QA dataset (carefully selected\nsubset of them) and converted to the same format (SQuAD format). Among\nthese 18 datasets, six datasets were made available for training,\nsix datasets were made available for development, and the final six\nfor testing. The dataset is released as part of the MRQA 2019 Shared Task.\n",
            "url": "https://mrqa.github.io/2019/shared.html",
            "license": "Unknwon",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "ms_marco",
        "data": {
            "description": "\nStarting with a paper released at NIPS 2016, MS MARCO is a collection of datasets focused on deep learning in search.\n\nThe first dataset was a question answering dataset featuring 100,000 real Bing questions and a human generated answer. \nSince then we released a 1,000,000 question dataset, a natural langauge generation dataset, a passage ranking dataset, \nkeyphrase extraction dataset, crawling dataset, and a conversational search.\n\nThere have been 277 submissions. 20 KeyPhrase Extraction submissions, 87 passage ranking submissions, 0 document ranking \nsubmissions, 73 QnA V2 submissions, 82 NLGEN submisions, and 15 QnA V1 submissions\n\nThis data comes in three tasks/forms: Original QnA dataset(v1.1), Question Answering(v2.1), Natural Language Generation(v2.1). \n\nThe original question answering datset featured 100,000 examples and was released in 2016. Leaderboard is now closed but data is availible below.\n\nThe current competitive tasks are Question Answering and Natural Language Generation. Question Answering features over 1,000,000 queries and \nis much like the original QnA dataset but bigger and with higher quality. The Natural Language Generation dataset features 180,000 examples and \nbuilds upon the QnA dataset to deliver answers that could be spoken by a smart speaker.\n\n\nversion v1.1",
            "url": "https://microsoft.github.io/msmarco/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "msra_ner",
        "data": {
            "description": "The Third International Chinese Language\nProcessing Bakeoff was held in Spring\n2006 to assess the state of the art in two\nimportant tasks: word segmentation and\nnamed entity recognition. Twenty-nine\ngroups submitted result sets in the two\ntasks across two tracks and a total of five\ncorpora. We found strong results in both\ntasks as well as continuing challenges.\n\nMSRA NER is one of the provided dataset.\nThere are three types of NE, PER (person),\nORG (organization) and LOC (location).\nThe dataset is in the BIO scheme.\n\nFor more details see https://faculty.washington.edu/levow/papers/sighan06.pdf\n",
            "url": "https://www.microsoft.com/en-us/download/details.aspx?id=52531",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "zh"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "msr_genomics_kbcomp",
        "data": {
            "description": "The database is derived from the NCI PID Pathway Interaction Database, and the textual mentions are extracted from cooccurring pairs of genes in PubMed abstracts, processed and annotated by Literome (Poon et al. 2014). This dataset was used in the paper “Compositional Learning of Embeddings for Relation Paths in Knowledge Bases and Text” (Toutanova, Lin, Yih, Poon, and Quirk, 2016). \n",
            "url": "_HOMEPAGE",
            "license": "",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "other",
                "conversational"
            ],
            "tasks": [
                "NCI-PID-PubMed"
            ]
        }
    },
    {
        "id": "msr_sqa",
        "data": {
            "description": "Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational QA setting, we present a more realistic task: answering sequences of simple but inter-related questions. We created SQA by asking crowdsourced workers to decompose 2,022 questions from WikiTableQuestions (WTQ), which contains highly-compositional questions about tables from Wikipedia. We had three workers decompose each WTQ question, resulting in a dataset of 6,066 sequences that contain 17,553 questions in total. Each question is also associated with answers in the form of cell locations in the tables.\n",
            "url": "https://msropendata.com/datasets/b25190ed-0f59-47b1-9211-5962858142c2",
            "license": "Microsoft Research Data License Agreement",
            "givenLicense": "ms-pl",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "msr_text_compression",
        "data": {
            "description": "This dataset contains sentences and short paragraphs with corresponding shorter (compressed) versions. There are up to five compressions for each input text, together with quality judgements of their meaning preservation and grammaticality. The dataset is derived using source texts from the Open American National Corpus (ww.anc.org) and crowd-sourcing. \n",
            "url": "https://msropendata.com/datasets/f8ce2ec9-7fbd-48f7-a8bb-2d2279373563",
            "license": "Microsoft Research Data License Agreement",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": []
        }
    },
    {
        "id": "msr_zhen_translation_parity",
        "data": {
            "description": "Translator Human Parity Data\n\nHuman evaluation results and translation output for the Translator Human Parity Data release, \nas described in https://blogs.microsoft.com/ai/machine-translation-news-test-set-human-parity/. \nThe Translator Human Parity Data release contains all human evaluation results and translations\nrelated to our paper \"Achieving Human Parity on Automatic Chinese to English News Translation\", \npublished on March 14, 2018.\n",
            "url": "https://msropendata.com/datasets/93f9aa87-9491-45ac-81c1-6498b6be0d0b",
            "license": "",
            "givenLicense": "ms-pl",
            "language": [
                "en"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "ms_terms",
        "data": {
            "description": "The Microsoft Terminology Collection can be used to develop localized versions of applications that integrate with Microsoft products.\nIt can also be used to integrate Microsoft terminology into other terminology collections or serve as a base IT glossary\nfor language development in the nearly 100 languages available. Terminology is provided in .tbx format, an industry standard for terminology exchange.\n",
            "url": "https://www.microsoft.com/en-us/language/terminology",
            "license": "",
            "givenLicense": "ms-pl",
            "language": [
                "af",
                "sq",
                "am",
                "ar",
                "hy",
                "as",
                "az",
                "bn",
                "other-bn-india",
                "eu",
                "be",
                "bs",
                "other-bs-latin",
                "bg",
                "ca",
                "ku",
                "chr",
                "zh",
                "other-zh-Hant_HK",
                "other-zh-Hant_TW",
                "hr",
                "cs",
                "da",
                "prs",
                "nl",
                "en",
                "et",
                "fil",
                "fi",
                "fr",
                "other-fr_CA",
                "gl",
                "ka",
                "de",
                "el",
                "gu",
                "ha",
                "he",
                "hi",
                "hu",
                "is",
                "ig",
                "id",
                "iu",
                "ga",
                "xh",
                "zu",
                "it",
                "ja",
                "quc",
                "kn",
                "kk",
                "km",
                "rw",
                "swh",
                "knn",
                "ko",
                "ky",
                "lo",
                "lv",
                "lt",
                "lb",
                "mk",
                "other-ms-brunei"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "mt_eng_vietnamese",
        "data": {
            "description": "Preprocessed Dataset from IWSLT'15 English-Vietnamese machine translation: English-Vietnamese.\n",
            "url": "https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "vi"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "muchocine",
        "data": {
            "description": "The Muchocine reviews dataset contains 3,872 longform movie reviews in Spanish language,\neach with a shorter summary review, and a rating on a 1-5 scale.\n",
            "url": "http://www.lsi.us.es/~fermin/index.php/Datasets",
            "license": "CC-BY-2.1",
            "givenLicense": "unknown",
            "language": [
                "es"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "multi_booked",
        "data": {
            "description": "MultiBooked is a corpus of Basque and Catalan Hotel Reviews Annotated for Aspect-level Sentiment Classification.\n\nThe corpora are compiled from hotel reviews taken mainly from booking.com. The corpora are in Kaf/Naf format, which is\nan xml-style stand-off format that allows for multiple layers of annotation. Each review was sentence- and\nword-tokenized and lemmatized using Freeling for Catalan and ixa-pipes for Basque. Finally, for each language two\nannotators annotated opinion holders, opinion targets, and opinion expressions for each review, following the\nguidelines set out in the OpeNER project.\n",
            "url": "http://hdl.handle.net/10230/33928",
            "license": "CC-BY 3.0",
            "givenLicense": "cc-by-3.0",
            "language": [
                "ca",
                "eu"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "multidoc2dial",
        "data": {
            "description": "MultiDoc2Dial is a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as a machine reading comprehension task based on a single given document or passage. We aim to address more realistic scenarios where a goal-oriented information-seeking conversation involves multiple topics, and hence is grounded on different documents. \n",
            "url": "https://doc2dial.github.io/multidoc2dial/",
            "license": "",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "multi_eurlex",
        "data": {
            "description": "MultiEURLEX comprises 65k EU laws in 23 official EU languages (some low-ish resource).\nEach EU law has been annotated with EUROVOC concepts (labels) by the Publication Office of EU.\nAs with the English EURLEX, the goal is to predict the relevant EUROVOC concepts (labels);\nthis is multi-label classification task (given the text, predict multiple labels).\n",
            "url": "https://github.io/iliaschalkidis",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en",
                "da",
                "de",
                "nl",
                "sv",
                "bg",
                "cs",
                "hr",
                "pl",
                "sk",
                "sl",
                "es",
                "fr",
                "it",
                "pt",
                "ro",
                "et",
                "fi",
                "hu",
                "lt",
                "lv",
                "el",
                "mt"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification",
                "topic-classification"
            ]
        }
    },
    {
        "id": "multilingual_librispeech",
        "data": {
            "description": "Multilingual LibriSpeech (MLS) dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish.\n",
            "url": "http://www.openslr.org/94",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "de",
                "nl",
                "fr",
                "it",
                "es",
                "pt",
                "pl"
            ],
            "categories": [
                "automatic-speech-recognition",
                "audio-classification"
            ],
            "tasks": [
                "audio-speaker-identification"
            ]
        }
    },
    {
        "id": "multi_news",
        "data": {
            "description": "\nMulti-News, consists of news articles and human-written summaries\nof these articles from the site newser.com.\nEach summary is professionally written by editors and\nincludes links to the original articles cited.\n\nThere are two features:\n  - document: text of news articles seperated by special token \"|||||\".\n  - summary: news summary.\n",
            "url": "https://github.com/Alex-Fabbri/Multi-News",
            "license": "For non-commercial research and educational purposes only",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "news-articles-summarization"
            ]
        }
    },
    {
        "id": "multi_nli",
        "data": {
            "description": "The Multi-Genre Natural Language Inference (MultiNLI) corpus is a\ncrowd-sourced collection of 433k sentence pairs annotated with textual\nentailment information. The corpus is modeled on the SNLI corpus, but differs in\nthat covers a range of genres of spoken and written text, and supports a\ndistinctive cross-genre generalization evaluation. The corpus served as the\nbasis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.\n",
            "url": "https://www.nyu.edu/projects/bowman/multinli/",
            "license": "",
            "givenLicense": "cc-by-3.0,cc-by-sa-3.0,mit,other",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference",
                "multi-input-text-classification"
            ]
        }
    },
    {
        "id": "multi_nli_mismatch",
        "data": {
            "description": "The Multi-Genre Natural Language Inference (MultiNLI) corpus is a\ncrowd-sourced collection of 433k sentence pairs annotated with textual\nentailment information. The corpus is modeled on the SNLI corpus, but differs in\nthat covers a range of genres of spoken and written text, and supports a\ndistinctive cross-genre generalization evaluation. The corpus served as the\nbasis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.\n",
            "url": "https://www.nyu.edu/projects/bowman/multinli/",
            "license": "",
            "givenLicense": "cc-by-3.0,cc-by-sa-3.0,mit,other",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference",
                "multi-input-text-classification"
            ]
        }
    },
    {
        "id": "multi_para_crawl",
        "data": {
            "description": "Parallel corpora from Web Crawls collected in the ParaCrawl project and further processed for making it a multi-parallel corpus by pivoting via English. Here we only provide the additional language pairs that came out of pivoting. The bitexts for English are available from the ParaCrawl release.\n40 languages, 669 bitexts\ntotal number of files: 40\ntotal number of tokens: 10.14G\ntotal number of sentence fragments: 505.48M\n\nPlease, acknowledge the ParaCrawl project at http://paracrawl.eu. This version is derived from the original release at their website adjusted for redistribution via the OPUS corpus collection. Please, acknowledge OPUS as well for this service.\n",
            "url": "http://opus.nlpl.eu/MultiParaCrawl.php",
            "license": "",
            "givenLicense": "cc0-1.0",
            "language": [
                "bg",
                "ca",
                "cs",
                "da",
                "de",
                "el",
                "es",
                "et",
                "eu",
                "fi",
                "fr",
                "ga",
                "gl",
                "ha",
                "hr",
                "hu",
                "ig",
                "is",
                "it",
                "km",
                "lt",
                "lv",
                "mt",
                "my",
                "nb",
                "ne",
                "nl",
                "nn",
                "pl",
                "ps",
                "pt",
                "ro",
                "ru",
                "si",
                "sk",
                "sl",
                "so",
                "sv",
                "sw",
                "tl"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "multi_re_qa",
        "data": {
            "description": "MultiReQA contains the sentence boundary annotation from eight publicly available QA datasets including SearchQA, TriviaQA, HotpotQA, NaturalQuestions, SQuAD, BioASQ, RelationExtraction, and TextbookQA. Five of these datasets, including SearchQA, TriviaQA, HotpotQA, NaturalQuestions, SQuAD, contain both training and test data, and three, including BioASQ, RelationExtraction, TextbookQA, contain only the test data",
            "url": "https://github.com/google-research-datasets/MultiReQA",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "multi_woz_v22",
        "data": {
            "description": "Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.\nMultiWOZ 2.1 (Eric et al., 2019) identified and fixed many erroneous annotations and user utterances in the original version, resulting in an\nimproved version of the dataset. MultiWOZ 2.2 is a yet another improved version of this dataset, which identifies and fizes dialogue state annotation errors\nacross 17.3% of the utterances on top of MultiWOZ 2.1 and redefines the ontology by disallowing vocabularies of slots with a large number of possible values\n(e.g., restaurant name, time of booking) and introducing standardized slot span annotations for these slots.\n",
            "url": "https://github.com/budzianowski/multiwoz/tree/master/data/MultiWOZ_2.2",
            "license": "Apache License 2.0",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "token-classification",
                "text-classification",
                "natural-language-processing"
            ],
            "tasks": [
                "dialogue-modeling",
                "multi-class-classification",
                "parsing"
            ]
        }
    },
    {
        "id": "multi_x_science_sum",
        "data": {
            "description": "\nMulti-XScience,a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing therelated-work section of a paper based on itsabstract and the articles it references.\n",
            "url": "https://github.com/yaolu/Multi-XScience",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "summarization-other-paper-abstract-generation"
            ]
        }
    },
    {
        "id": "mutual_friends",
        "data": {
            "description": "Our goal is to build systems that collaborate with people by exchanging\ninformation through natural language and reasoning over structured knowledge\nbase. In the MutualFriend task, two agents, A and B, each have a private\nknowledge base, which contains a list of friends with multiple attributes\n(e.g., name, school, major, etc.). The agents must chat with each other\nto find their unique mutual friend.",
            "url": "https://stanfordnlp.github.io/cocoa/",
            "license": "Unknown",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "mwsc",
        "data": {
            "description": "Examples taken from the Winograd Schema Challenge modified to ensure that answers are a single word from the context.\nThis modified Winograd Schema Challenge (MWSC) ensures that scores are neither inflated nor deflated by oddities in phrasing.\n",
            "url": "http://decanlp.com",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "myanmar_news",
        "data": {
            "description": "The Myanmar news dataset contains article snippets in four categories:\nBusiness, Entertainment, Politics, and Sport.\n\nThese were collected in October 2017 by Aye Hninn Khine\n",
            "url": "https://github.com/ayehninnkhine/MyanmarNewsClassificationSystem",
            "license": "GPL-3.0",
            "givenLicense": "gpl-3.0",
            "language": [
                "my"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "topic-classification"
            ]
        }
    },
    {
        "id": "narrativeqa",
        "data": {
            "description": "The NarrativeQA dataset for question answering on long documents (movie scripts, books). It includes the list of documents with Wikipedia summaries, links to full stories, and questions and answers.\n",
            "url": "https://github.com/deepmind/narrativeqa",
            "license": "",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "abstractive-qa"
            ]
        }
    },
    {
        "id": "narrativeqa_manual",
        "data": {
            "description": "The Narrative QA Manual dataset is a reading comprehension dataset, in which the reader must answer questions about stories by reading entire books or movie scripts. The QA tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience.\\\nTHIS DATASET REQUIRES A MANUALLY DOWNLOADED FILE! Because of a script in the original repository which downloads the stories from original URLs everytime, The links are sometimes broken or invalid.  Therefore, you need to manually download the stories for this dataset using the script provided by the authors (https://github.com/deepmind/narrativeqa/blob/master/download_stories.sh). Running the shell script creates a folder named \"tmp\" in the root directory and downloads the stories there. This folder containing the storiescan be used to load the dataset via `datasets.load_dataset(\"narrativeqa_manual\", data_dir=\"<path/to/folder>\")`.                ",
            "url": "https://deepmind.com/research/publications/narrativeqa-reading-comprehension-challenge",
            "license": "https://github.com/deepmind/narrativeqa/blob/master/LICENSE",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "abstractive-qa"
            ]
        }
    },
    {
        "id": "natural_questions",
        "data": {
            "description": "\nThe NQ corpus contains questions from real users, and it requires QA systems to\nread and comprehend an entire Wikipedia article that may or may not contain the\nanswer to the question. The inclusion of real user questions, and the\nrequirement that solutions should read an entire page to find the answer, cause\nNQ to be a more realistic and challenging task than prior QA datasets.\n",
            "url": "https://ai.google.com/research/NaturalQuestions/dataset",
            "license": "",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "ncbi_disease",
        "data": {
            "description": "This paper presents the disease name and concept annotations of the NCBI disease corpus, a collection of 793 PubMed\nabstracts fully annotated at the mention and concept level to serve as a research resource for the biomedical natural\nlanguage processing community. Each PubMed abstract was manually annotated by two annotators with disease mentions\nand their corresponding concepts in Medical Subject Headings (MeSH®) or Online Mendelian Inheritance in Man (OMIM®).\nManual curation was performed using PubTator, which allowed the use of pre-annotations as a pre-step to manual annotations.\nFourteen annotators were randomly paired and differing annotations were discussed for reaching a consensus in two\nannotation phases. In this setting, a high inter-annotator agreement was observed. Finally, all results were checked\nagainst annotations of the rest of the corpus to assure corpus-wide consistency.\n\nFor more details, see: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3951655/\n\nThe original dataset can be downloaded from: https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/NCBI_corpus.zip\nThis dataset has been converted to CoNLL format for NER using the following tool: https://github.com/spyysalo/standoff2conll\nNote: there is a duplicate document (PMID 8528200) in the original data, and the duplicate is recreated in the converted data.\n",
            "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3951655/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "nchlt",
        "data": {
            "description": "The development of linguistic resources for use in natural language processingis of utmost importance for the continued growth of research anddevelopment in the field, especially for resource-scarce languages. In this paper we describe the process and challenges of simultaneouslydevelopingmultiple linguistic resources for ten of the official languages of South Africa. The project focussed on establishing a set of foundational resources that can foster further development of both resources and technologies for the NLP industry in South Africa. The development efforts during the project included creating monolingual unannotated corpora, of which a subset of the corpora for each language was annotated on token, orthographic, morphological and morphosyntactic layers. The annotated subsetsincludes both development and test setsand were used in the creation of five core-technologies, viz. atokeniser, sentenciser,lemmatiser, part of speech tagger and morphological decomposer for each language. We report on the quality of these tools for each language and provide some more context of the importance of the resources within the South African context.\n",
            "url": "https://www.aclweb.org/anthology/W02-2024/",
            "license": "",
            "givenLicense": "cc-by-2.5",
            "language": [
                "af",
                "nr",
                "nso",
                "ss",
                "tn",
                "ts",
                "ve",
                "xh",
                "zu"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "ncslgr",
        "data": {
            "description": "\nA small corpus of American Sign Language (ASL) video data from native signers, annotated with non-manual features.\n",
            "url": "https://www.bu.edu/asllrp/ncslgr.html",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "ase",
                "en"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "nell",
        "data": {
            "description": "This dataset provides version 1115 of the belief\nextracted by CMU's Never Ending Language Learner (NELL) and version\n1110 of the candidate belief extracted by NELL. See\nhttp://rtw.ml.cmu.edu/rtw/overview.  NELL is an open information\nextraction system that attempts to read the Clueweb09 of 500 million\nweb pages (http://boston.lti.cs.cmu.edu/Data/clueweb09/) and general\nweb searches.\n\nThe dataset has 4 configurations: nell_belief, nell_candidate,\nnell_belief_sentences, and nell_candidate_sentences. nell_belief is\ncertainties of belief are lower. The two sentences config extracts the\nCPL sentence patterns filled with the applicable 'best' literal string\nfor the entities filled into the sentence patterns. And also provides\nsentences found using web searches containing the entities and\nrelationships.\n\nThere are roughly 21M entries for nell_belief_sentences, and 100M\nsentences for nell_candidate_sentences.\n",
            "url": "http://rtw.ml.cmu.edu/rtw/",
            "license": "\n",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-to-tabular",
                "text-retrieval",
                "text-to-structured"
            ],
            "tasks": [
                "entity-linking-retrieval",
                "fact-checking-retrieval",
                "relation-extraction"
            ]
        }
    },
    {
        "id": "neural_code_search",
        "data": {
            "description": "Neural-Code-Search-Evaluation-Dataset presents an evaluation dataset consisting of natural language query and code snippet pairs and a search corpus consisting of code snippets collected from the most popular Android repositories on GitHub.\n",
            "url": "https://github.com/facebookresearch/Neural-Code-Search-Evaluation-Dataset/tree/master/data",
            "license": "CC-BY-NC 4.0 (Attr Non-Commercial Inter.)",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "news_commentary",
        "data": {
            "description": "A parallel corpus of News Commentaries provided by WMT for training SMT. The source is taken from CASMACAT: http://www.casmacat.eu/corpus/news-commentary.html\n\n12 languages, 63 bitexts\ntotal number of files: 61,928\ntotal number of tokens: 49.66M\ntotal number of sentence fragments: 1.93M\n",
            "url": "http://opus.nlpl.eu/News-Commentary.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar",
                "cs",
                "de",
                "en",
                "es",
                "fr",
                "it",
                "ja",
                "nl",
                "pt",
                "ru",
                "zh"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "newsgroup",
        "data": {
            "description": "\nThe 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across\n20 different newsgroups. The 20 newsgroups collection has become a popular data set for experiments in text applications of\nmachine learning techniques, such as text classification and text clustering.\n\ndoes not include cross-posts and includes only the \"From\" and \"Subject\" headers.",
            "url": "http://qwone.com/~jason/20Newsgroups/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "newsph",
        "data": {
            "description": "Large-scale dataset of Filipino news articles. Sourced for the NewsPH-NLI Project (Cruz et al., 2020).\n",
            "url": "https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks",
            "license": "GPL-3.0",
            "givenLicense": "gpl-3.0",
            "language": [
                "fil",
                "tl"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "newsph_nli",
        "data": {
            "description": "    First benchmark dataset for sentence entailment in the low-resource Filipino language. Constructed through exploting the structure of news articles. Contains 600,000 premise-hypothesis pairs, in 70-15-15 split for training, validation, and testing.\n",
            "url": "https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks",
            "license": "Filipino-Text-Benchmarks is licensed under the GNU General Public License v3.0",
            "givenLicense": "unknown",
            "language": [
                "tl"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference"
            ]
        }
    },
    {
        "id": "newspop",
        "data": {
            "description": "\nThis is a large data set of news items and their respective social feedback on multiple platforms: Facebook, Google+ and LinkedIn.\nThe collected data relates to a period of 8 months, between November 2015 and July 2016, accounting for about 100,000 news items on four different topics: economy, microsoft, obama and palestine.\nThis data set is tailored for evaluative comparisons in predictive analytics tasks, although allowing for tasks in other research areas such as topic detection and tracking, sentiment analysis in short text, first story detection or news recommendation.\n",
            "url": "https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms",
            "license": "Creative Commons Attribution 4.0 International License (CC-BY)",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "text-classification-other-social-media-shares-prediction"
            ]
        }
    },
    {
        "id": "newsqa",
        "data": {
            "description": "NewsQA is a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles.\n",
            "url": "https://www.microsoft.com/en-us/research/project/newsqa-dataset/",
            "license": "NewsQA CodeCopyright (c) Microsoft CorporationAll rights reserved.MIT LicensePermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.© 2020 GitHub, Inc.",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "newsroom",
        "data": {
            "description": "\nNEWSROOM is a large dataset for training and evaluating summarization systems.\nIt contains 1.3 million articles and summaries written by authors and\neditors in the newsrooms of 38 major publications.\n\nDataset features includes:\n  - text: Input news text.\n  - summary: Summary for the news.\nAnd additional features:\n  - title: news title.\n  - url: url of the news.\n  - date: date of the article.\n  - density: extractive density.\n  - coverage: extractive coverage.\n  - compression: compression ratio.\n  - density_bin: low, medium, high.\n  - coverage_bin: extractive, abstractive.\n  - compression_bin: low, medium, high.\n\nThis dataset can be downloaded upon requests. Unzip all the contents\n\"train.jsonl, dev.josnl, test.jsonl\" to the tfds folder.\n\n",
            "url": "https://summari.es",
            "license": "",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "news-articles-summarization"
            ]
        }
    },
    {
        "id": "nkjp-ner",
        "data": {
            "description": "The NKJP-NER is based on a human-annotated part of National Corpus of Polish (NKJP). We extracted sentences with named entities of exactly one type. The task is to predict the type of the named entity.\n",
            "url": "https://klejbenchmark.com/tasks/",
            "license": "GNU GPL v.3",
            "givenLicense": "gpl-3.0",
            "language": [
                "pl"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "nli_tr",
        "data": {
            "description": "The Natural Language Inference in Turkish (NLI-TR) is a set of two large scale datasets that were obtained by translating the foundational NLI corpora (SNLI and MNLI) using Amazon Translate.\n",
            "url": "https://github.com/boun-tabi/NLI-TR",
            "license": "",
            "givenLicense": "cc-by-3.0,cc-by-4.0,cc-by-sa-3.0,mit,other",
            "language": [
                "tr"
            ],
            "categories": [
                "text-classification",
                "text-scoring"
            ],
            "tasks": [
                "natural-language-inference",
                "semantic-similarity-scoring"
            ]
        }
    },
    {
        "id": "nlu_evaluation_data",
        "data": {
            "description": "Raw part of NLU Evaluation Data. It contains 25 715 non-empty examples (original dataset has 25716 examples) from 68 unique intents belonging to 18 scenarios.\n",
            "url": "https://github.com/xliuhw/NLU-Evaluation-Data",
            "license": "Creative Commons Attribution 4.0 International License (CC BY 4.0)",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification",
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "norec",
        "data": {
            "description": "NoReC was created as part of the SANT project (Sentiment Analysis for Norwegian Text), a collaboration between the Language Technology Group (LTG) at the Department of Informatics at the University of Oslo, the Norwegian Broadcasting Corporation (NRK), Schibsted Media Group and Aller Media. This first release of the corpus comprises 35,194 reviews extracted from eight different news sources: Dagbladet, VG, Aftenposten, Bergens Tidende, Fædrelandsvennen, Stavanger Aftenblad, DinSide.no and P3.no. In terms of publishing date the reviews mainly cover the time span 2003–2017, although it also includes a handful of reviews dating back as far as 1998.\n",
            "url": "https://github.com/ljos/navnkjenner",
            "license": "",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "nb",
                "nn",
                "no"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "norne",
        "data": {
            "description": "NorNE is a manually annotated\ncorpus of named entities which extends the annotation of the existing\nNorwegian Dependency Treebank. Comprising both of the official standards of\nwritten Norwegian (Bokmål and Nynorsk), the corpus contains around 600,000\ntokens and annotates a rich set of entity types including persons,\norganizations, locations, geo-political entities, products, and events,\nin addition to a class corresponding to nominals derived from names.\n",
            "url": "https://github.com/ltgoslo/norne",
            "license": "",
            "givenLicense": "other",
            "language": [
                "no"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "norwegian_ner",
        "data": {
            "description": "Named entities Recognition dataset for Norwegian.\n",
            "url": "https://github.com/ljos/navnkjenner",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "no"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "nq_open",
        "data": {
            "description": "The NQ-Open task, introduced by Lee et.al. 2019,\nis an open domain question answering benchmark that is derived from Natural Questions.\nThe goal is to predict an English answer string for an input English question.\nAll questions can be answered using the contents of English Wikipedia.\n",
            "url": "https://efficientqa.github.io/",
            "license": "",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "nsmc",
        "data": {
            "description": "This is a movie review dataset in the Korean language. Reviews were scraped from Naver movies. The dataset construction is based on the method noted in Large movie review dataset from Maas et al., 2011.\n",
            "url": "https://github.com/e9t/nsmc/",
            "license": "CC0 1.0 Universal (CC0 1.0)",
            "givenLicense": "cc-by-2.0",
            "language": [
                "ko"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "numeric_fused_head",
        "data": {
            "description": "Fused Head constructions are noun phrases in which the head noun is missing and is said to be \"fused\" with its dependent modifier. This missing information is implicit and is important for sentence understanding.The missing heads are easily filled in by humans,  but pose a challenge for computational models.\n\nFor example, in the sentence: \"I bought 5 apples but got only 4.\", 4 is a Fused-Head, and the missing head is apples, which appear earlier in the sentence.\n\nThis is a crowd-sourced dataset of 10k numerical fused head examples (1M tokens).\n",
            "url": "https://nlp.biu.ac.il/~lazary/fh/",
            "license": "MIT",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "fused-head-identification"
            ]
        }
    },
    {
        "id": "numer_sense",
        "data": {
            "description": "NumerSense is a new numerical commonsense reasoning probing task, with a diagnostic dataset consisting of 3,145 masked-word-prediction probes.\n\nWe propose to study whether numerical commonsense knowledge can be induced from pre-trained language models like BERT, and to what extent this access to knowledge robust against adversarial examples is. We hope this will be beneficial for tasks such as knowledge base completion and open-domain question answering.\n",
            "url": "https://inklab.usc.edu/NumerSense/",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "slot-filling"
            ]
        }
    },
    {
        "id": "oclar",
        "data": {
            "description": "The researchers of OCLAR Marwan et al. (2019), they gathered Arabic costumer reviews from Google reviewsa and Zomato website \n(https://www.zomato.com/lebanon) on wide scope of domain, including restaurants, hotels, hospitals, local shops, etc.\nThe corpus finally contains 3916 reviews in 5-rating scale. For this research purpose, the positive class considers\nrating stars from 5 to 3 of 3465 reviews, and the negative class is represented from values of 1 and 2 of about 451 texts.\n",
            "url": "http://archive.ics.uci.edu/ml/datasets/Opinion+Corpus+for+Lebanese+Arabic+Reviews+%28OCLAR%29#",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "sentiment-classification",
                "sentiment-scoring"
            ]
        }
    },
    {
        "id": "offcombr",
        "data": {
            "description": "OffComBR: an annotated dataset containing for hate speech detection in Portuguese composed of news comments on the Brazilian Web.\n",
            "url": "http://www.inf.ufrgs.br/~rppelle/hatedetector/",
            "license": "Unknown",
            "givenLicense": "unknown",
            "language": [
                "pt"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-hate-speech-detection"
            ]
        }
    },
    {
        "id": "offenseval2020_tr",
        "data": {
            "description": "OffensEval-TR 2020 is a Turkish offensive language corpus. The corpus consist of randomly sampled tweets, and annotated in a similar way to OffensEval and GermEval.\n",
            "url": "https://coltekin.github.io/offensive-turkish/",
            "license": "",
            "givenLicense": "cc-by-2.0",
            "language": [
                "tr"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-offensive-language-classification"
            ]
        }
    },
    {
        "id": "offenseval_dravidian",
        "data": {
            "description": "Offensive language identification in dravidian lanaguages dataset. The goal of this task is to identify offensive language content of the code-mixed dataset of comments/posts in Dravidian Languages ( (Tamil-English, Malayalam-English, and Kannada-English)) collected from social media.\n",
            "url": "https://competitions.codalab.org/competitions/27654#learn_the_details",
            "license": "Creative Commons Attribution 4.0 International Licence",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en",
                "kn",
                "ml",
                "ta"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-offensive-language"
            ]
        }
    },
    {
        "id": "ofis_publik",
        "data": {
            "description": "Texts from the Ofis Publik ar Brezhoneg (Breton Language Board) provided by Francis Tyers\n2 languages, total number of files: 278\ntotal number of tokens: 2.12M\ntotal number of sentence fragments: 0.13M\n",
            "url": "http://opus.nlpl.eu/OfisPublik.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "br",
                "fr"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "ohsumed",
        "data": {
            "description": "The OHSUMED test collection is a set of 348,566 references from\nMEDLINE, the on-line medical information database, consisting of\ntitles and/or abstracts from 270 medical journals over a five-year\nperiod (1987-1991). The available fields are title, abstract, MeSH\nindexing terms, author, source, and publication type.\n",
            "url": "http://davis.wpi.edu/xmdv/datasets/ohsumed.html",
            "license": "CC BY-NC 4.0",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification"
            ]
        }
    },
    {
        "id": "ollie",
        "data": {
            "description": "The Ollie dataset includes two configs for the data\nused to train the Ollie informatation extraction algorithm, for 18M\nsentences and 3M sentences respectively. \n\nThis data is for academic use only. From the authors:\n\nOllie is a program that automatically identifies and extracts binary\nrelationships from English sentences. Ollie is designed for Web-scale\ninformation extraction, where target relations are not specified in\nadvance.\n\nOllie is our second-generation information extraction system . Whereas\nReVerb operates on flat sequences of tokens, Ollie works with the\ntree-like (graph with only small cycles) representation using\nStanford's compression of the dependencies. This allows Ollie to\ncapture expression that ReVerb misses, such as long-range relations.\n\nOllie also captures context that modifies a binary relation. Presently\nOllie handles attribution (He said/she believes) and enabling\nconditions (if X then).\n\nMore information is available at the Ollie homepage:\nhttps://knowitall.github.io/ollie/\n",
            "url": "https://knowitall.github.io/ollie/",
            "license": "The University of Washington acamdemic license:\nhttps://raw.githubusercontent.com/knowitall/ollie/master/LICENSE\n",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "text-to-structured"
            ],
            "tasks": [
                "relation-extraction"
            ]
        }
    },
    {
        "id": "omp",
        "data": {
            "description": "The “One Million Posts” corpus is an annotated data set consisting of\nuser comments posted to an Austrian newspaper website (in German language).\n\nDER STANDARD is an Austrian daily broadsheet newspaper. On the newspaper’s website,\nthere is a discussion section below each news article where readers engage in\nonline discussions. The data set contains a selection of user posts from the\n12 month time span from 2015-06-01 to 2016-05-31. There are 11,773 labeled and\n1,000,000 unlabeled posts in the data set. The labeled posts were annotated by\nprofessional forum moderators employed by the newspaper.\n\nThe data set contains the following data for each post:\n\n* Post ID\n* Article ID\n* Headline (max. 250 characters)\n* Main Body (max. 750 characters)\n* User ID (the user names used by the website have been re-mapped to new numeric IDs)\n* Time stamp\n* Parent post (replies give rise to tree-like discussion thread structures)\n* Status (online or deleted by a moderator)\n* Number of positive votes by other community members\n* Number of negative votes by other community members\n\nFor each article, the data set contains the following data:\n\n* Article ID\n* Publishing date\n* Topic Path (e.g.: Newsroom / Sports / Motorsports / Formula 1)\n* Title\n* Body\n\nDetailed descriptions of the post selection and annotation procedures are given in the paper.\n\n## Annotated Categories\n\nPotentially undesirable content:\n\n* Sentiment (negative/neutral/positive)\n    An important goal is to detect changes in the prevalent sentiment in a discussion, e.g.,\n    the location within the fora and the point in time where a turn from positive/neutral\n    sentiment to negative sentiment takes place.\n* Off-Topic (yes/no)\n    Posts which digress too far from the topic of the corresponding article.\n* Inappropriate (yes/no)\n    Swearwords, suggestive and obscene language, insults, threats etc.\n* Discriminating (yes/no)\n    Racist, sexist, misogynistic, homophobic, antisemitic and other misanthropic content.\n\nNeutral content that requires a reaction:\n\n* Feedback (yes/no)\n    Sometimes users ask questions or give feedback to the author of the article or the\n    newspaper in general, which may require a reply/reaction.\n\nPotentially desirable content:\n\n* Personal Stories (yes/no)\n    In certain fora, users are encouraged to share their personal stories, experiences,\n    anecdotes etc. regarding the respective topic.\n* Arguments Used (yes/no)\n    It is desirable for users to back their statements with rational argumentation,\n    reasoning and sources.\n",
            "url": "https://ofai.github.io/million-post-corpus/",
            "license": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "de"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "onestop_english",
        "data": {
            "description": "This dataset is a compilation of the OneStopEnglish corpus of texts written at three reading levels into one file.\nText documents are classified into three reading levels - ele, int, adv (Elementary, Intermediate and Advance).\nThis dataset demonstrates its usefulness for through two applica-tions - automatic  readability  assessment  and automatic text simplification.\nThe corpus consists of 189 texts, each in three versions/reading levels (567 in total).\n",
            "url": "https://github.com/nishkalavallabhi/OneStopEnglishCorpus",
            "license": "Creative Commons Attribution-ShareAlike 4.0 International License",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation",
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "text-simplification"
            ]
        }
    },
    {
        "id": "onestop_qa",
        "data": {
            "description": "OneStopQA is a multiple choice reading comprehension dataset annotated according to the STARC (Structured Annotations for Reading Comprehension) scheme. The reading materials are Guardian articles taken from the [OneStopEnglish corpus](https://github.com/nishkalavallabhi/OneStopEnglishCorpus). Each article comes in three difficulty levels, Elementary, Intermediate and Advanced. Each paragraph is annotated with three multiple choice reading comprehension questions. The reading comprehension questions can be answered based on any of the three paragraph levels.\n",
            "url": "https://github.com/berzak/onestop-qa",
            "license": "Creative Commons Attribution-ShareAlike 4.0 International License",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en-US"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "openai_humaneval",
        "data": {
            "description": "The HumanEval dataset released by OpenAI contains 164 handcrafted programming challenges together with unittests to very the viability of a proposed solution.\n",
            "url": "https://github.com/openai/human-eval",
            "license": "MIT",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-code-generation"
            ]
        }
    },
    {
        "id": "openbookqa",
        "data": {
            "description": "OpenBookQA aims to promote research in advanced question-answering, probing a deeper understanding of both the topic\n(with salient facts summarized as an open book, also provided with the dataset) and the language it is expressed in. In\nparticular, it contains questions that require multi-step reasoning, use of additional common and commonsense knowledge,\nand rich text comprehension.\nOpenBookQA is a new kind of question-answering dataset modeled after open book exams for assessing human understanding\nof a subject.\n",
            "url": "https://allenai.org/data/open-book-qa",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "openslr",
        "data": {
            "description": "OpenSLR is a site devoted to hosting speech and language resources, such as training corpora for speech recognition,\nand software related to speech recognition. We intend to be a convenient place for anyone to put resources that\nthey have created, so that they can be downloaded publicly.\n",
            "url": "https://openslr.org/",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "af",
                "bn",
                "ca",
                "en-GB",
                "en-IE",
                "en-NG",
                "es-CL",
                "es-CO",
                "es-PE",
                "es-PR",
                "eu",
                "gl",
                "gu",
                "jv",
                "km",
                "kn",
                "ml",
                "mr",
                "my",
                "ne",
                "si",
                "st",
                "su",
                "ta",
                "te",
                "tn",
                "ve",
                "xh",
                "yo"
            ],
            "categories": [
                "automatic-speech-recognition"
            ],
            "tasks": []
        }
    },
    {
        "id": "open_subtitles",
        "data": {
            "description": "This is a new collection of translated movie subtitles from http://www.opensubtitles.org/.\n\nIMPORTANT: If you use the OpenSubtitle corpus: Please, add a link to http://www.opensubtitles.org/ to your website and to your reports and publications produced with the data!\n\nThis is a slightly cleaner version of the subtitle collection using improved sentence alignment and better language checking.\n\n62 languages, 1,782 bitexts\ntotal number of files: 3,735,070\ntotal number of tokens: 22.10G\ntotal number of sentence fragments: 3.35G\n",
            "url": "http://opus.nlpl.eu/OpenSubtitles.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "af",
                "ar",
                "bg",
                "bn",
                "br",
                "bs",
                "ca",
                "cs",
                "da",
                "de",
                "el",
                "en",
                "eo",
                "es",
                "et",
                "eu",
                "fa",
                "fi",
                "fr",
                "gl",
                "he",
                "hi",
                "hr",
                "hu",
                "hy",
                "id",
                "is",
                "it",
                "ja",
                "ka",
                "kk",
                "ko",
                "lt",
                "lv",
                "mk",
                "ml",
                "ms",
                "nl",
                "no",
                "pl",
                "pt",
                "pt_br",
                "ro",
                "ru",
                "si",
                "sk",
                "sl",
                "sq",
                "sr",
                "sv",
                "ta",
                "te",
                "th",
                "tl",
                "tr",
                "uk",
                "ur",
                "vi",
                "ze_en",
                "ze_zh",
                "zh_cn",
                "zh_tw"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "openwebtext",
        "data": {
            "description": "An open-source replication of the WebText dataset from OpenAI.\n",
            "url": "https://skylion007.github.io/OpenWebTextCorpus/",
            "license": "",
            "givenLicense": "cc0-1.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "opinosis",
        "data": {
            "description": "\nThe Opinosis Opinion Dataset consists of sentences extracted from reviews for 51 topics.\nTopics and opinions are obtained from Tripadvisor, Edmunds.com and Amazon.com.\n",
            "url": "http://kavita-ganesan.com/opinosis/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "opus100",
        "data": {
            "description": "OPUS-100 is English-centric, meaning that all training pairs include English on either the source or target side.\nThe corpus covers 100 languages (including English).OPUS-100 contains approximately 55M sentence pairs.\nOf the 99 language pairs, 44 have 1M sentence pairs of training data, 73 have at least 100k, and 95 have at least 10k.\n",
            "url": "http://opus.nlpl.eu/opus-100.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "no",
                "af",
                "am",
                "an",
                "ar",
                "as",
                "az",
                "be",
                "bg",
                "bn",
                "br",
                "bs",
                "ca",
                "cs",
                "cy",
                "da",
                "de",
                "dz",
                "el",
                "en",
                "eo",
                "es",
                "et",
                "eu",
                "fa",
                "fi",
                "fr",
                "fy",
                "ga",
                "gd",
                "gl",
                "gu",
                "ha",
                "he",
                "hi",
                "hr",
                "hu",
                "hy",
                "id",
                "ig",
                "is",
                "it",
                "ja",
                "ka",
                "kk",
                "km",
                "kn",
                "ko",
                "ku",
                "ky",
                "li",
                "lt",
                "lv",
                "mg",
                "mk",
                "ml",
                "mn",
                "mr",
                "ms",
                "mt",
                "my",
                "nb",
                "ne",
                "nl",
                "nn",
                "oc",
                "or",
                "pa",
                "pl",
                "ps",
                "pt",
                "ro",
                "ru",
                "rw",
                "se",
                "sh",
                "si",
                "sk",
                "sl",
                "sq",
                "sr",
                "sv",
                "ta",
                "te",
                "tg",
                "th",
                "tk",
                "tr",
                "tt",
                "ug",
                "uk",
                "ur",
                "uz",
                "vi",
                "wa",
                "xh",
                "yi",
                "yo",
                "zh",
                "zu"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "opus_books",
        "data": {
            "description": "This is a collection of copyright free books aligned by Andras Farkas, which are available from http://www.farkastranslations.com/bilingual_books.php\nNote that the texts are rather dated due to copyright issues and that some of them are manually reviewed (check the meta-data at the top of the corpus files in XML). The source is multilingually aligned, which is available from http://www.farkastranslations.com/bilingual_books.php. In OPUS, the alignment is formally bilingual but the multilingual alignment can be recovered from the XCES sentence alignment files. Note also that the alignment units from the original source may include multi-sentence paragraphs, which are split and sentence-aligned in OPUS.\nAll texts are freely available for personal, educational and research use. Commercial use (e.g. reselling as parallel books) and mass redistribution without explicit permission are not granted. Please acknowledge the source when using the data!\n\n16 languages, 64 bitexts\ntotal number of files: 158\ntotal number of tokens: 19.50M\ntotal number of sentence fragments: 0.91M\n",
            "url": "http://opus.nlpl.eu/Books.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ca",
                "de",
                "el",
                "en",
                "eo",
                "es",
                "fi",
                "fr",
                "hu",
                "it",
                "nl",
                "no",
                "pl",
                "pt",
                "ru",
                "sv"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_dgt",
        "data": {
            "description": "A collection of translation memories provided by the JRC. Source: https://ec.europa.eu/jrc/en/language-technologies/dgt-translation-memory\n25 languages, 299 bitexts\ntotal number of files: 817,410\ntotal number of tokens: 2.13G\ntotal number of sentence fragments: 113.52M\n",
            "url": "http://opus.nlpl.eu/DGT.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "bg",
                "cs",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "fi",
                "fr",
                "ga",
                "hr",
                "hu",
                "it",
                "lt",
                "lv",
                "mt",
                "nl",
                "pl",
                "pt",
                "ro",
                "sh",
                "sk",
                "sl",
                "sv"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_dogc",
        "data": {
            "description": "This is a collection of documents from the Official Journal of the Government of Catalonia, in Catalan and Spanish languages, provided by Antoni Oliver Gonzalez from the Universitat Oberta de Catalunya.\n",
            "url": "http://opus.nlpl.eu/DOGC.php",
            "license": "",
            "givenLicense": "cc0-1.0",
            "language": [
                "ca",
                "es"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_elhuyar",
        "data": {
            "description": "Dataset provided by the foundation Elhuyar, which is having data in languages Spanish to Basque.\n",
            "url": "http://opus.nlpl.eu/Elhuyar.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "es",
                "eu"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_euconst",
        "data": {
            "description": "A parallel corpus collected from the European Constitution for 21 language.\n",
            "url": "http://opus.nlpl.eu/EUconst.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "cs",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "fi",
                "fr",
                "ga",
                "hu",
                "it",
                "lt",
                "lv",
                "mt",
                "nl",
                "pl",
                "pt",
                "sk",
                "sl",
                "sv"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_finlex",
        "data": {
            "description": "The Finlex Data Base is a comprehensive collection of legislative and other judicial information of Finland, which is available in Finnish, Swedish and partially in English. This corpus is taken from the Semantic Finlex serice that provides the Finnish and Swedish data as linked open data and also raw XML files.",
            "url": "http://opus.nlpl.eu/Finlex.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "fi",
                "sv"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_fiskmo",
        "data": {
            "description": "fiskmo, a massive parallel corpus for Finnish and Swedish.",
            "url": "http://opus.nlpl.eu/fiskmo.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "fi",
                "sv"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_gnome",
        "data": {
            "description": "A parallel corpus of GNOME localization files. Source: https://l10n.gnome.org\n\n187 languages, 12,822 bitexts\ntotal number of files: 113,344\ntotal number of tokens: 267.27M\ntotal number of sentence fragments: 58.12M\n",
            "url": "http://opus.nlpl.eu/GNOME.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "af",
                "am",
                "an",
                "ang",
                "ar",
                "ar_TN",
                "ara",
                "as",
                "ast",
                "az",
                "az_IR",
                "bal",
                "be",
                "bem",
                "bg",
                "bg_BG",
                "bn",
                "bn_IN",
                "bo",
                "br",
                "brx",
                "bs",
                "ca",
                "cat",
                "crh",
                "cs",
                "csb",
                "cy",
                "da",
                "da_DK",
                "de",
                "de_CH",
                "dv",
                "dz",
                "el",
                "en",
                "en_AU",
                "en_CA",
                "en_GB",
                "en_NZ",
                "en_US",
                "en_ZA",
                "eo",
                "es",
                "es_AR",
                "es_CL",
                "es_CO",
                "es_CR",
                "es_DO",
                "es_EC",
                "es_ES",
                "es_GT",
                "es_HN",
                "es_MX",
                "es_NI",
                "es_PA",
                "es_PE",
                "es_PR",
                "es_SV",
                "es_UY",
                "es_VE",
                "et",
                "eu",
                "fa",
                "fa_IR",
                "fi",
                "fo",
                "foo",
                "fr",
                "fur",
                "fy",
                "ga",
                "gd",
                "gl",
                "gn",
                "gr",
                "gu",
                "gv",
                "ha",
                "he",
                "hi",
                "hi_IN",
                "hr",
                "hu",
                "hy",
                "ia",
                "id",
                "ig",
                "io",
                "is",
                "it",
                "it_IT",
                "ja",
                "jbo",
                "ka",
                "kg",
                "kk",
                "km",
                "kn",
                "ko",
                "kr",
                "ks",
                "ku",
                "ky",
                "la",
                "lg",
                "li",
                "lo",
                "lt",
                "lv",
                "mai",
                "mg",
                "mi",
                "mk",
                "ml",
                "mn",
                "mr",
                "ms",
                "ms_MY",
                "mt",
                "mus",
                "my",
                "nb",
                "nb_NO",
                "nds",
                "ne",
                "nhn",
                "nl",
                "nn",
                "nn_NO",
                "n/o",
                "no_nb",
                "nqo",
                "nr",
                "nso",
                "oc",
                "or",
                "os",
                "pa",
                "pl",
                "ps",
                "pt",
                "pt_BR",
                "pt_PT",
                "quz",
                "ro",
                "ru",
                "rw",
                "si",
                "sk",
                "sl",
                "so",
                "sq",
                "sr",
                "sr_ME",
                "st",
                "sv",
                "sw",
                "szl",
                "ta",
                "te",
                "tg",
                "tg_TJ",
                "th",
                "tk",
                "tl",
                "tl_PH",
                "tmp",
                "tr",
                "tr_TR",
                "ts",
                "tt",
                "ug",
                "uk",
                "ur",
                "ur_PK",
                "uz",
                "vi",
                "vi_VN",
                "wa",
                "xh",
                "yi",
                "yo",
                "zh_CN",
                "zh_HK",
                "zh_TW",
                "zu"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_infopankki",
        "data": {
            "description": "A parallel corpus of 12 languages, 66 bitexts.\n",
            "url": "http://opus.nlpl.eu/infopankki-v1.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar",
                "en",
                "es",
                "et",
                "fa",
                "fi",
                "fr",
                "ru",
                "so",
                "sv",
                "tr",
                "zh"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_memat",
        "data": {
            "description": "Xhosa-English parallel corpora, funded by EPSRC, the Medical Machine Translation project worked on machine translation between ixiXhosa and English, with a focus on the medical domain.",
            "url": "http://opus.nlpl.eu/memat.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "xh"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_montenegrinsubs",
        "data": {
            "description": "Opus MontenegrinSubs dataset for machine translation task, for language pair en-me: english and montenegrin",
            "url": "http://opus.nlpl.eu/MontenegrinSubs.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "cnr"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_openoffice",
        "data": {
            "description": "A collection of documents from http://www.openoffice.org/.\n",
            "url": "http://opus.nlpl.eu/OpenOffice.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "de",
                "en_GB",
                "es",
                "fr",
                "ja",
                "ru",
                "sv",
                "zh_CN"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_paracrawl",
        "data": {
            "description": "Parallel corpora from Web Crawls collected in the ParaCrawl project\n40 languages, 41 bitexts\ntotal number of files: 20,995\ntotal number of tokens: 21.40G\ntotal number of sentence fragments: 1.12G\n",
            "url": "http://opus.nlpl.eu/ParaCrawl.php",
            "license": "",
            "givenLicense": "cc0-1.0",
            "language": [
                "bg",
                "ca",
                "cs",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "eu",
                "fi",
                "fr",
                "ga",
                "gl",
                "ha",
                "hr",
                "hu",
                "ig",
                "is",
                "it",
                "km",
                "lt",
                "lv",
                "mt",
                "my",
                "nb",
                "ne",
                "nl",
                "nn",
                "pl",
                "pt",
                "ro",
                "ru",
                "si",
                "sk",
                "sl",
                "so",
                "sv",
                "sw",
                "tl"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_rf",
        "data": {
            "description": "RF is a tiny parallel corpus of the Declarations of the Swedish Government and its translations. \n",
            "url": "http://opus.nlpl.eu/RF.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "de",
                "en",
                "es",
                "fr",
                "sv"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_tedtalks",
        "data": {
            "description": "This is a Croatian-English parallel corpus of transcribed and translated TED talks, originally extracted from https://wit3.fbk.eu. The corpus is compiled by Željko Agić and is taken from http://lt.ffzg.hr/zagic provided under the CC-BY-NC-SA license.\n2 languages, total number of files: 2\ntotal number of tokens: 2.81M\ntotal number of sentence fragments: 0.17M\n",
            "url": "http://opus.nlpl.eu/TedTalks.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "hr"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_ubuntu",
        "data": {
            "description": "A parallel corpus of Ubuntu localization files. Source: https://translations.launchpad.net\n244 languages, 23,988 bitexts\ntotal number of files: 30,959\ntotal number of tokens: 29.84M\ntotal number of sentence fragments: 7.73M\n",
            "url": "http://opus.nlpl.eu/Ubuntu.php",
            "license": "",
            "givenLicense": "bsd-3-clause",
            "language": [
                "ace",
                "af",
                "ak",
                "am",
                "an",
                "ang",
                "ar",
                "ar-SY",
                "ary",
                "as",
                "ast",
                "az",
                "ba",
                "bal",
                "be",
                "bem",
                "ber",
                "bg",
                "bho",
                "bn",
                "bn-IN",
                "bo",
                "br",
                "brx",
                "bs",
                "bua",
                "byn",
                "ca",
                "ce",
                "ceb",
                "chr",
                "ckb",
                "co",
                "crh",
                "cs",
                "csb",
                "cv",
                "cy",
                "da",
                "de",
                "de-AT",
                "de-DE",
                "dsb",
                "dv",
                "dz",
                "el",
                "en",
                "en-AU",
                "en-CA",
                "en-GB",
                "en-NZ",
                "en-US",
                "eo",
                "es",
                "es-AR",
                "es-CL",
                "es-CO",
                "es-CR",
                "es-DO",
                "es-EC",
                "es-ES",
                "es-GT",
                "es-HN",
                "es-MX",
                "es-NI",
                "es-PA",
                "es-PE",
                "es-PR",
                "es-SV",
                "es-UY",
                "es-VE",
                "et",
                "eu",
                "fa",
                "fa-AF",
                "ff",
                "fi",
                "fil",
                "fo",
                "fr",
                "fr-CA",
                "fr-FR",
                "frm",
                "frp",
                "fur",
                "fy",
                "ga",
                "gd",
                "gl",
                "gn",
                "grc",
                "gu",
                "guc",
                "gv",
                "ha",
                "haw",
                "he",
                "hi",
                "hil",
                "hne",
                "hr",
                "hsb",
                "ht",
                "hu",
                "hy",
                "ia",
                "id",
                "ig",
                "io",
                "is",
                "it",
                "iu",
                "ja",
                "jbo",
                "jv",
                "ka",
                "kab",
                "kg",
                "kk",
                "kl",
                "km",
                "kn",
                "ko",
                "kok",
                "ks",
                "ksh",
                "ku",
                "kw",
                "ky",
                "la",
                "lb",
                "lg",
                "li",
                "lij",
                "lld",
                "ln",
                "lo",
                "lt",
                "ltg",
                "lv",
                "mai",
                "mg",
                "mh",
                "mhr",
                "mi",
                "miq",
                "mk",
                "ml",
                "mn",
                "mr",
                "ms",
                "mt",
                "mus",
                "my",
                "nan",
                "nap",
                "nb",
                "nds",
                "ne",
                "nhn",
                "nl",
                "nl-NL",
                "nn",
                "no",
                "nso",
                "ny",
                "oc",
                "om",
                "or",
                "os",
                "pa",
                "pam",
                "pap",
                "pl",
                "pms",
                "pmy",
                "ps",
                "pt",
                "pt-BR",
                "pt-PT",
                "qu",
                "rm",
                "ro",
                "rom",
                "ru",
                "rw",
                "sa",
                "sc",
                "sco",
                "sd",
                "se",
                "shn",
                "shs",
                "si",
                "sk",
                "sl",
                "sm",
                "sml",
                "sn",
                "so",
                "son",
                "sq",
                "sr",
                "st",
                "sv",
                "sw",
                "syr",
                "szl",
                "ta",
                "ta-LK",
                "te",
                "tet",
                "tg",
                "th",
                "ti",
                "tk",
                "tl",
                "tlh",
                "tr",
                "trv",
                "ts",
                "tt",
                "ug",
                "uk",
                "ur",
                "uz",
                "ve",
                "vec",
                "vi",
                "wa",
                "wae",
                "wo",
                "xal",
                "xh",
                "yi",
                "yo",
                "zh",
                "zh-CN",
                "zh-HK",
                "zh-TW",
                "zu",
                "zza"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_wikipedia",
        "data": {
            "description": "This is a corpus of parallel sentences extracted from Wikipedia by Krzysztof Wołk and Krzysztof Marasek. Please cite the following publication if you use the data: Krzysztof Wołk and Krzysztof Marasek: Building Subject-aligned Comparable Corpora and Mining it for Truly Parallel Sentence Pairs., Procedia Technology, 18, Elsevier, p.126-132, 2014\n20 languages, 36 bitexts\ntotal number of files: 114\ntotal number of tokens: 610.13M\ntotal number of sentence fragments: 25.90M\n",
            "url": "http://opus.nlpl.eu/Wikipedia.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar",
                "bg",
                "cs",
                "de",
                "el",
                "en",
                "es",
                "fa",
                "fr",
                "he",
                "hu",
                "it",
                "nl",
                "pl",
                "pt",
                "ro",
                "ru",
                "sl",
                "tr",
                "vi"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "opus_xhosanavy",
        "data": {
            "description": "This dataset is designed for machine translation from English to Xhosa.",
            "url": "http://opus.nlpl.eu/XhosaNavy-v1.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "xh"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "orange_sum",
        "data": {
            "description": "The OrangeSum dataset was inspired by the XSum dataset. It was created by scraping the \"Orange Actu\" website: https://actu.orange.fr/. Orange S.A. is a large French multinational telecommunications corporation, with 266M customers worldwide. Scraped pages cover almost a decade from Feb 2011 to Sep 2020. They belong to five main categories: France, world, politics, automotive, and society. The society category is itself divided into 8 subcategories: health, environment, people, culture, media, high-tech, unsual (\"insolite\" in French), and miscellaneous.\n\nEach article featured a single-sentence title as well as a very brief abstract, both professionally written by the author of the article. These two fields were extracted from each page, thus creating two summarization tasks: OrangeSum Title and OrangeSum Abstract.\n",
            "url": "https://github.com/Tixierae/OrangeSum/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "fr"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "news-articles-headline-generation",
                "news-articles-summarization"
            ]
        }
    },
    {
        "id": "oscar",
        "data": {
            "description": "The Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.",
            "url": "https://oscar-corpus.com",
            "license": "\n    These data are released under this licensing scheme\n    We do not own any of the text from which these data has been extracted.\n    We license the actual packaging of these data under the Creative Commons CC0 license     (\"no rights reserved\") http://creativecommons.org/publicdomain/zero/1.0/\n    To the extent possible under law, Inria has waived all copyright     and related or neighboring rights to OSCAR\n    This work is published from: France.\n\n    Should you consider that our data contains material that is owned by you     and should therefore not be reproduced here, please:\n    * Clearly identify yourself, with detailed contact data such as an address,     telephone number or email address at which you can be contacted.\n    * Clearly identify the copyrighted work claimed to be infringed.\n    * Clearly identify the material that is claimed to be infringing and     information reasonably sufficient to allow us to locate the material.\n\n    We will comply to legitimate requests by removing the affected sources     from the next release of the corpus. ",
            "givenLicense": "cc0-1.0",
            "language": [
                "af",
                "als",
                "am",
                "an",
                "ar",
                "arz",
                "as",
                "ast",
                "av",
                "az",
                "azb",
                "ba",
                "bar",
                "bcl",
                "be",
                "bg",
                "bh",
                "bn",
                "bo",
                "bpy",
                "br",
                "bs",
                "bxr",
                "ca",
                "cbk",
                "ce",
                "ceb",
                "ckb",
                "cs",
                "cv",
                "cy",
                "da",
                "de",
                "diq",
                "dsb",
                "dv",
                "el",
                "eml",
                "en",
                "eo",
                "es",
                "et",
                "eu",
                "fa",
                "fi",
                "fr",
                "frr",
                "fy",
                "ga",
                "gd",
                "gl",
                "gn",
                "gom",
                "gu",
                "he",
                "hi",
                "hr",
                "hsb",
                "ht",
                "hu",
                "hy",
                "ia",
                "id",
                "ie",
                "ilo",
                "io",
                "is",
                "it",
                "ja",
                "jbo",
                "jv",
                "ka",
                "kk",
                "km",
                "kn",
                "ko",
                "krc",
                "ku",
                "kv",
                "kw",
                "ky",
                "la",
                "lb",
                "lez",
                "li",
                "lmo",
                "lo",
                "lrc",
                "lt",
                "lv",
                "mai",
                "mg",
                "mhr",
                "min",
                "mk",
                "ml",
                "mn",
                "mr",
                "mrj",
                "ms",
                "mt",
                "mwl",
                "my",
                "myv",
                "mzn",
                "nah",
                "nap",
                "nds",
                "ne",
                "new",
                "nl",
                "nn",
                "no",
                "oc",
                "or",
                "os",
                "pa",
                "pam",
                "pl",
                "pms",
                "pnb",
                "ps",
                "pt",
                "qu",
                "rm",
                "ro",
                "ru",
                "sa",
                "sah",
                "scn",
                "sd",
                "sh",
                "si",
                "sk",
                "sl",
                "so",
                "sq",
                "sr",
                "su",
                "sv",
                "sw",
                "ta",
                "te",
                "tg",
                "th",
                "tk",
                "tl",
                "tr",
                "tt",
                "tyv",
                "ug",
                "uk",
                "ur",
                "uz",
                "vec",
                "vi",
                "vo",
                "wa",
                "war",
                "wuu",
                "xal",
                "xmf",
                "yi",
                "yo",
                "yue",
                "zh"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "para_crawl",
        "data": {
            "description": "Web-Scale Parallel Corpora for Official European Languages.",
            "url": "https://paracrawl.eu/releases.html",
            "license": "",
            "givenLicense": "cc0-1.0",
            "language": [
                "bg",
                "cs",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "fi",
                "fr",
                "ga",
                "hr",
                "hu",
                "it",
                "lt",
                "lv",
                "mt",
                "nl",
                "pl",
                "pt",
                "ro",
                "sk",
                "sl",
                "sv"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "para_pat",
        "data": {
            "description": "ParaPat: The Multi-Million Sentences Parallel Corpus of Patents Abstracts\n\nThis dataset contains the developed parallel corpus from the open access Google\nPatents dataset in 74 language pairs, comprising more than 68 million sentences\nand 800 million tokens. Sentences were automatically aligned using the Hunalign algorithm\nfor the largest 22 language pairs, while the others were abstract (i.e. paragraph) aligned.\n\nWe demonstrate the capabilities of our corpus by training Neural Machine Translation\n(NMT) models for the main 9 language pairs, with a total of 18 models.\n",
            "url": "https://figshare.com/articles/ParaPat_The_Multi-Million_Sentences_Parallel_Corpus_of_Patents_Abstracts/12627632",
            "license": "CC BY 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "cs",
                "de",
                "el",
                "en",
                "es",
                "fr",
                "hu",
                "ja",
                "ko",
                "pt",
                "ro",
                "ru",
                "sk",
                "uk",
                "zh",
                "hu"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "parsinlu_reading_comprehension",
        "data": {
            "description": "A Persian reading comprehenion task (generating an answer, given a question and a context paragraph). \nThe questions are mined using Google auto-complete, their answers and the corresponding evidence documents are manually annotated by native speakers.    \n",
            "url": "https://github.com/persiannlp/parsinlu/",
            "license": "CC BY-NC-SA 4.0",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "fa"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "pass",
        "data": {
            "description": "PASS (Pictures without humAns for Self-Supervision) is a large-scale dataset of 1,440,191 images that does not include any humans\nand which can be used for high-quality pretraining while significantly reducing privacy concerns.\nThe PASS images are sourced from the YFCC-100M dataset.\n",
            "url": "https://www.robots.ox.ac.uk/~vgg/research/pass/",
            "license": "Creative Commons Attribution 4.0 International",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "other",
                "image-classification"
            ],
            "tasks": [
                "image-self-supervised"
            ]
        }
    },
    {
        "id": "paws",
        "data": {
            "description": "PAWS: Paraphrase Adversaries from Word Scrambling\n\nThis dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature\nthe importance of modeling structure, context, and word order information for the problem\nof paraphrase identification. The dataset has two subsets, one based on Wikipedia and the\nother one based on the Quora Question Pairs (QQP) dataset.\n\nFor further details, see the accompanying paper: PAWS: Paraphrase Adversaries from Word Scrambling\n(https://arxiv.org/abs/1904.01130)\n\nPAWS-QQP is not available due to license of QQP. It must be reconstructed by downloading the original\ndata and then running our scripts to produce the data and attach the labels.\n\nNOTE: There might be some missing or wrong labels in the dataset and we have replaced them with -1.\n",
            "url": "https://github.com/google-research-datasets/paws",
            "license": "The dataset may be freely used for any purpose, although acknowledgement of Google LLC (\"Google\") as the data source would be appreciated. The dataset is provided \"AS IS\" without any warranty, express or implied. Google disclaims all liability for any damages, direct or indirect, resulting from the use of the dataset.",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "semantic-similarity-classification",
                "semantic-similarity-scoring",
                "text-classification-other-paraphrase-identification",
                "text-scoring",
                "multi-input-text-classification"
            ]
        }
    },
    {
        "id": "paws-x",
        "data": {
            "description": "PAWS-X, a multilingual version of PAWS (Paraphrase Adversaries from Word Scrambling) for six languages.\n\nThis dataset contains 23,659 human translated PAWS evaluation pairs and 296,406 machine\ntranslated training pairs in six typologically distinct languages: French, Spanish, German,\nChinese, Japanese, and Korean. English language is available by default. All translated\npairs are sourced from examples in PAWS-Wiki.\n\nFor further details, see the accompanying paper: PAWS-X: A Cross-lingual Adversarial Dataset\nfor Paraphrase Identification (https://arxiv.org/abs/1908.11828)\n\nNOTE: There might be some missing or wrong labels in the dataset and we have replaced them with -1.\n",
            "url": "https://github.com/google-research-datasets/paws/tree/master/pawsx",
            "license": "The dataset may be freely used for any purpose, although acknowledgement of Google LLC (\"Google\") as the data source would be appreciated. The dataset is provided \"AS IS\" without any warranty, express or implied. Google disclaims all liability for any damages, direct or indirect, resulting from the use of the dataset.",
            "givenLicense": "other",
            "language": [
                "de",
                "en",
                "es",
                "fr",
                "ja",
                "ko",
                "zh"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "semantic-similarity-classification",
                "semantic-similarity-scoring",
                "text-classification-other-paraphrase-identification",
                "text-scoring",
                "multi-input-text-classification"
            ]
        }
    },
    {
        "id": "pec",
        "data": {
            "description": "A dataset of around 350K persona-based empathetic conversations. \nEach speaker is associated with a persona, which comprises multiple persona sentences. \nThe response of each conversation is empathetic.\n",
            "url": "https://github.com/zhongpeixiang/PEC",
            "license": "",
            "givenLicense": "gpl-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "text-retrieval"
            ],
            "tasks": [
                "dialogue-modeling",
                "utterance-retrieval"
            ]
        }
    },
    {
        "id": "peer_read",
        "data": {
            "description": "PearRead is a dataset of scientific peer reviews available to help researchers study this important artifact. The dataset consists of over 14K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR, as well as over 10K textual peer reviews written by experts for a subset of the papers.\n",
            "url": "https://github.com/allenai/PeerRead",
            "license": "Creative Commons Public License",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-acceptability-classification"
            ]
        }
    },
    {
        "id": "peoples_daily_ner",
        "data": {
            "description": "People's Daily NER Dataset is a commonly used dataset for Chinese NER, with\ntext from People's Daily (人民日报), the largest official newspaper.\n\nThe dataset is in BIO scheme. Entity types are: PER (person), ORG (organization)\nand LOC (location).\n",
            "url": "https://github.com/OYE93/Chinese-NLP-Corpus/tree/master/NER/People's%20Daily",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "zh"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "per_sent",
        "data": {
            "description": "Person SenTiment (PerSenT) is a crowd-sourced dataset that captures the sentiment of an author towards the main entity in a news article. This dataset contains annotation for 5.3k documents and 38k paragraphs covering 3.2k unique entities.\n\nThe dataset consists of sentiment annotations on news articles about people. For each article, annotators judge what the author’s sentiment is towards the main (target) entity of the article. The annotations also include similar judgments on paragraphs within the article.\n\nTo split the dataset, entities into 4 mutually exclusive sets. Due to the nature of news collections, some entities tend to dominate the collection. In the collection, there were four entities which were the main entity in nearly 800 articles. To avoid these entities from dominating the train or test splits, we moved them to a separate test collection. We split the remaining into a training, dev, and test sets at random. Thus our collection includes one standard test set consisting of articles drawn at random (Test Standard -- `test_random`), while the other is a test set which contains multiple articles about a small number of popular entities (Test Frequent -- `test_fixed`).\n",
            "url": "https://stonybrooknlp.github.io/PerSenT",
            "license": "Creative Commons Attribution 4.0 International License",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "persian_ner",
        "data": {
            "description": "The dataset includes 250,015 tokens and 7,682 Persian sentences in total. It is available in 3 folds to be used in turn as training and test sets. The NER tags are in IOB format.\n",
            "url": "",
            "license": "Creative Commons Attribution 4.0 International License",
            "givenLicense": "cc-by-4.0",
            "language": [
                "fa"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "pg19",
        "data": {
            "description": "This repository contains the PG-19 language modeling benchmark.\nIt includes a set of books extracted from the Project Gutenberg books library, that were published before 1919.\nIt also contains metadata of book titles and publication dates.\n\nPG-19 is over double the size of the Billion Word benchmark and contains documents that are 20X longer, on average, than the WikiText long-range language modelling benchmark.\nBooks are partitioned into a train, validation, and test set. Book metadata is stored in metadata.csv which contains (book_id, short_book_title, publication_date).\n\nUnlike prior benchmarks, we do not constrain the vocabulary size --- i.e. mapping rare words to an UNK token --- but instead release the data as an open-vocabulary benchmark. The only processing of the text that has been applied is the removal of boilerplate license text, and the mapping of offensive discriminatory words as specified by Ofcom to placeholder tokens. Users are free to model the data at the character-level, subword-level, or via any mechanism that can model an arbitrary string of text.\nTo compare models we propose to continue measuring the word-level perplexity, by calculating the total likelihood of the dataset (via any chosen subword vocabulary or character-based scheme) divided by the number of tokens --- specified below in the dataset statistics table.\nOne could use this dataset for benchmarking long-range language models, or use it to pre-train for other natural language processing tasks which require long-range reasoning, such as LAMBADA or NarrativeQA. We would not recommend using this dataset to train a general-purpose language model, e.g. for applications to a production-system dialogue agent, due to the dated linguistic style of old texts and the inherent biases present in historical writing.\n",
            "url": "https://github.com/deepmind/pg19",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "php",
        "data": {
            "description": "A parallel corpus originally extracted from http://se.php.net/download-docs.php. The original documents are written in English and have been partly translated into 21 languages. The original manuals contain about 500,000 words. The amount of actually translated texts varies for different languages between 50,000 and 380,000 words. The corpus is rather noisy and may include parts from the English original in some of the translations. The corpus is tokenized and each language pair has been sentence aligned.\n\n23 languages, 252 bitexts\ntotal number of files: 71,414\ntotal number of tokens: 3.28M\ntotal number of sentence fragments: 1.38M\n",
            "url": "http://opus.nlpl.eu/PHP.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "cs",
                "de",
                "en",
                "es",
                "fi",
                "fr",
                "he",
                "hu",
                "it",
                "ja",
                "ko",
                "nl",
                "pl",
                "pt_BR",
                "ro",
                "ru",
                "sk",
                "sl",
                "sv",
                "tr",
                "tw",
                "zh",
                "zh_TW"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "piaf",
        "data": {
            "description": "Piaf is a reading comprehension dataset. This version, published in February 2020, contains 3835 questions on French Wikipedia.\n",
            "url": "https://piaf.etalab.studio",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "fr-FR"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "pib",
        "data": {
            "description": "Sentence aligned parallel corpus between 11 Indian Languages, crawled and extracted from the press information bureau\nwebsite.\n",
            "url": "http://preon.iiit.ac.in/~jerin/bhasha/",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en",
                "hi",
                "ta",
                "te",
                "ml",
                "ur",
                "bn",
                "mr",
                "gu",
                "or",
                "pa"
            ],
            "categories": [
                "translation",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "piqa",
        "data": {
            "description": "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick?\nQuestions requiring this kind of physical commonsense pose a challenge to state-of-the-art\nnatural language understanding systems. The PIQA dataset introduces the task of physical commonsense reasoning\nand a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA.\n\nPhysical commonsense knowledge is a major challenge on the road to true AI-completeness,\nincluding robots that interact with the world and understand natural language.\n\nThe dataset focuses on everyday situations with a preference for atypical solutions.\nThe dataset is inspired by instructables.com, which provides users with instructions on how to build, craft,\nbake, or manipulate objects using everyday materials.\n\nThe underlying task is formualted as multiple choice question answering:\ngiven a question `q` and two possible solutions `s1`, `s2`, a model or\na human must choose the most appropriate solution, of which exactly one is correct.\nThe dataset is further cleaned of basic artifacts using the AFLite algorithm which is an improvement of\nadversarial filtering. The dataset contains 16,000 examples for training, 2,000 for development and 3,000 for testing.\n",
            "url": "https://yonatanbisk.com/piqa/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "pn_summary",
        "data": {
            "description": "A well-structured summarization dataset for the Persian language consists of 93,207 records. It is prepared for Abstractive/Extractive tasks (like cnn_dailymail for English). It can also be used in other scopes like Text Generation, Title Generation, and News Category Classification.\nIt is imperative to consider that the newlines were replaced with the `[n]` symbol. Please interpret them into normal newlines (for ex. `t.replace(\"[n]\", \"\n\")`) and then use them for your purposes.\n",
            "url": "https://github.com/hooshvare/pn-summary",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "fa"
            ],
            "categories": [
                "summarization",
                "text-classification"
            ],
            "tasks": [
                "news-articles-summarization",
                "news-articles-headline-generation",
                "text-simplification",
                "topic-classification"
            ]
        }
    },
    {
        "id": "poem_sentiment",
        "data": {
            "description": "Poem Sentiment is a sentiment dataset of poem verses from Project Gutenberg. This dataset can be used for tasks such as sentiment classification or style transfer for poems.\n",
            "url": "https://github.com/google-research-datasets/poem-sentiment",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "polemo2",
        "data": {
            "description": "The PolEmo2.0 is a set of online reviews from medicine and hotels domains. The task is to predict the sentiment of a review. There are two separate test sets, to allow for in-domain (medicine and hotels) as well as out-of-domain (products and university) validation.\n",
            "url": "https://clarin-pl.eu/dspace/handle/11321/710",
            "license": "CC BY-NC-SA 4.0",
            "givenLicense": "bsd-3-clause",
            "language": [
                "pl"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "poleval2019_cyberbullying",
        "data": {
            "description": "    In Task 6-1, the participants are to distinguish between normal/non-harmful tweets (class: 0) and tweets\n    that contain any kind of harmful information (class: 1). This includes cyberbullying, hate speech and\n    related phenomena.\n\n    In Task 6-2, the participants shall distinguish between three classes of tweets: 0 (non-harmful),\n    1 (cyberbullying), 2 (hate-speech). There are various definitions of both cyberbullying and hate-speech,\n    some of them even putting those two phenomena in the same group. The specific conditions on which we based\n    our annotations for both cyberbullying and hate-speech, which have been worked out during ten years of research\n    will be summarized in an introductory paper for the task, however, the main and definitive condition to 1\n    distinguish the two is whether the harmful action is addressed towards a private person(s) (cyberbullying),\n    or a public person/entity/large group (hate-speech).\n",
            "url": "http://2019.poleval.pl/index.php/tasks/task6",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "pl"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification"
            ]
        }
    },
    {
        "id": "poleval2019_mt",
        "data": {
            "description": "PolEval is a SemEval-inspired evaluation campaign for natural language processing tools for Polish.Submitted solutions compete against one another within certain tasks selected by organizers, using available data and are evaluated according topre-established procedures. One of the tasks in PolEval-2019 was Machine Translation (Task-4).\nThe task is to train as good as possible machine translation system, using any technology,with limited textual resources.The competition will be done for 2 language pairs, more popular English-Polish (into Polish direction) and pair that can be called low resourcedRussian-Polish (in both directions).\n\nHere, Polish-English is also made available to allow for training in both directions. However, the test data is ONLY available for English-Polish.\n",
            "url": "http://2019.poleval.pl/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "pl",
                "ru"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "polsum",
        "data": {
            "description": "Polish Summaries Corpus: the corpus of Polish news summaries.\n",
            "url": "http://zil.ipipan.waw.pl/PolishSummariesCorpus",
            "license": "CC BY v.3",
            "givenLicense": "cc-by-3.0",
            "language": [
                "pl"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "news-articles-summarization"
            ]
        }
    },
    {
        "id": "polyglot_ner",
        "data": {
            "description": "Polyglot-NER\nA training dataset automatically generated from Wikipedia and Freebase the task\nof named entity recognition. The dataset contains the basic Wikipedia based\ntraining data for 40 languages we have (with coreference resolution) for the task of\nnamed entity recognition. The details of the procedure of generating them is outlined in\nSection 3 of the paper (https://arxiv.org/abs/1410.3791). Each config contains the data\ncorresponding to a different language. For example, \"es\" includes only spanish examples.\n",
            "url": "https://sites.google.com/site/rmyeid/projects/polylgot-ner",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ca",
                "de",
                "es",
                "fi",
                "hi",
                "id",
                "ko",
                "ms",
                "pl",
                "ru",
                "sr",
                "tl",
                "vi",
                "ar",
                "cs",
                "el",
                "et",
                "fr",
                "hr",
                "it",
                "lt",
                "nl",
                "pt",
                "sk",
                "sv",
                "tr",
                "zh",
                "bg",
                "da",
                "en",
                "fa",
                "he",
                "hu",
                "ja",
                "lv",
                "no",
                "ro",
                "sl",
                "th",
                "uk"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "prachathai67k",
        "data": {
            "description": "`prachathai-67k`: News Article Corpus and Multi-label Text Classificdation from Prachathai.com\nThe prachathai-67k dataset was scraped from the news site Prachathai.\nWe filtered out those articles with less than 500 characters of body text, mostly images and cartoons.\nIt contains 67,889 articles wtih 12 curated tags from August 24, 2004 to November 15, 2018.\nThe dataset was originally scraped by @lukkiddd and cleaned by @cstorm125.\nYou can also see preliminary exploration at https://github.com/PyThaiNLP/prachathai-67k/blob/master/exploration.ipynb\n",
            "url": "https://github.com/PyThaiNLP/prachathai-67k/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "topic-classification"
            ]
        }
    },
    {
        "id": "pragmeval",
        "data": {
            "description": "Evaluation of language understanding with a 11 datasets benchmark focusing on discourse and pragmatics\n",
            "url": "",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "proto_qa",
        "data": {
            "description": "This dataset is for studying computational models trained to reason about prototypical situations. Using deterministic filtering a sampling from a larger set of all transcriptions was built. It contains 9789 instances where each instance represents a survey question from Family Feud game. Each instance exactly is a question, a set of answers, and a count associated with each answer.\nEach line is a json dictionary, in which:\n1. question - contains the question (in original and a normalized form)\n2. answerstrings - contains the original answers provided by survey respondents (when available), along with the counts for each string. Because the FamilyFeud data has only cluster names rather than strings, those cluster names are included with 0 weight.\n3. answer-clusters - lists clusters, with the count of each cluster and the strings included in that cluster. Each cluster is given a unique ID that can be linked to in the assessment files.\n\n",
            "url": "https://github.com/iesl/protoqa-data",
            "license": "cc-by-4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa",
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "psc",
        "data": {
            "description": "The Polish Summaries Corpus contains news articles and their summaries. We used summaries of the same article as positive pairs and sampled the most similar summaries of different articles as negatives.\n",
            "url": "http://zil.ipipan.waw.pl/PolishSummariesCorpus",
            "license": "CC BY-SA 3.0",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "pl"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "news-articles-summarization"
            ]
        }
    },
    {
        "id": "ptb_text_only",
        "data": {
            "description": "This is the Penn Treebank Project: Release 2 CDROM, featuring a million words of 1989 Wall Street Journal material. This corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure.\n",
            "url": "https://catalog.ldc.upenn.edu/LDC99T42",
            "license": "LDC User Agreement for Non-Members",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "pubmed_qa",
        "data": {
            "description": "PubMedQA is a novel biomedical question answering (QA) dataset collected from PubMed abstracts.\nThe task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative\nstatins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts.\nPubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances.\nEach PubMedQA instance is composed of (1) a question which is either an existing research article\ntitle or derived from one, (2) a context which is the corresponding abstract without its conclusion,\n(3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question,\nand (4) a yes/no/maybe answer which summarizes the conclusion.\nPubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their\nquantitative contents, is required to answer the questions.\n",
            "url": "https://pubmedqa.github.io/",
            "license": "MIT License\nCopyright (c) 2019 pubmedqa\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "py_ast",
        "data": {
            "description": "dataset consisting of parsed Parsed ASTs that were used to train and\nevaluate the DeepSyn tool.\nThe Python programs are collected from GitHub repositories\nby removing duplicate files, removing project forks (copy of another existing repository)\n,keeping only programs that parse and have at most 30'000 nodes in the AST and\nwe aim to remove obfuscated files",
            "url": "https://www.sri.inf.ethz.ch/py150",
            "license": "",
            "givenLicense": "bsd-2-clause,mit",
            "language": [
                "code"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "text-generation",
                "fill-mask",
                "text-generation-other-code-modeling"
            ],
            "tasks": []
        }
    },
    {
        "id": "qa4mre",
        "data": {
            "description": "\nQA4MRE dataset was created for the CLEF 2011/2012/2013 shared tasks to promote research in \nquestion answering and reading comprehension. The dataset contains a supporting \npassage and a set of questions corresponding to the passage. Multiple options \nfor answers are provided for each question, of which only one is correct. The \ntraining and test datasets are available for the main track.\nAdditional gold standard documents are available for two pilot studies: one on \nalzheimers data, and the other on entrance exams data.\n",
            "url": "http://nlp.uned.es/clef-qa/repository/pastCampaigns.php",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "qangaroo",
        "data": {
            "description": "  We have created two new Reading Comprehension datasets focussing on multi-hop (alias multi-step) inference.\n\nSeveral pieces of information often jointly imply another fact. In multi-hop inference, a new fact is derived by combining facts via a chain of multiple steps.\n\nOur aim is to build Reading Comprehension methods that perform multi-hop inference on text, where individual facts are spread out across different documents.\n\nThe two QAngaroo datasets provide a training and evaluation resource for such methods.\n",
            "url": "http://qangaroo.cs.ucl.ac.uk/index.html",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "qanta",
        "data": {
            "description": "\nThe Qanta dataset is a question answering dataset based on the academic trivia game Quizbowl.\n",
            "url": "http://www.qanta.org/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "qasc",
        "data": {
            "description": "\nQASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice \nquestions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.\n",
            "url": "https://allenai.org/data/qasc",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "qasper",
        "data": {
            "description": "A dataset containing 1585 papers with 5049 information-seeking questions asked by regular readers of NLP papers, and answered by a separate set of NLP practitioners.\n",
            "url": "https://allenai.org/data/qasper",
            "license": "CC BY 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en-US"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "closed-domain-qa"
            ]
        }
    },
    {
        "id": "qa_srl",
        "data": {
            "description": "The dataset contains question-answer pairs to model verbal predicate-argument structure. The questions start with wh-words (Who, What, Where, What, etc.) and contain a verb predicate in the sentence; the answers are phrases in the sentence. \nThere were 2 datsets used in the paper, newswire and wikipedia. Unfortunately the newswiredataset is built from CoNLL-2009 English training set that is covered under license\nThus, we are providing only Wikipedia training set here. Please check README.md for more details on newswire dataset.\nFor the Wikipedia domain, randomly sampled sentences from the English Wikipedia (excluding questions and sentences with fewer than 10 or more than 60 words) were taken.\nThis new dataset is designed to solve this great NLP task and is crafted with a lot of care. \n",
            "url": "https://dada.cs.washington.edu/qasrl/#page-top",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa",
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "qa_zre",
        "data": {
            "description": "A dataset reducing relation extraction to simple reading comprehension questions\n",
            "url": "http://nlp.cs.washington.edu/zeroshot",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "question-answering-other-zero-shot-relation-extraction"
            ]
        }
    },
    {
        "id": "qed",
        "data": {
            "description": "QED, is a linguistically informed, extensible framework for explanations in question answering. A QED explanation specifies the relationship between a question and answer according to formal semantic notions such as referential equality, sentencehood, and entailment. It is an expertannotated dataset of QED explanations built upon a subset of the Google Natural Questions dataset.\n",
            "url": "https://github.com/google-research-datasets/QED",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "question-answering-other-explanations-in-question-answering"
            ]
        }
    },
    {
        "id": "qed_amara",
        "data": {
            "description": "The QCRI Educational Domain Corpus (formerly QCRI AMARA Corpus) is an open multilingual collection of subtitles for educational videos and lectures collaboratively transcribed and translated over the AMARA web-based platform.\nDeveloped by: Qatar Computing Research Institute, Arabic Language Technologies Group\nThe QED Corpus is made public for RESEARCH purpose only.\nThe corpus is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Copyright Qatar Computing Research Institute. All rights reserved.\n225 languages, 9,291 bitexts\ntotal number of files: 271,558\ntotal number of tokens: 371.76M\ntotal number of sentence fragments: 30.93M\n",
            "url": "http://opus.nlpl.eu/QED.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "aa",
                "ab",
                "ae",
                "aeb",
                "af",
                "aka",
                "amh",
                "an",
                "ar",
                "arq",
                "arz",
                "as",
                "ase",
                "ast",
                "av",
                "ay",
                "az",
                "ba",
                "bam",
                "be",
                "ber",
                "bg",
                "bh",
                "bi",
                "bn",
                "bnt",
                "bo",
                "br",
                "bs",
                "bug",
                "ca",
                "ce",
                "ceb",
                "ch",
                "cho",
                "cku",
                "cnh",
                "co",
                "cr",
                "cs",
                "cu",
                "cv",
                "cy",
                "da",
                "de",
                "dv",
                "dz",
                "ee",
                "efi",
                "el",
                "en",
                "eo",
                "es",
                "et",
                "eu",
                "fa",
                "ff",
                "fi",
                "fil",
                "fj",
                "fo",
                "fr",
                "ful",
                "ga",
                "gd",
                "gl",
                "gn",
                "gu",
                "hai",
                "hau",
                "haw",
                "haz",
                "hb",
                "hch",
                "he",
                "hi",
                "ho",
                "hr",
                "ht",
                "hu",
                "hup",
                "hus",
                "hy",
                "hz",
                "ia",
                "ibo",
                "id",
                "ie",
                "ik",
                "inh",
                "io",
                "iro",
                "is",
                "it",
                "iu",
                "ja",
                "jv",
                "ka",
                "kar",
                "kau",
                "kik",
                "kin",
                "kj",
                "kk",
                "kl",
                "km",
                "kn",
                "ko",
                "ksh",
                "ku",
                "kv",
                "kw",
                "ky",
                "la",
                "lb",
                "lg",
                "li",
                "lin",
                "lkt",
                "lld",
                "lo",
                "lt",
                "ltg",
                "lu",
                "luo",
                "luy",
                "lv",
                "mad",
                "mfe",
                "mi",
                "mk",
                "ml",
                "mlg",
                "mn",
                "mni",
                "mo",
                "moh",
                "mos",
                "mr",
                "ms",
                "mt",
                "mus",
                "my",
                "nb",
                "nci",
                "nd",
                "ne",
                "nl",
                "nn",
                "nso",
                "nv",
                "nya",
                "oc",
                "or",
                "orm",
                "pam",
                "pan",
                "pap",
                "pi",
                "pl",
                "pnb",
                "prs",
                "ps",
                "pt",
                "que",
                "rm",
                "ro",
                "ru",
                "run",
                "rup",
                "ry",
                "sa",
                "sc",
                "scn",
                "sco",
                "sd",
                "sg",
                "sgn",
                "sh",
                "si",
                "sk",
                "sl",
                "sm",
                "sna",
                "som",
                "sot",
                "sq",
                "sr",
                "srp",
                "sv",
                "swa",
                "szl",
                "ta",
                "te",
                "tet",
                "tg",
                "th",
                "tir",
                "tk",
                "tl",
                "tlh",
                "to",
                "tr",
                "ts",
                "tt",
                "tw",
                "ug",
                "uk",
                "umb",
                "ur",
                "uz",
                "ve",
                "vi",
                "vls",
                "vo",
                "wa",
                "wol",
                "xh",
                "yaq",
                "yi",
                "yor",
                "za",
                "zam",
                "zh",
                "zul"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "quac",
        "data": {
            "description": "Question Answering in Context is a dataset for modeling, understanding,\nand participating in information seeking dialog. Data instances consist\nof an interactive dialog between two crowd workers: (1) a student who\nposes a sequence of freeform questions to learn as much as possible\nabout a hidden Wikipedia text, and (2) a teacher who answers the questions\nby providing short excerpts (spans) from the text. QuAC introduces\nchallenges not found in existing machine comprehension datasets: its\nquestions are often more open-ended, unanswerable, or only meaningful\nwithin the dialog context.\n",
            "url": "https://quac.ai/",
            "license": "MIT",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling",
                "extractive-qa"
            ]
        }
    },
    {
        "id": "quail",
        "data": {
            "description": "QuAIL is a  reading comprehension dataset. QuAIL contains 15K multi-choice questions in texts 300-350 tokens long 4 domains (news, user stories, fiction, blogs).QuAIL is balanced and annotated for question types.",
            "url": "https://text-machine-lab.github.io/blog/2020/quail/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "quarel",
        "data": {
            "description": "\nQuaRel is a crowdsourced dataset of 2771 multiple-choice story questions, including their logical forms.\n",
            "url": "https://allenai.org/data/quarel",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "quartz",
        "data": {
            "description": "QuaRTz is a crowdsourced dataset of 3864 multiple-choice questions about open domain qualitative relationships. Each \nquestion is paired with one of 405 different background sentences (sometimes short paragraphs).\nThe QuaRTz dataset V1 contains 3864 questions about open domain qualitative relationships. Each question is paired with \none of 405 different background sentences (sometimes short paragraphs).\n\nThe dataset is split into train (2696), dev (384) and test (784). A background sentence will only appear in a single split.\n",
            "url": "https://allenai.org/data/quartz",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "quickdraw",
        "data": {
            "description": "The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!.\nThe drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located.\n",
            "url": "https://quickdraw.withgoogle.com/data",
            "license": "CC BY 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification"
            ],
            "tasks": [
                "multi-class-image-classification"
            ]
        }
    },
    {
        "id": "quora",
        "data": {
            "description": "The Quora dataset is composed of question pairs, and the task is to determine if the questions are paraphrases of each other (have the same meaning).",
            "url": "https://www.kaggle.com/c/quora-question-pairs",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "quoref",
        "data": {
            "description": "Quoref is a QA dataset which tests the coreferential reasoning capability of reading comprehension systems. In this \nspan-selection benchmark containing 24K questions over 4.7K paragraphs from Wikipedia, a system must resolve hard \ncoreferences before selecting the appropriate span(s) in the paragraphs for answering questions.\n",
            "url": "https://leaderboard.allenai.org/quoref/submissions/get-started",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "race",
        "data": {
            "description": "Race is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The\n dataset is collected from English examinations in China, which are designed for middle school and high school students.\nThe dataset can be served as the training and test sets for machine comprehension.\n\n",
            "url": "http://www.cs.cmu.edu/~glai1/data/race/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "reasoning_bg",
        "data": {
            "description": "This new dataset is designed to do reading comprehension in Bulgarian language. \n",
            "url": "https://github.com/mhardalov/bg-reason-BERT",
            "license": "Apache-2.0 License",
            "givenLicense": "apache-2.0",
            "language": [
                "bg"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "recipe_nlg",
        "data": {
            "description": "The dataset contains 2231142 cooking recipes (>2 millions). It's processed in more careful way and provides more samples than any other dataset in the area.\n",
            "url": "https://recipenlg.cs.put.poznan.pl/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation",
                "text-generation",
                "fill-mask",
                "text-retrieval"
            ],
            "tasks": [
                "document-retrieval",
                "entity-linking-retrieval",
                "explanation-generation",
                "language-modeling",
                "masked-language-modeling",
                "summarization"
            ]
        }
    },
    {
        "id": "reclor",
        "data": {
            "description": "Logical reasoning is an important ability to examine, analyze, and critically evaluate arguments as they occur in ordinary\nlanguage as the definition from LSAC. ReClor is a dataset extracted from logical reasoning questions of standardized graduate\nadmission examinations. Empirical results show that the state-of-the-art models struggle on ReClor with poor performance\nindicating more research is needed to essentially enhance the logical reasoning ability of current models. We hope this\ndataset could help push Machine Reading Comprehension (MRC) towards more complicated reasonin\n",
            "url": "http://whyu.me/reclor/",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "red_caps",
        "data": {
            "description": "RedCaps is a large-scale dataset of 12M image-text pairs collected from Reddit.\nImages and captions from Reddit depict and describe a wide variety of objects and scenes.\nThe data is collected from a manually curated set of subreddits (350 total),\nwhich give coarse image labels and allow steering of the dataset composition\nwithout labeling individual instances.\n",
            "url": "https://redcaps.xyz/",
            "license": "CC BY 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "image-to-text"
            ],
            "tasks": [
                "image-captioning"
            ]
        }
    },
    {
        "id": "reddit",
        "data": {
            "description": "\nThis corpus contains preprocessed posts from the Reddit dataset.\nThe dataset consists of 3,848,330 posts with an average length of 270 words for content,\nand 28 words for the summary.\n\nFeatures includes strings: author, body, normalizedBody, content, summary, subreddit, subreddit_id.\nContent is used as document and summary is used as summary.\n",
            "url": "https://github.com/webis-de/webis-tldr-17-corpus",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "summarization-other-reddit-posts-summarization"
            ]
        }
    },
    {
        "id": "reddit_tifu",
        "data": {
            "description": "\nReddit dataset, where TIFU denotes the name of subbreddit /r/tifu.\nAs defined in the publication, styel \"short\" uses title as summary and\n\"long\" uses tldr as summary.\n\nFeatures includes:\n  - document: post text without tldr.\n  - tldr: tldr line.\n  - title: trimmed title without tldr.\n  - ups: upvotes.\n  - score: score.\n  - num_comments: number of comments.\n  - upvote_ratio: upvote ratio.\n",
            "url": "https://github.com/ctr4si/MMN",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "summarization-other-reddit-posts-summarization"
            ]
        }
    },
    {
        "id": "re_dial",
        "data": {
            "description": "ReDial (Recommendation Dialogues) is an annotated dataset of dialogues, where users\nrecommend movies to each other. The dataset was collected by a team of researchers working at\nPolytechnique Montréal, MILA – Quebec AI Institute, Microsoft Research Montréal, HEC Montreal, and Element AI.\n\nThe dataset allows research at the intersection of goal-directed dialogue systems\n(such as restaurant recommendation) and free-form (also called “chit-chat”) dialogue systems.\n",
            "url": "https://redialdata.github.io/website/",
            "license": "CC BY 4.0 License.",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "other",
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification",
                "text-classification-other-dialogue-sentiment-classification"
            ]
        }
    },
    {
        "id": "refresd",
        "data": {
            "description": "The Rationalized English-French Semantic Divergences (REFreSD) dataset consists of 1,039 \nEnglish-French sentence-pairs annotated with sentence-level divergence judgments and token-level \nrationales. For any questions, write to ebriakou@cs.umd.edu.\n",
            "url": "https://github.com/Elbria/xling-SemDiv/tree/master/REFreSD",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en",
                "fr"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "semantic-similarity-classification",
                "semantic-similarity-scoring",
                "text-scoring"
            ]
        }
    },
    {
        "id": "reuters21578",
        "data": {
            "description": "The Reuters-21578 dataset  is one of the most widely used data collections for text\ncategorization research. It is collected from the Reuters financial newswire service in 1987.\n",
            "url": "https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "riddle_sense",
        "data": {
            "description": "Answering such a riddle-style question is a challenging cognitive process, in that it requires \ncomplex commonsense reasoning abilities, an understanding of figurative language, and counterfactual reasoning \nskills, which are all important abilities for advanced natural language understanding (NLU). However, \nthere is currently no dedicated datasets aiming to test these abilities. Herein, we present RiddleSense, \na new multiple-choice question answering task, which comes with the first large dataset (5.7k examples) for answering \nriddle-style commonsense questions. We systematically evaluate a wide range of models over the challenge, \nand point out that there is a large gap between the best-supervised model and human performance — suggesting \nintriguing future research in the direction of higher-order commonsense reasoning and linguistic creativity towards \nbuilding advanced NLU systems. \n\n",
            "url": "https://inklab.usc.edu/RiddleSense/",
            "license": "",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "roman_urdu",
        "data": {
            "description": "This is an extensive compilation of Roman Urdu Dataset (Urdu written in Latin/Roman script) tagged for sentiment analysis.\n",
            "url": "https://archive.ics.uci.edu/ml/datasets/Roman+Urdu+Data+Set",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ur"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "roman_urdu_hate_speech",
        "data": {
            "description": " The Roman Urdu Hate-Speech and Offensive Language Detection (RUHSOLD) dataset is a  Roman Urdu dataset of tweets annotated by experts in the relevant language.  The authors develop the gold-standard for two sub-tasks.  First sub-task is based on binary labels of Hate-Offensive content and Normal content (i.e., inoffensive language).  These labels are self-explanatory.  The authors refer to this sub-task as coarse-grained classification.  Second sub-task defines Hate-Offensive content with  four labels at a granular level.  These labels are the most relevant for the demographic of users who converse in RU and  are defined in related literature. The authors refer to this sub-task as fine-grained classification.  The objective behind creating two gold-standards is to enable the researchers to evaluate the hate speech detection  approaches on both easier (coarse-grained) and challenging (fine-grained) scenarios. ",
            "url": "https://github.com/haroonshakeel/roman_urdu_hate_speech",
            "license": "MIT License",
            "givenLicense": "mit",
            "language": [
                "ur"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "text-classification-other-binary"
            ]
        }
    },
    {
        "id": "ronec",
        "data": {
            "description": "RONEC - the Romanian Named Entity Corpus, at version 2.0, holds 12330 sentences with over 0.5M tokens, annotated with 15 classes, to a total of 80.283 distinctly annotated entities. It is used for named entity recognition and represents the largest Romanian NER corpus to date.\n",
            "url": "https://github.com/dumitrescustefan/ronec",
            "license": "MIT License",
            "givenLicense": "mit",
            "language": [
                "ro"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "ropes",
        "data": {
            "description": "ROPES (Reasoning Over Paragraph Effects in Situations) is a QA dataset\nwhich tests a system's ability to apply knowledge from a passage\nof text to a new situation. A system is presented a background\npassage containing a causal or qualitative relation(s) (e.g.,\n\"animal pollinators increase efficiency of fertilization in flowers\"),\na novel situation that uses this background, and questions that require\nreasoning about effects of the relationships in the background\npassage in the background of the situation.\n",
            "url": "https://allenai.org/data/ropes",
            "license": "CC BY 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "ro_sent",
        "data": {
            "description": "This dataset is a Romanian Sentiment Analysis dataset.\nIt is present in a processed form, as used by the authors of `Romanian Transformers`\nin their examples and based on the original data present in\n`https://github.com/katakonst/sentiment-analysis-tensorflow`.\n",
            "url": "https://github.com/dumitrescustefan/Romanian-Transformers/tree/examples/examples/sentiment_analysis",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ro"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "ro_sts",
        "data": {
            "description": "The RO-STS (Romanian Semantic Textual Similarity) dataset contains 8628 pairs of sentences with their similarity score. It is a high-quality translation of the STS benchmark dataset.\n",
            "url": "https://github.com/dumitrescustefan/RO-STS/",
            "license": "CC BY-SA 4.0 License",
            "givenLicense": "cc-by-4.0",
            "language": [
                "ro"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "semantic-similarity-scoring"
            ]
        }
    },
    {
        "id": "ro_sts_parallel",
        "data": {
            "description": "The RO-STS-Parallel (a Parallel Romanian English dataset - translation of the Semantic Textual Similarity) contains 17256 sentences in Romanian and English. It is a high-quality translation of the English STS benchmark dataset into Romanian.\n",
            "url": "https://github.com/dumitrescustefan/RO-STS",
            "license": "CC BY-SA 4.0 License",
            "givenLicense": "cc-by-4.0",
            "language": [
                "ro",
                "en"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "rotten_tomatoes",
        "data": {
            "description": "Movie Review Dataset.\nThis is a dataset of containing 5,331 positive and 5,331 negative processed\nsentences from Rotten Tomatoes movie reviews. This data was first used in Bo\nPang and Lillian Lee, ``Seeing stars: Exploiting class relationships for\nsentiment categorization with respect to rating scales.'', Proceedings of the\nACL, 2005.\n",
            "url": "http://www.cs.cornell.edu/people/pabo/movie-review-data/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "russian_super_glue",
        "data": {
            "description": "Recent advances in the field of universal language models and transformers require the development of a methodology for\ntheir broad diagnostics and testing for general intellectual skills - detection of natural language inference,\ncommonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first\ntime, a benchmark of nine tasks, collected and organized analogically to the SuperGLUE methodology, was developed from\nscratch for the Russian language. We provide baselines, human level evaluation, an open-source framework for evaluating\nmodels and an overall leaderboard of transformer models for the Russian language.\n\"LiDiRus (Linguistic Diagnostic for Russian) is a diagnostic dataset that covers a large volume of linguistic phenomena,\nwhile allowing you to evaluate information systems on a simple test of textual entailment recognition.\nSee more details diagnostics.\n",
            "url": "https://russiansuperglue.com/tasks/task_info/LiDiRus",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "ru-RU"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference",
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "rvl_cdip",
        "data": {
            "description": "The RVL-CDIP (Ryerson Vision Lab Complex Document Information Processing) dataset consists of 400,000 grayscale images in 16 classes, with 25,000 images per class. There are 320,000 training images, 40,000 validation images, and 40,000 test images.\n",
            "url": "https://www.cs.cmu.edu/~aharley/rvl-cdip/",
            "license": "https://www.industrydocuments.ucsf.edu/help/copyright/",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification"
            ],
            "tasks": [
                "multi-class-image-classification"
            ]
        }
    },
    {
        "id": "s2orc",
        "data": {
            "description": "A large corpus of 81.1M English-language academic papers spanning many academic disciplines.\nRich metadata, paper abstracts, resolved bibliographic references, as well as structured full\ntext for 8.1M open access papers. Full text annotated with automatically-detected inline mentions of\ncitations, figures, and tables, each linked to their corresponding paper objects. Aggregated papers\nfrom hundreds of academic publishers and digital archives into a unified source, and create the largest\npublicly-available collection of machine-readable academic text to date.\n",
            "url": "http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/",
            "license": "Semantic Scholar Open Research Corpus is licensed under ODC-BY.",
            "givenLicense": "cc-by-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "other",
                "text-generation",
                "fill-mask",
                "text-classification"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling",
                "multi-class-classification",
                "multi-label-classification",
                "other-other-citation-recommendation"
            ]
        }
    },
    {
        "id": "samsum",
        "data": {
            "description": "\nSAMSum Corpus contains over 16k chat dialogues with manually annotated\nsummaries.\nThere are two features:\n  - dialogue: text of dialogue.\n  - summary: human written summary of the dialogue.\n  - id: id of a example.\n",
            "url": "https://arxiv.org/abs/1911.12237",
            "license": "CC BY-NC-ND 4.0",
            "givenLicense": "cc-by-nc-nd-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "summarization-other-conversations-summarization"
            ]
        }
    },
    {
        "id": "sanskrit_classic",
        "data": {
            "description": "This dataset combines some of the classical Sanskrit texts.\n",
            "url": "https://github.com/parmarsuraj99/hf_datasets/tree/master/sanskrit_classic",
            "license": "",
            "givenLicense": "other",
            "language": [
                "sa"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "saudinewsnet",
        "data": {
            "description": "The dataset contains a set of 31,030 Arabic newspaper articles alongwith metadata, extracted from various online Saudi newspapers and written in MSA.",
            "url": "https://github.com/parallelfold/SaudiNewsNet",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "sberquad",
        "data": {
            "description": "Sber Question Answering Dataset (SberQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. Russian original analogue presented in Sberbank Data Science Journey 2017.\n",
            "url": "",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ru"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "sbu_captions",
        "data": {
            "description": "The SBU Captioned Photo Dataset is a collection of over 1 million images with associated text descriptions extracted from Flicker.\n",
            "url": "http://www.cs.virginia.edu/~vicente/sbucaptions",
            "license": "unknown",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "image-to-text"
            ],
            "tasks": [
                "image-captioning"
            ]
        }
    },
    {
        "id": "scan",
        "data": {
            "description": "SCAN tasks with various splits.\n\nSCAN is a set of simple language-driven navigation tasks for studying\ncompositional learning and zero-shot generalization.\n\nSee https://github.com/brendenlake/SCAN for a description of the splits.\n\nExample usage:\ndata = datasets.load_dataset('scan/length')\n",
            "url": "https://github.com/brendenlake/SCAN",
            "license": "",
            "givenLicense": "bsd",
            "language": [
                "en"
            ],
            "categories": [
                "sequence-modeling"
            ],
            "tasks": [
                "other-multi-turn"
            ]
        }
    },
    {
        "id": "scb_mt_enth_2020",
        "data": {
            "description": "scb-mt-en-th-2020: A Large English-Thai Parallel Corpus\nThe primary objective of our work is to build a large-scale English-Thai dataset for machine translation.\nWe construct an English-Thai machine translation dataset with over 1 million segment pairs, curated from various sources,\nnamely news, Wikipedia articles, SMS messages, task-based dialogs, web-crawled data and government documents.\nMethodology for gathering data, building parallel texts and removing noisy sentence pairs are presented in a reproducible manner.\nWe train machine translation models based on this dataset. Our models' performance are comparable to that of\nGoogle Translation API (as of May 2020) for Thai-English and outperform Google when the Open Parallel Corpus (OPUS) is\nincluded in the training data for both Thai-English and English-Thai translation.\nThe dataset, pre-trained models, and source code to reproduce our work are available for public use.\n",
            "url": "https://airesearch.in.th/",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en",
                "th"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "scene_parse_150",
        "data": {
            "description": "Scene parsing is to segment and parse an image into different image regions associated with semantic categories, such as sky, road, person, and bed.\nMIT Scene Parsing Benchmark (SceneParse150) provides a standard training and evaluation platform for the algorithms of scene parsing.\nThe data for this benchmark comes from ADE20K Dataset which contains more than 20K scene-centric images exhaustively annotated with objects and object parts.\nSpecifically, the benchmark is divided into 20K images for training, 2K images for validation, and another batch of held-out images for testing.\nThere are totally 150 semantic categories included for evaluation, which include stuffs like sky, road, grass, and discrete objects like person, car, bed.\nNote that there are non-uniform distribution of objects occuring in the images, mimicking a more natural object occurrence in daily scene.\n",
            "url": "http://sceneparsing.csail.mit.edu/",
            "license": "BSD 3-Clause License",
            "givenLicense": "bsd-3-clause",
            "language": [
                "en"
            ],
            "categories": [
                "image-segmentation"
            ],
            "tasks": [
                "instance-segmentation",
                "other-scene-parsing"
            ]
        }
    },
    {
        "id": "schema_guided_dstc8",
        "data": {
            "description": "The Schema-Guided Dialogue dataset (SGD) was developed for the Dialogue State Tracking task of the Eights Dialogue Systems Technology Challenge (dstc8).\nThe SGD dataset consists of over 18k annotated multi-domain, task-oriented conversations between a human and a virtual assistant.\nThese conversations involve interactions with services and APIs spanning 17 domains, ranging from banks and events to media, calendar, travel, and weather.\nFor most of these domains, the SGD dataset contains multiple different APIs, many of which have overlapping functionalities but different interfaces,\nwhich reflects common real-world scenarios.\n",
            "url": "https://github.com/google-research-datasets/dstc8-schema-guided-dialogue",
            "license": "CC BY-SA 4.0",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "token-classification",
                "text-classification",
                "natural-language-processing"
            ],
            "tasks": [
                "dialogue-modeling",
                "multi-class-classification",
                "parsing"
            ]
        }
    },
    {
        "id": "scicite",
        "data": {
            "description": "\nThis is a dataset for classifying citation intents in academic papers.\nThe main citation intent label for each Json object is specified with the label\nkey while the citation context is specified in with a context key. Example:\n{\n 'string': 'In chacma baboons, male-infant relationships can be linked to both\n    formation of friendships and paternity success [30,31].'\n 'sectionName': 'Introduction',\n 'label': 'background',\n 'citingPaperId': '7a6b2d4b405439',\n 'citedPaperId': '9d1abadc55b5e0',\n ...\n }\nYou may obtain the full information about the paper using the provided paper ids\nwith the Semantic Scholar API (https://api.semanticscholar.org/).\nThe labels are:\nMethod, Background, Result\n",
            "url": "https://github.com/allenai/scicite",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "scielo",
        "data": {
            "description": "A parallel corpus of full-text scientific articles collected from Scielo database in the following languages: English, Portuguese and Spanish. The corpus is sentence aligned for all language pairs, as well as trilingual aligned for a small subset of sentences. Alignment was carried out using the Hunalign algorithm.\n",
            "url": "http://www.euromatrixplus.net/multi-un/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "es",
                "pt"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "scientific_papers",
        "data": {
            "description": "\nScientific papers datasets contains two sets of long and structured documents.\nThe datasets are obtained from ArXiv and PubMed OpenAccess repositories.\n\nBoth \"arxiv\" and \"pubmed\" have two features:\n  - article: the body of the document, pagragraphs seperated by \"/n\".\n  - abstract: the abstract of the document, pagragraphs seperated by \"/n\".\n  - section_names: titles of sections, seperated by \"/n\".\n\n",
            "url": "https://github.com/armancohan/long-summarization",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "scifact",
        "data": {
            "description": "SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts, and annotated with labels and rationales.\n",
            "url": "https://scifact.apps.allenai.org/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "sciq",
        "data": {
            "description": "The SciQ dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. For the majority of the questions, an additional paragraph with supporting evidence for the correct answer is provided.\n\n",
            "url": "https://allenai.org/data/sciq",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "scitail",
        "data": {
            "description": "The SciTail dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question \nand the correct answer choice are converted into an assertive statement to form the hypothesis. We use information \nretrieval to obtain relevant text from a large text corpus of web sentences, and use these sentences as a premise P. We \ncrowdsource the annotation of such premise-hypothesis pair as supports (entails) or not (neutral), in order to create \nthe SciTail dataset. The dataset contains 27,026 examples with 10,101 examples with entails label and 16,925 examples \nwith neutral label\n",
            "url": "https://allenai.org/data/scitail",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "scitldr",
        "data": {
            "description": "A new multi-target dataset of 5.4K TLDRs over 3.2K papers.\nSCITLDR contains both author-written and expert-derived TLDRs,\nwhere the latter are collected using a novel annotation protocol\nthat produces high-quality summaries while minimizing annotation burden.\n",
            "url": "https://github.com/allenai/scitldr",
            "license": "Apache License 2.0",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "summarization-other-scientific-documents-summarization"
            ]
        }
    },
    {
        "id": "search_qa",
        "data": {
            "description": "\n# pylint: disable=line-too-long\nWe publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind \nCNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article \nand generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. \nFollowing this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context\n tuple of the SearchQA comes with additional meta-data such as the snippet's URL, which we believe will be valuable resources for future research. We conduct human evaluation \n as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human \n and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.\n\n",
            "url": "https://github.com/nyu-dl/dl4ir-searchQA",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "sede",
        "data": {
            "description": "SEDE (Stack Exchange Data Explorer) is new dataset for Text-to-SQL tasks with more than 12,000 SQL queries and their\nnatural language description. It's based on a real usage of users from the Stack Exchange Data Explorer platform,\nwhich brings complexities and challenges never seen before in any other semantic parsing dataset like\nincluding complex nesting, dates manipulation, numeric and text manipulation, parameters, and most\nimportantly: under-specification and hidden-assumptions.\n\nPaper (NLP4Prog workshop at ACL2021): https://arxiv.org/abs/2106.05006\n",
            "url": "https://github.com/hirupert/sede",
            "license": "Apache-2.0 License",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification",
                "natural-language-processing"
            ],
            "tasks": [
                "parsing"
            ]
        }
    },
    {
        "id": "selqa",
        "data": {
            "description": "The SelQA dataset provides crowdsourced annotation for two selection-based question answer tasks, \nanswer sentence selection and answer triggering.\n",
            "url": "",
            "license": "",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "sem_eval_2010_task_8",
        "data": {
            "description": "The SemEval-2010 Task 8 focuses on Multi-way classification of semantic relations between pairs of nominals.\nThe task was designed to compare different approaches to semantic relation classification\nand to provide a standard testbed for future research.\n",
            "url": "https://semeval2.fbk.eu/semeval2.php?location=tasks&taskid=11",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "sem_eval_2014_task_1",
        "data": {
            "description": "The SemEval-2014 Task 1 focuses on Evaluation of Compositional Distributional Semantic Models\non Full Sentences through Semantic Relatedness and Entailment. The task was designed to\npredict the degree of relatedness between two sentences and to detect the entailment\nrelation holding between them.\n",
            "url": "https://alt.qcri.org/semeval2014/task1/",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "natural-language-inference",
                "semantic-similarity-scoring"
            ]
        }
    },
    {
        "id": "sem_eval_2018_task_1",
        "data": {
            "description": " SemEval-2018 Task 1: Affect in Tweets: SubTask 5: Emotion Classification.\n This is a dataset for multilabel emotion classification for tweets.\n 'Given a tweet, classify it as 'neutral or no emotion' or as one, or more, of eleven given emotions that best represent the mental state of the tweeter.'\n It contains 22467 tweets in three languages manually annotated by crowdworkers using Best–Worst Scaling.\n",
            "url": "https://competitions.codalab.org/competitions/17751",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "ar",
                "es"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification",
                "text-classification-other-emotion-classification"
            ]
        }
    },
    {
        "id": "sem_eval_2020_task_11",
        "data": {
            "description": "Propagandistic news articles use specific techniques to convey their message,\nsuch as whataboutism, red Herring, and name calling, among many others.\nThe Propaganda Techniques Corpus (PTC) allows to study automatic algorithms to\ndetect them. We provide a permanent leaderboard to allow researchers both to\nadvertise their progress and to be up-to-speed with the state of the art on the\ntasks offered (see below for a definition).\n",
            "url": "https://propaganda.qcri.org/ptc/index.html",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification",
                "token-classification"
            ],
            "tasks": [
                "text-classification-other-propaganda-technique-classification",
                "token-classification-other-propaganda-span-identification"
            ]
        }
    },
    {
        "id": "sent_comp",
        "data": {
            "description": "Large corpus of uncompressed and compressed sentences from news articles.\n",
            "url": "https://github.com/google-research-datasets/sentence-compression",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "other"
            ],
            "tasks": [
                "other-other-sentence-compression"
            ]
        }
    },
    {
        "id": "senti_lex",
        "data": {
            "description": "This dataset add sentiment lexicons for 81 languages generated via graph propagation based on a knowledge graph--a graphical representation of real-world entities and the links between them.\n",
            "url": "https://sites.google.com/site/datascienceslab/projects/multilingualsentiment",
            "license": "GNU General Public License v3",
            "givenLicense": "gpl-3.0",
            "language": [
                "af",
                "an",
                "ar",
                "az",
                "be",
                "bg",
                "bn",
                "br",
                "bs",
                "ca",
                "cs",
                "cy",
                "da",
                "de",
                "el",
                "eo",
                "es",
                "et",
                "eu",
                "fa",
                "fi",
                "fo",
                "fr",
                "fy",
                "ga",
                "gd",
                "gl",
                "gu",
                "he",
                "hi",
                "hr",
                "ht",
                "hu",
                "hy",
                "ia",
                "id",
                "io",
                "is",
                "it",
                "ja",
                "ka",
                "km",
                "kn",
                "ko",
                "ku",
                "ky",
                "la",
                "lb",
                "lt",
                "lv",
                "mk",
                "mr",
                "ms",
                "mt",
                "nl",
                "nn",
                "no",
                "pl",
                "pt",
                "rm",
                "ro",
                "ru",
                "sk",
                "sl",
                "sq",
                "sr",
                "sv",
                "sw",
                "ta",
                "te",
                "th",
                "tk",
                "tl",
                "tr",
                "uk",
                "ur",
                "uz",
                "vi",
                "vo",
                "wa",
                "yi",
                "zh",
                "zhw"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "sentiment140",
        "data": {
            "description": "Sentiment140 consists of Twitter messages with emoticons, which are used as noisy labels for\nsentiment classification. For more detailed information please refer to the paper.\n",
            "url": "http://help.sentiment140.com/home",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "senti_ws",
        "data": {
            "description": "SentimentWortschatz, or SentiWS for short, is a publicly available German-language resource for sentiment analysis, and pos-tagging. The POS tags are [\"NN\", \"VVINF\", \"ADJX\", \"ADV\"] -> [\"noun\", \"verb\", \"adjective\", \"adverb\"], and positive and negative polarity bearing words are weighted within the interval of [-1, 1].\n",
            "url": "",
            "license": "Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "de"
            ],
            "categories": [
                "token-classification",
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "sentiment-scoring",
                "part-of-speech-tagging"
            ]
        }
    },
    {
        "id": "sepedi_ner",
        "data": {
            "description": "Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.\n",
            "url": "https://repo.sadilar.org/handle/20.500.12185/328",
            "license": "Creative Commons Attribution 2.5 South Africa License",
            "givenLicense": "other",
            "language": [
                "nso"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "sesotho_ner_corpus",
        "data": {
            "description": "Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.\n",
            "url": "https://repo.sadilar.org/handle/20.500.12185/334",
            "license": "Creative Commons Attribution 2.5 South Africa License",
            "givenLicense": "other",
            "language": [
                "st"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "setimes",
        "data": {
            "description": "SETimes – A Parallel Corpus of English and South-East European Languages\nThe corpus is based on the content published on the SETimes.com news portal. The news portal publishes “news and views from Southeast Europe” in ten languages: Bulgarian, Bosnian, Greek, English, Croatian, Macedonian, Romanian, Albanian and Serbian. This version of the corpus tries to solve the issues present in an older version of the corpus (published inside OPUS, described in the LREC 2010 paper by Francis M. Tyers and Murat Serdar Alperen). The following procedures were applied to resolve existing issues:\n\n- stricter extraction process – no HTML residues present\n- language identification on every non-English document – non-English online documents contain English material in case the article was not translated into that language\n- resolving encoding issues in Croatian and Serbian – diacritics were partially lost due to encoding errors – text was rediacritized.\n",
            "url": "http://nlp.ffzg.hr/resources/corpora/setimes/",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "bg",
                "bs",
                "el",
                "en",
                "hr",
                "mk",
                "ro",
                "sq",
                "sr",
                "tr"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "setswana_ner_corpus",
        "data": {
            "description": "Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.\n",
            "url": "https://repo.sadilar.org/handle/20.500.12185/341",
            "license": "",
            "givenLicense": "other",
            "language": [
                "tn"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "sharc",
        "data": {
            "description": "ShARC is a Conversational Question Answering dataset focussing on question answering from texts containing rules. The goal is to answer questions by possibly asking follow-up questions first. It is assumed assume that the question is often underspecified, in the sense that the question does not provide enough information to be answered directly. However, an agent can use the supporting rule text to infer what needs to be asked in order to determine the final answer.\n",
            "url": "https://sharc-data.github.io/index.html",
            "license": "",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "question-answering-other-conversational-qa"
            ]
        }
    },
    {
        "id": "sharc_modified",
        "data": {
            "description": "ShARC, a conversational QA task, requires a system to answer user questions based on rules expressed in natural language text. However, it is found that in the ShARC dataset there are multiple spurious patterns that could be exploited by neural models. SharcModified is a new dataset which reduces the patterns identified in the original dataset. To reduce the sensitivity of neural models, for each occurence of an instance conforming to any of the patterns, we automatically construct alternatives where we choose to either replace the current instance with an alternative instance which does not exhibit the pattern; or retain the original instance. The modified ShARC has two versions sharc-mod and history-shuffled. For morre details refer to Appendix A.3 .\n",
            "url": "https://github.com/nikhilweee/neural-conv-qa",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "question-answering-other-conversational-qa"
            ]
        }
    },
    {
        "id": "sick",
        "data": {
            "description": "Shared and internationally recognized benchmarks are fundamental for the development of any computational system.\nWe aim to help the research community working on compositional distributional semantic models (CDSMs) by providing SICK (Sentences Involving Compositional Knowldedge), a large size English benchmark tailored for them.\nSICK consists of about 10,000 English sentence pairs that include many examples of the lexical, syntactic and semantic phenomena that CDSMs are expected to account for, but do not require dealing with other aspects of existing sentential data sets (idiomatic multiword expressions, named entities, telegraphic language) that are not within the scope of CDSMs.\nBy means of crowdsourcing techniques, each pair was annotated for two crucial semantic tasks: relatedness in meaning (with a 5-point rating scale as gold score) and entailment relation between the two elements (with three possible gold labels: entailment, contradiction, and neutral).\nThe SICK data set was used in SemEval-2014 Task 1, and it freely available for research purposes.\n",
            "url": "http://marcobaroni.org/composes/sick.html",
            "license": "",
            "givenLicense": "cc-by-nc-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference"
            ]
        }
    },
    {
        "id": "silicone",
        "data": {
            "description": "The Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE (SILICONE) benchmark is a collection\n of resources for training, evaluating, and analyzing natural language understanding systems\n specifically designed for spoken language. All datasets are in the English language and cover a\n variety of domains including daily life, scripted scenarios, joint task completion, phone call\n conversations, and televsion dialogue. Some datasets additionally include emotion and/or sentimant\n labels.\n",
            "url": "http://yanran.li/dailydialog.html",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "text-classification"
            ],
            "tasks": [
                "dialogue-modeling",
                "language-modeling",
                "masked-language-modeling",
                "sentiment-classification",
                "text-classification-other-dialogue-act-classification",
                "text-classification-other-emotion-classification",
                "text-scoring"
            ]
        }
    },
    {
        "id": "simple_questions_v2",
        "data": {
            "description": "SimpleQuestions is a dataset for simple QA, which consists\nof a total of 108,442 questions written in natural language by human\nEnglish-speaking annotators each paired with a corresponding fact,\nformatted as (subject, relationship, object), that provides the answer\nbut also a complete explanation.  Fast have been extracted from the\nKnowledge Base Freebase (freebase.com).  We randomly shuffle these\nquestions and use 70% of them (75910) as training set, 10% as\nvalidation set (10845), and the remaining 20% as test set.\n",
            "url": "https://research.fb.com/downloads/babi/",
            "license": "",
            "givenLicense": "cc-by-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "siswati_ner_corpus",
        "data": {
            "description": "Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.\n",
            "url": "https://repo.sadilar.org/handle/20.500.12185/346",
            "license": "Creative Commons Attribution 2.5 South Africa License",
            "givenLicense": "other",
            "language": [
                "ss"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "smartdata",
        "data": {
            "description": "DFKI SmartData Corpus is a dataset of 2598 German-language documents\nwhich has been annotated with fine-grained geo-entities, such as streets,\nstops and routes, as well as standard named entity types. It has also\nbeen annotated with a set of 15 traffic- and industry-related n-ary\nrelations and events, such as Accidents, Traffic jams, Acquisitions,\nand Strikes. The corpus consists of newswire texts, Twitter messages,\nand traffic reports from radio stations, police and railway companies.\nIt allows for training and evaluating both named entity recognition\nalgorithms that aim for fine-grained typing of geo-entities, as well\nas n-ary relation extraction systems.",
            "url": "https://www.dfki.de/web/forschung/projekte-publikationen/publikationen-uebersicht/publikation/9427/",
            "license": "CC-BY 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "de"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "sms_spam",
        "data": {
            "description": "The SMS Spam Collection v.1 is a public set of SMS labeled messages that have been collected for mobile phone spam research.\nIt has one collection composed by 5,574 English, real and non-enconded messages, tagged according being legitimate (ham) or spam.\n",
            "url": "http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification"
            ]
        }
    },
    {
        "id": "snips_built_in_intents",
        "data": {
            "description": "Snips' built in intents dataset was initially used to compare different voice assistants and released as a public dataset hosted at\nhttps://github.com/sonos/nlu-benchmark 2016-12-built-in-intents. The dataset contains 328 utterances over 10 intent classes. The\nrelated paper mentioned on the github page is https://arxiv.org/abs/1805.10190 and a related Medium post is\nhttps://medium.com/snips-ai/benchmarking-natural-language-understanding-systems-d35be6ce568d .\n",
            "url": "https://github.com/sonos/nlu-benchmark/tree/master/2016-12-built-in-intents",
            "license": "",
            "givenLicense": "cc0-1.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification"
            ]
        }
    },
    {
        "id": "snli",
        "data": {
            "description": "The SNLI corpus (version 1.0) is a collection of 570k human-written English\nsentence pairs manually labeled for balanced classification with the labels\nentailment, contradiction, and neutral, supporting the task of natural language\ninference (NLI), also known as recognizing textual entailment (RTE).\n",
            "url": "https://nlp.stanford.edu/projects/snli/",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference",
                "multi-input-text-classification"
            ]
        }
    },
    {
        "id": "snow_simplified_japanese_corpus",
        "data": {
            "description": "About SNOW T15: The simplified corpus for the Japanese language. The corpus has 50,000 manually simplified and aligned sentences. This corpus contains the original sentences, simplified sentences and English translation of the original sentences. It can be used for automatic text simplification as well as translating simple Japanese into English and vice-versa. The core vocabulary is restricted to 2,000 words where it is selected by accounting for several factors such as meaning preservation, variation, simplicity and the UniDic word segmentation criterion.\nFor details, refer to the explanation page of Japanese simplification (http://www.jnlp.org/research/Japanese_simplification). The original texts are from \"small_parallel_enja: 50k En/Ja Parallel Corpus for Testing SMT Methods\", which is a bilingual corpus for machine translation. About SNOW T23: An expansion corpus of 35,000 sentences rewritten in easy Japanese (simple Japanese vocabulary) based on SNOW T15. The original texts are from \"Tanaka Corpus\" (http://www.edrdg.org/wiki/index.php/Tanaka_Corpus).\n",
            "url": "http://www.jnlp.org/SNOW/T15, http://www.jnlp.org/SNOW/T23",
            "license": "CC BY 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en",
                "ja"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "social_bias_frames",
        "data": {
            "description": "Social Bias Frames is a new way of representing the biases and offensiveness that are implied in language.\nFor example, these frames are meant to distill the implication that \"women (candidates) are less qualified\"\nbehind the statement \"we shouldn’t lower our standards to hire more women.\"\n",
            "url": "https://homes.cs.washington.edu/~msap/social-bias-frames/",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation",
                "text-classification"
            ],
            "tasks": [
                "text2text-generation-other-explanation-generation",
                "hate-speech-detection"
            ]
        }
    },
    {
        "id": "social_i_qa",
        "data": {
            "description": "We introduce Social IQa: Social Interaction QA, a new question-answering benchmark for testing social commonsense intelligence. Contrary to many prior benchmarks that focus on physical or taxonomic knowledge, Social IQa focuses on reasoning about people’s actions and their social implications. For example, given an action like \"Jesse saw a concert\" and a question like \"Why did Jesse do this?\", humans can easily infer that Jesse wanted \"to see their favorite performer\" or \"to enjoy the music\", and not \"to see what's happening inside\" or \"to see if it works\". The actions in Social IQa span a wide variety of social situations, and answer candidates contain both human-curated answers and adversarially-filtered machine-generated candidates. Social IQa contains over 37,000 QA pairs for evaluating models’ abilities to reason about the social implications of everyday events and situations. (Less)\n",
            "url": "https://leaderboard.allenai.org/socialiqa/submissions/get-started",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "sofc_materials_articles",
        "data": {
            "description": "The SOFC-Exp corpus consists of 45 open-access scholarly articles annotated by domain experts.\nA corpus and an inter-annotator agreement study demonstrate the complexity of the suggested\nnamed entity recognition and slot filling tasks as well as high annotation quality is presented\nin the accompanying paper.\n",
            "url": "https://arxiv.org/abs/2006.03039",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "token-classification",
                "text-classification"
            ],
            "tasks": [
                "named-entity-recognition",
                "slot-filling",
                "topic-classification"
            ]
        }
    },
    {
        "id": "sogou_news",
        "data": {
            "description": "The Sogou News dataset is a mixture of 2,909,551 news articles from the SogouCA and SogouCS news corpora, in 5 categories. \nThe number of training samples selected for each class is 90,000 and testing 12,000. Note that the Chinese characters have been converted to Pinyin.\nclassification labels of the news are determined by their domain names in the URL. For example, the news with\nURL http://sports.sohu.com is categorized as a sport class.\n",
            "url": "",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "so_stacksample",
        "data": {
            "description": "Dataset with the text of 10% of questions and answers from the Stack Overflow programming Q&A website.\n\nThis is organized as three tables:\n\nQuestions contains the title, body, creation date, closed date (if applicable), score, and owner ID for all non-deleted Stack Overflow questions whose Id is a multiple of 10.\nAnswers contains the body, creation date, score, and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table.\nTags contains the tags on each of these questions\n",
            "url": "https://www.kaggle.com/stackoverflow/stacksample",
            "license": "All Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required.",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "abstractive-qa",
                "open-domain-abstractive-qa"
            ]
        }
    },
    {
        "id": "spanish_billion_words",
        "data": {
            "description": "An unannotated Spanish corpus of nearly 1.5 billion words, compiled from different resources from the web.\nThis resources include the spanish portions of SenSem, the Ancora Corpus, some OPUS Project Corpora and the Europarl,\nthe Tibidabo Treebank, the IULA Spanish LSP Treebank, and dumps from the Spanish Wikipedia, Wikisource and Wikibooks.\nThis corpus is a compilation of 100 text files. Each line of these files represents one of the 50 million sentences from the corpus.\n",
            "url": "https://crscardellino.github.io/SBWCE/",
            "license": "https://creativecommons.org/licenses/by-sa/4.0/",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "es"
            ],
            "categories": [
                "other",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling",
                "other-other-pretraining-language-models"
            ]
        }
    },
    {
        "id": "spc",
        "data": {
            "description": "This is a collection of parallel corpora collected by Hercules Dalianis and his research group for bilingual dictionary construction.\nMore information in: Hercules Dalianis, Hao-chun Xing, Xin Zhang: Creating a Reusable English-Chinese Parallel Corpus for Bilingual Dictionary Construction, In Proceedings of LREC2010 (source: http://people.dsv.su.se/~hercules/SEC/) and Konstantinos Charitakis (2007): Using Parallel Corpora to Create a Greek-English Dictionary with UPLUG, In Proceedings of NODALIDA 2007. Afrikaans-English: Aldin Draghoender and Mattias Kanhov: Creating a reusable English – Afrikaans parallel corpora for bilingual dictionary construction\n\n4 languages, 3 bitexts\ntotal number of files: 6\ntotal number of tokens: 1.32M\ntotal number of sentence fragments: 0.15M\n",
            "url": "http://opus.nlpl.eu/SPC.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "af",
                "el",
                "en",
                "zh"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "species_800",
        "data": {
            "description": "We have developed an efficient algorithm and implementation of a dictionary-based approach to named entity recognition,\nwhich we here use to identifynames of species and other taxa in text. The tool, SPECIES, is more than an order of\nmagnitude faster and as accurate as existing tools. The precision and recall was assessed both on an existing gold-standard\ncorpus and on a new corpus of 800 abstracts, which were manually annotated after the development of the tool. The corpus\ncomprises abstracts from journals selected to represent many taxonomic groups, which gives insights into which types of\norganism names are hard to detect and which are easy. Finally, we have tagged organism names in the entire Medline database\nand developed a web resource, ORGANISMS, that makes the results accessible to the broad community of biologists.\n",
            "url": "https://species.jensenlab.org/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "speech_commands",
        "data": {
            "description": "\nThis is a set of one-second .wav audio files, each containing a single spoken\nEnglish word or background noise. These words are from a small set of commands, and are spoken by a\nvariety of different speakers. This data set is designed to help train simple\nmachine learning models. This dataset is covered in more detail at\n[https://arxiv.org/abs/1804.03209](https://arxiv.org/abs/1804.03209).\n\nVersion 0.01 of the data set (configuration `\"v0.01\"`) was released on August 3rd 2017 and contains\n64,727 audio files.\n\nIn version 0.01 thirty different words were recoded: \"Yes\", \"No\", \"Up\", \"Down\", \"Left\",\n\"Right\", \"On\", \"Off\", \"Stop\", \"Go\", \"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\",\n\"Bed\", \"Bird\", \"Cat\", \"Dog\", \"Happy\", \"House\", \"Marvin\", \"Sheila\", \"Tree\", \"Wow\".\n\n\nIn version 0.02 more words were added: \"Backward\", \"Forward\", \"Follow\", \"Learn\", \"Visual\".\n\nIn both versions, ten of them are used as commands by convention: \"Yes\", \"No\", \"Up\", \"Down\", \"Left\",\n\"Right\", \"On\", \"Off\", \"Stop\", \"Go\". Other words are considered to be auxiliary (in current implementation\nit is marked by `True` value of `\"is_unknown\"` feature). Their function is to teach a model to distinguish core words\nfrom unrecognized ones.\n\nThe `_silence_` class contains a set of longer audio clips that are either recordings or\na mathematical simulation of noise.\n\n",
            "url": "https://www.tensorflow.org/datasets/catalog/speech_commands",
            "license": "Creative Commons BY 4.0 License",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "audio-classification"
            ],
            "tasks": [
                "keyword-spotting"
            ]
        }
    },
    {
        "id": "spider",
        "data": {
            "description": "Spider is a large-scale complex and cross-domain semantic parsing and text-toSQL dataset annotated by 11 college students\n",
            "url": "https://yale-lily.github.io/spider",
            "license": "CC BY-SA 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-text-to-sql"
            ]
        }
    },
    {
        "id": "squad",
        "data": {
            "description": "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
            "url": "https://rajpurkar.github.io/SQuAD-explorer/",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "squad_adversarial",
        "data": {
            "description": "Here are two different adversaries, each of which uses a different procedure to pick the sentence it adds to the paragraph:\nAddSent: Generates up to five candidate adversarial sentences that don't answer the question, but have a lot of words in common with the question. Picks the one that most confuses the model.\nAddOneSent: Similar to AddSent, but just picks one of the candidate sentences at random. This adversary is does not query the model in any way.\n",
            "url": "https://worksheets.codalab.org/worksheets/0xc86d3ebe69a3427d91f9aaa63f7d1e7d/",
            "license": "MIT License",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "squad_es",
        "data": {
            "description": "automatic translation of the Stanford Question Answering Dataset (SQuAD) v2 into Spanish\n",
            "url": "https://github.com/ccasimiro88/TranslateAlignRetrieve",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "squad_it",
        "data": {
            "description": "SQuAD-it is derived from the SQuAD dataset and it is obtained through semi-automatic translation of the SQuAD dataset\ninto Italian. It represents a large-scale dataset for open question answering processes on factoid questions in Italian.\n The dataset contains more than 60,000 question/answer pairs derived from the original English dataset. The dataset is\n split into training and test sets to support the replicability of the benchmarking of QA systems:\n",
            "url": "https://github.com/crux82/squad-it",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "it-IT"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa",
                "extractive-qa"
            ]
        }
    },
    {
        "id": "squad_kor_v1",
        "data": {
            "description": "KorQuAD 1.0 is a large-scale Korean dataset for machine reading comprehension task consisting of human generated questions for Wikipedia articles. We benchmark the data collecting process of SQuADv1.0 and crowdsourced 70,000+ question-answer pairs. 1,637 articles and 70,079 pairs of question answers were collected. 1,420 articles are used for the training set, 140 for the dev set, and 77 for the test set. 60,407 question-answer pairs are for the training set, 5,774 for the dev set, and 3,898 for the test set.\n",
            "url": "https://korquad.github.io/KorQuad%201.0/",
            "license": "CC BY-ND 2.0 KR",
            "givenLicense": "cc-by-nd-4.0",
            "language": [
                "ko"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "squad_kor_v2",
        "data": {
            "description": "KorQuAD 2.0 is a Korean question and answering dataset consisting of a total of 100,000+ pairs. There are three major differences from KorQuAD 1.0, which is the standard Korean Q & A data. The first is that a given document is a whole Wikipedia page, not just one or two paragraphs. Second, because the document also contains tables and lists, it is necessary to understand the document structured with HTML tags. Finally, the answer can be a long text covering not only word or phrase units, but paragraphs, tables, and lists. As a baseline model, BERT Multilingual is used, released by Google as an open source. It shows 46.0% F1 score, a very low score compared to 85.7% of the human F1 score. It indicates that this data is a challenging task. Additionally, we increased the performance by no-answer data augmentation. Through the distribution of this data, we intend to extend the limit of MRC that was limited to plain text to real world tasks of various lengths and formats.\n",
            "url": "https://korquad.github.io/",
            "license": "CC BY-ND 2.0 KR",
            "givenLicense": "cc-by-nd-4.0",
            "language": [
                "ko"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "squadshifts",
        "data": {
            "description": "SquadShifts consists of four new test sets for the Stanford Question Answering Dataset (SQuAD) from four different domains: Wikipedia articles, New York \\ \nTimes articles, Reddit comments, and Amazon product reviews. Each dataset was generated using the same data generating pipeline, Amazon Mechanical Turk interface, and data cleaning code as the original SQuAD v1.1 dataset. The \"new-wikipedia\" dataset measures overfitting on the original SQuAD v1.1 dataset.  The \"new-york-times\", \"reddit\", and \"amazon\" datasets measure robustness to natural distribution shifts. We encourage SQuAD model developers to also evaluate their methods on these new datasets! ",
            "url": "https://modestyachts.github.io/squadshifts-website/index.html",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "squad_v1_pt",
        "data": {
            "description": "Portuguese translation of the SQuAD dataset. The translation was performed automatically using the Google Cloud API.\n",
            "url": "https://github.com/nunorc/squad-v1.1-pt",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "pt"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "squad_v2",
        "data": {
            "description": "combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers\n to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but\n also determine when no answer is supported by the paragraph and abstain from answering.\n",
            "url": "https://rajpurkar.github.io/SQuAD-explorer/",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa",
                "extractive-qa"
            ]
        }
    },
    {
        "id": "srwac",
        "data": {
            "description": "The Serbian web corpus srWaC was built by crawling the .rs top-level domain in 2014. The corpus was near-deduplicated on paragraph level, normalised via diacritic restoration, morphosyntactically annotated and lemmatised. The corpus is shuffled by paragraphs. Each paragraph contains metadata on the URL, domain and language identification (Serbian vs. Croatian).\nVersion 1.0 of this corpus is described in http://www.aclweb.org/anthology/W14-0405. Version 1.1 contains newer and better linguistic annotations.\n",
            "url": "http://nlp.ffzg.hr/resources/corpora/srwac/",
            "license": "CC BY-SA 4.0",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "sr"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "sst",
        "data": {
            "description": "The Stanford Sentiment Treebank, the first corpus with fully labeled parse trees that allows for a\ncomplete analysis of the compositional effects of sentiment in language.\n",
            "url": "https://nlp.stanford.edu/sentiment/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "sentiment-classification",
                "sentiment-scoring"
            ]
        }
    },
    {
        "id": "sst2",
        "data": {
            "description": "The Stanford Sentiment Treebank consists of sentences from movie reviews and\nhuman annotations of their sentiment. The task is to predict the sentiment of a\ngiven sentence. We use the two-way (positive/negative) class split, and use only\nsentence-level labels.\n",
            "url": "https://nlp.stanford.edu/sentiment/",
            "license": "Unknown",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "stereoset",
        "data": {
            "description": "Stereoset is a dataset that measures stereotype bias in language models. Stereoset consists of 17,000 sentences that\nmeasures model preferences across gender, race, religion, and profession.\n",
            "url": "https://Stereoset.mit.edu/",
            "license": "CC BY-SA 4.0",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-stereotype-detection"
            ]
        }
    },
    {
        "id": "story_cloze",
        "data": {
            "description": "\nStory Cloze Test' is a commonsense reasoning framework for evaluating story understanding,\nstory generation, and script learning.This test requires a system to choose the correct ending\nto a four-sentence story.\n",
            "url": "https://cs.rochester.edu/nlp/rocstories/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "other"
            ],
            "tasks": [
                "other-other-story-completion"
            ]
        }
    },
    {
        "id": "stsb_mt_sv",
        "data": {
            "description": "Machine translated Swedish version of the original STS-B (http://ixa2.si.ehu.eus/stswiki)",
            "url": "https://github.com/timpal0l/sts-benchmark-swedish",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "sv"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "semantic-similarity-scoring"
            ]
        }
    },
    {
        "id": "stsb_multi_mt",
        "data": {
            "description": "This is a multilingual translation of the STSbenchmark dataset. Translation has been done with deepl.com.\n",
            "url": "https://github.com/PhilipMay/stsb-multi-mt",
            "license": "custom license - see project page",
            "givenLicense": "other",
            "language": [
                "en",
                "de",
                "es",
                "fr",
                "it",
                "nl",
                "pl",
                "pt",
                "ru",
                "zh"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "semantic-similarity-scoring"
            ]
        }
    },
    {
        "id": "style_change_detection",
        "data": {
            "description": "The goal of the style change detection task is to identify text positions within a given multi-author document at which the author switches. Detecting these positions is a crucial part of the authorship identification process, and for multi-author document analysis in general.\n\nAccess to the dataset needs to be requested from zenodo.\n",
            "url": "https://pan.webis.de/clef20/pan20-web/style-change-detection.html",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "subjqa",
        "data": {
            "description": "SubjQA is a question answering dataset that focuses on subjective questions and answers.\nThe dataset consists of roughly 10,000 questions over reviews from 6 different domains: books, movies, grocery,\nelectronics, TripAdvisor (i.e. hotels), and restaurants.",
            "url": "",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "superb",
        "data": {
            "description": "Self-supervised learning (SSL) has proven vital for advancing research in\nnatural language processing (NLP) and computer vision (CV). The paradigm\npretrains a shared model on large volumes of unlabeled data and achieves\nstate-of-the-art (SOTA) for various tasks with minimal adaptation. However, the\nspeech processing community lacks a similar setup to systematically explore the\nparadigm. To bridge this gap, we introduce Speech processing Universal\nPERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the\nperformance of a shared model across a wide range of speech processing tasks\nwith minimal architecture changes and labeled data. Among multiple usages of the\nshared model, we especially focus on extracting the representation learned from\nSSL due to its preferable re-usability. We present a simple framework to solve\nSUPERB tasks by learning task-specialized lightweight prediction heads on top of\nthe frozen shared model. Our results demonstrate that the framework is promising\nas SSL representations show competitive generalizability and accessibility\nacross SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a\nbenchmark toolkit to fuel the research in representation learning and general\nspeech processing.\n\nNote that in order to limit the required storage for preparing this dataset, the\naudio is stored in the .wav format and is not converted to a float32 array. To\nconvert the audio file to a float32 array, please make use of the `.map()`\nfunction as follows:\n\n\n```python\nimport soundfile as sf\n\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\n\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n",
            "url": "http://www.openslr.org/12",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "automatic-speech-recognition",
                "audio-classification"
            ],
            "tasks": [
                "keyword-spotting",
                "audio-classification-other-query-by-example-spoken-term-detection",
                "speaker-identification",
                "audio-classification-other-automatic-speaker-verification",
                "audio-classification-other-speaker-diarization",
                "audio-intent-classification",
                "other-audio-slot-filling",
                "audio-emotion-recognition"
            ]
        }
    },
    {
        "id": "super_glue",
        "data": {
            "description": "SuperGLUE (https://super.gluebenchmark.com/) is a new benchmark styled after\nGLUE with a new set of more difficult language understanding tasks, improved\nresources, and a new public leaderboard.\n\nBoolQ (Boolean Questions, Clark et al., 2019a) is a QA task where each example consists of a short\npassage and a yes/no question about the passage. The questions are provided anonymously and\nunsolicited by users of the Google search engine, and afterwards paired with a paragraph from a\nWikipedia article containing the answer. Following the original work, we evaluate with accuracy.",
            "url": "https://github.com/google-research-datasets/boolean-questions",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "svhn",
        "data": {
            "description": "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting.\nIt can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images)\nand comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.\n",
            "url": "http://ufldl.stanford.edu/housenumbers/",
            "license": "Custom (non-commercial)",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "image-classification",
                "object-detection"
            ],
            "tasks": []
        }
    },
    {
        "id": "swag",
        "data": {
            "description": "Given a partial description like \"she opened the hood of the car,\"\nhumans can reason about the situation and anticipate what might come\nnext (\"then, she examined the engine\"). SWAG (Situations With Adversarial Generations)\nis a large-scale dataset for this task of grounded commonsense\ninference, unifying natural language inference and physically grounded reasoning.\n\nThe dataset consists of 113k multiple choice questions about grounded situations\n(73k training, 20k validation, 20k test).\nEach question is a video caption from LSMDC or ActivityNet Captions,\nwith four answer choices about what might happen next in the scene.\nThe correct answer is the (real) video caption for the next event in the video;\nthe three incorrect answers are adversarially generated and human verified,\nso as to fool machines but not humans. SWAG aims to be a benchmark for\nevaluating grounded commonsense NLI and for learning representations.\n\nThe full data contain more information,\nbut the regular configuration will be more interesting for modeling\n(note that the regular data are shuffled). The test set for leaderboard submission\nis under the regular configuration.\n",
            "url": "https://rowanzellers.com/swag/",
            "license": "Unknown",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "natural-language-inference"
            ]
        }
    },
    {
        "id": "swahili",
        "data": {
            "description": "The Swahili dataset developed specifically for language modeling task.\nThe dataset contains 28,000 unique words with 6.84M, 970k, and 2M words for the train,\nvalid and test partitions respectively which represent the ratio 80:10:10.\nThe entire dataset is lowercased, has no punctuation marks and,\nthe start and end of sentence markers have been incorporated to facilitate easy tokenization during language modeling.\n",
            "url": "https://zenodo.org/record/3553423",
            "license": "Attribution 4.0 International",
            "givenLicense": "cc-by-4.0",
            "language": [
                "sw"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "swahili_news",
        "data": {
            "description": "Swahili is spoken by 100-150 million people across East Africa. In Tanzania, it is one of two national languages (the other is English) and it is the official language of instruction in all schools. News in Swahili is an important part of the media sphere in Tanzania.\n\nNews contributes to education, technology, and the economic growth of a country, and news in local languages plays an important cultural role in many Africa countries. In the modern age, African languages in news and other spheres are at risk of being lost as English becomes the dominant language in online spaces.\n\nThe Swahili news dataset was created to reduce the gap of using the Swahili language to create NLP technologies and help AI practitioners in Tanzania and across Africa continent to practice their NLP skills to solve different problems in organizations or societies related to Swahili language. Swahili News were collected from different websites that provide news in the Swahili language. I was able to find some websites that provide news in Swahili only and others in different languages including Swahili.\n\nThe dataset was created for a specific task of text classification, this means each news content can be categorized into six different topics (Local news, International news , Finance news, Health news, Sports news, and Entertainment news). The dataset comes with a specified train/test split. The train set contains 75% of the dataset and test set contains 25% of the dataset.\n",
            "url": "https://zenodo.org/record/4300294#.X84BQdgzZPb",
            "license": "Creative Commons Attribution 4.0 International",
            "givenLicense": "cc-by-4.0",
            "language": [
                "sw"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification"
            ]
        }
    },
    {
        "id": "swda",
        "data": {
            "description": "The Switchboard Dialog Act Corpus (SwDA) extends the Switchboard-1 Telephone Speech Corpus, Release 2 with\nturn/utterance-level dialog-act tags. The tags summarize syntactic, semantic, and pragmatic information about the\nassociated turn. The SwDA project was undertaken at UC Boulder in the late 1990s.\nThe SwDA is not inherently linked to the Penn Treebank 3 parses of Switchboard, and it is far from straightforward to\nalign the two resources. In addition, the SwDA is not distributed with the Switchboard's tables of metadata about the\nconversations and their participants.\n",
            "url": "http://compprag.christopherpotts.net/swda.html",
            "license": "Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License",
            "givenLicense": "cc-by-nc-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification"
            ]
        }
    },
    {
        "id": "swedish_medical_ner",
        "data": {
            "description": "SwedMedNER is a dataset for training and evaluating Named Entity Recognition systems on medical texts in Swedish.\nIt is derived from medical articles on the Swedish Wikipedia, Läkartidningen, and 1177 Vårdguiden.\n",
            "url": "https://github.com/olofmogren/biomedical-ner-data-swedish",
            "license": "Creative Commons Attribution-ShareAlike 4.0 International Public License (CC BY-SA 4.0)\nSee http://creativecommons.org/licenses/by-sa/4.0/ for the summary of the license.\n",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "sv-SE"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "swedish_ner_corpus",
        "data": {
            "description": "    Webbnyheter 2012 from Spraakbanken, semi-manually annotated and adapted for CoreNLP Swedish NER. Semi-manually defined in this case as: Bootstrapped from Swedish Gazetters then manually correcte/reviewed by two independent native speaking swedish annotators. No annotator agreement calculated.\n",
            "url": "https://github.com/klintan/swedish-ner-corpus",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "sv"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "swedish_reviews",
        "data": {
            "description": "Swedish reviews scarped from various public available websites",
            "url": "https://github.com/timpal0l/swedish-sentiment",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "sv"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "swiss_judgment_prediction",
        "data": {
            "description": "Swiss-Judgment-Prediction is a multilingual, diachronic dataset of 85K Swiss Federal Supreme Court (FSCS) cases annotated with\nthe respective binarized judgment outcome (approval/dismissal), posing a challenging text classification task.  \nWe also provide additional metadata, i.e., the publication year, the legal area and the canton of origin per case,  \nto promote robustness and fairness studies on the critical area of legal NLP. \n",
            "url": "https://github.com/JoelNiklaus/SwissCourtRulingCorpus",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "de",
                "fr",
                "it"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-judgement-prediction"
            ]
        }
    },
    {
        "id": "tab_fact",
        "data": {
            "description": "The problem of verifying whether a textual hypothesis holds the truth based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are restricted to dealing with unstructured textual evidence (e.g., sentences and passages, a pool of passages), while verification using structured forms of evidence, such as tables, graphs, and databases, remains unexplored. TABFACT is large scale dataset with 16k Wikipedia tables as evidence for 118k human annotated statements designed for fact verification with semi-structured evidence. The statements are labeled as either ENTAILED or REFUTED. TABFACT is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning.\n",
            "url": "https://tabfact.github.io/",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "fact-checking"
            ]
        }
    },
    {
        "id": "tamilmixsentiment",
        "data": {
            "description": "The first gold standard Tamil-English code-switched, sentiment-annotated corpus containing 15,744 comment posts from YouTube. Train: 11,335 Validation: 1,260 and Test: 3,149.  This makes the largest general domain sentiment dataset for this relatively low-resource language with code-mixing phenomenon.  The dataset contains all the three types of code-mixed sentences - Inter-Sentential switch, Intra-Sentential switch and Tag switching. Most comments were written in Roman script with either Tamil grammar with English lexicon or English grammar with Tamil lexicon. Some comments were written in Tamil script with English expressions in between.\n",
            "url": "https://dravidian-codemix.github.io/2020/datasets.html",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "ta"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "tanzil",
        "data": {
            "description": "This is a collection of Quran translations compiled by the Tanzil project\nThe translations provided at this page are for non-commercial purposes only. If used otherwise, you need to obtain necessary permission from the translator or the publisher.\n\nIf you are using more than three of the following translations in a website or application, we require you to put a link back to this page to make sure that subsequent users have access to the latest updates.\n\n42 languages, 878 bitexts\ntotal number of files: 105\ntotal number of tokens: 22.33M\ntotal number of sentence fragments: 1.01M\n",
            "url": "http://opus.nlpl.eu/Tanzil.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "am",
                "ar",
                "az",
                "bg",
                "bn",
                "bs",
                "cs",
                "de",
                "dv",
                "en",
                "es",
                "fa",
                "fr",
                "ha",
                "hi",
                "id",
                "it",
                "ja",
                "ko",
                "ku",
                "ml",
                "ms",
                "nl",
                "no",
                "pl",
                "pt",
                "ro",
                "ru",
                "sd",
                "so",
                "sq",
                "sv",
                "sw",
                "ta",
                "tg",
                "th",
                "tr",
                "tt",
                "ug",
                "ur",
                "uz",
                "zh"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "tapaco",
        "data": {
            "description": "A freely available paraphrase corpus for 73 languages extracted from the Tatoeba database. Tatoeba is a\ncrowdsourcing project mainly geared towards language learners. Its aim is to provide example sentences and translations for particular\nlinguistic constructions and words. The paraphrase corpus is created by populating a graph with Tatoeba sentences and equivalence links\nbetween sentences “meaning the same thing”. This graph is then traversed to extract sets of paraphrases. Several language-independent\nfilters and pruning steps are applied to remove uninteresting sentences. A manual evaluation performed on three languages shows\nthat between half and three quarters of inferred paraphrases are correct and that most remaining ones are either correct but trivial, or\nnear-paraphrases that neutralize a morphological distinction. The corpus contains a total of 1.9 million sentences, with 200 – 250 000\nsentences per language. It covers a range of languages for which, to our knowledge, no other paraphrase dataset exists. \n",
            "url": "https://zenodo.org/record/3707949#.X9Dh0cYza3I",
            "license": "Creative Commons Attribution 2.0 Generic",
            "givenLicense": "cc-by-2.0",
            "language": [
                "af",
                "ar",
                "az",
                "be",
                "ber",
                "bg",
                "bn",
                "br",
                "ca",
                "cbk",
                "cmn",
                "cs",
                "da",
                "de",
                "el",
                "en",
                "eo",
                "es",
                "et",
                "eu",
                "fi",
                "fr",
                "gl",
                "gos",
                "he",
                "hi",
                "hr",
                "hu",
                "hy",
                "ia",
                "id",
                "ie",
                "io",
                "is",
                "it",
                "ja",
                "jbo",
                "kab",
                "ko",
                "kw",
                "la",
                "lfn",
                "lt",
                "mk",
                "mr",
                "nb",
                "nds",
                "nl",
                "orv",
                "ota",
                "pes",
                "pl",
                "pt",
                "rn",
                "ro",
                "ru",
                "sl",
                "sr",
                "sv",
                "tk",
                "tl",
                "tlh",
                "toki",
                "tr",
                "tt",
                "ug",
                "uk",
                "ur",
                "vi",
                "vo",
                "war",
                "wuu",
                "yue"
            ],
            "categories": [
                "text2text-generation",
                "translation",
                "text-classification"
            ],
            "tasks": [
                "text2text-generation-other-paraphrase-generation",
                "semantic-similarity-classification"
            ]
        }
    },
    {
        "id": "tashkeela",
        "data": {
            "description": "Arabic vocalized texts.\nit contains 75 million of fully vocalized words mainly97 books from classical and modern Arabic language.\n",
            "url": "https://github.com/zaidalyafeai/Tashkeela",
            "license": "",
            "givenLicense": "gpl-2.0",
            "language": [
                "ar"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling",
                "other-diacritics-prediction"
            ]
        }
    },
    {
        "id": "taskmaster1",
        "data": {
            "description": "Taskmaster-1:Toward a Realistic and Diverse Dialog Dataset) is an evaluation set for commonsense question-answering in the sentence completion style of SWAG. As opposed to other automatically generated NLI datasets, CODAH is adversarially constructed by humans who can view feedback from a pre-trained model and use this information to design challenging commonsense questions. Our experimental results show that CODAH questions present a complementary extension to the SWAG dataset, testing additional modes of common sense.\n",
            "url": "https://github.com/google-research-datasets/Taskmaster",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "taskmaster2",
        "data": {
            "description": "Taskmaster is dataset for goal oriented conversations. The Taskmaster-2 dataset consists of 17,289 dialogs in the seven domains which include restaurants, food ordering, movies, hotels, flights, music and sports. Unlike Taskmaster-1, which includes both written \"self-dialogs\" and spoken two-person dialogs, Taskmaster-2 consists entirely of spoken two-person dialogs. In addition, while Taskmaster-1 is almost exclusively task-based, Taskmaster-2 contains a good number of search- and recommendation-oriented dialogs. All dialogs in this release were created using a Wizard of Oz (WOz) methodology in which crowdsourced workers played the role of a 'user' and trained call center operators played the role of the 'assistant'. In this way, users were led to believe they were interacting with an automated system that “spoke” using text-to-speech (TTS) even though it was in fact a human behind the scenes. As a result, users could express themselves however they chose in the context of an automated interface.\n",
            "url": "https://github.com/google-research-datasets/Taskmaster/tree/master/TM-2-2020",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "taskmaster3",
        "data": {
            "description": "Taskmaster is dataset for goal oriented conversations. The Taskmaster-3 dataset consists of 23,757 movie ticketing dialogs. By \"movie ticketing\" we mean conversations where the customer's goal is to purchase tickets after deciding on theater, time, movie name, number of tickets, and date, or opt out of the transaction. This collection was created using the \"self-dialog\" method. This means a single, crowd-sourced worker is paid to create a conversation writing turns for both speakers, i.e. the customer and the ticketing agent.\n",
            "url": "https://github.com/google-research-datasets/Taskmaster/tree/master/TM-3-2020",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "dialogue-modeling"
            ]
        }
    },
    {
        "id": "tatoeba",
        "data": {
            "description": "This is a collection of translated sentences from Tatoeba\n359 languages, 3,403 bitexts\ntotal number of files: 750\ntotal number of tokens: 65.54M\ntotal number of sentence fragments: 8.96M\n",
            "url": "http://opus.nlpl.eu/Tatoeba.php",
            "license": "",
            "givenLicense": "cc-by-2.0",
            "language": [
                "ab",
                "acm",
                "ady",
                "af",
                "afb",
                "afh",
                "aii",
                "ain",
                "ajp",
                "akl",
                "aln",
                "am",
                "an",
                "ang",
                "aoz",
                "apc",
                "ar",
                "arq",
                "ary",
                "arz",
                "as",
                "ast",
                "avk",
                "awa",
                "ayl",
                "az",
                "ba",
                "bal",
                "bar",
                "be",
                "ber",
                "bg",
                "bho",
                "bjn",
                "bm",
                "bn",
                "bo",
                "br",
                "brx",
                "bs",
                "bua",
                "bvy",
                "bzt",
                "ca",
                "cay",
                "cbk",
                "ce",
                "ceb",
                "ch",
                "chg",
                "chn",
                "cho",
                "chr",
                "cjy",
                "ckb",
                "ckt",
                "cmn",
                "co",
                "cpi",
                "crh",
                "crk",
                "cs",
                "csb",
                "cv",
                "cy",
                "cycl",
                "da",
                "de",
                "dng",
                "drt",
                "dsb",
                "dtp",
                "dv",
                "dws",
                "ee",
                "egl",
                "el",
                "emx",
                "en",
                "enm",
                "eo",
                "es",
                "et",
                "eu",
                "ext",
                "fi",
                "fj",
                "fkv",
                "fo",
                "fr",
                "frm",
                "fro",
                "frr",
                "fuc",
                "fur",
                "fuv",
                "fy",
                "ga",
                "gag",
                "gan",
                "gbm",
                "gcf",
                "gd",
                "gil",
                "gl",
                "gn",
                "gom",
                "gos",
                "got",
                "grc",
                "gsw",
                "gu",
                "gv",
                "ha",
                "hak",
                "haw",
                "hbo",
                "he",
                "hi",
                "hif",
                "hil",
                "hnj",
                "hoc",
                "hr",
                "hrx",
                "hsb",
                "hsn",
                "ht",
                "hu",
                "hy",
                "ia",
                "iba",
                "id",
                "ie",
                "ig",
                "ii",
                "ike",
                "ilo",
                "io",
                "is",
                "it",
                "izh",
                "ja",
                "jam",
                "jbo",
                "jdt",
                "jpa",
                "jv",
                "ka",
                "kaa",
                "kab",
                "kam",
                "kek",
                "kha",
                "kjh",
                "kk",
                "kl",
                "km",
                "kmr",
                "kn",
                "ko",
                "koi",
                "kpv",
                "krc",
                "krl",
                "ksh",
                "ku",
                "kum",
                "kw",
                "kxi",
                "ky",
                "kzj",
                "la",
                "laa",
                "lad",
                "lb",
                "ldn",
                "lfn",
                "lg",
                "lij",
                "liv",
                "lkt",
                "lld",
                "lmo",
                "ln",
                "lo",
                "lt",
                "ltg",
                "lut",
                "lv",
                "lzh",
                "lzz",
                "mad",
                "mai",
                "max",
                "mdf",
                "mfe",
                "mg",
                "mgm",
                "mh",
                "mhr",
                "mi",
                "mic",
                "min",
                "mk",
                "ml",
                "mn",
                "mni",
                "mnw",
                "moh",
                "mr",
                "mt",
                "mvv",
                "mwl",
                "mww",
                "my",
                "myv",
                "na",
                "nah",
                "nan",
                "nb",
                "nch",
                "nds",
                "ngt",
                "ngu",
                "niu",
                "nl",
                "nlv",
                "nn",
                "nog",
                "non",
                "nov",
                "npi",
                "nst",
                "nus",
                "nv",
                "ny",
                "nys",
                "oar",
                "oc",
                "ofs",
                "ood",
                "or",
                "orv",
                "os",
                "osp",
                "ota",
                "otk",
                "pa",
                "pag",
                "pal",
                "pam",
                "pap",
                "pau",
                "pcd",
                "pdc",
                "pes",
                "phn",
                "pi",
                "pl",
                "pms",
                "pnb",
                "ppl",
                "prg",
                "ps",
                "pt",
                "qu",
                "quc",
                "qya",
                "rap",
                "rif",
                "rm",
                "rn",
                "ro",
                "rom",
                "ru",
                "rue",
                "rw",
                "sa",
                "sah",
                "sc",
                "scn",
                "sco",
                "sd",
                "sdh",
                "se",
                "sg",
                "sgs",
                "shs",
                "shy",
                "si",
                "sjn",
                "sl",
                "sm",
                "sma",
                "sn",
                "so",
                "sq",
                "sr",
                "stq",
                "su",
                "sux",
                "sv",
                "swg",
                "swh",
                "syc",
                "ta",
                "te",
                "tet",
                "tg",
                "th",
                "thv",
                "ti",
                "tig",
                "tk",
                "tl",
                "tlh",
                "tly",
                "tmr",
                "tmw",
                "tn",
                "to",
                "toi",
                "toki",
                "tpi",
                "tpw",
                "tr",
                "ts",
                "tt",
                "tts",
                "tvl",
                "ty",
                "tyv",
                "tzl",
                "udm",
                "ug",
                "uk",
                "umb",
                "ur",
                "uz",
                "vec",
                "vep",
                "vi",
                "vo",
                "vro",
                "wa",
                "war",
                "wo",
                "wuu",
                "xal",
                "xh",
                "xqa",
                "yi",
                "yo",
                "yue",
                "zlm",
                "zsm",
                "zu",
                "zza"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "ted_hrlr",
        "data": {
            "description": "Data sets derived from TED talk transcripts for comparing similar language pairs\nwhere one is high resource and the other is low resource.\n",
            "url": "https://github.com/neulab/word-embeddings-for-nmt",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "ted_iwlst2013",
        "data": {
            "description": "A parallel corpus of TED talk subtitles provided by CASMACAT: http://www.casmacat.eu/corpus/ted2013.html. The files are originally provided by https://wit3.fbk.eu.\n\n15 languages, 14 bitexts\ntotal number of files: 28\ntotal number of tokens: 67.67M\ntotal number of sentence fragments: 3.81M\n",
            "url": "http://opus.nlpl.eu/TED2013.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar",
                "de",
                "en",
                "es",
                "fa",
                "fr",
                "it",
                "nl",
                "pl",
                "pt",
                "ro",
                "ru",
                "sl",
                "tr",
                "zh"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "ted_multi",
        "data": {
            "description": "Massively multilingual (60 language) data set derived from TED Talk transcripts.\nEach record consists of parallel arrays of language and text. Missing and\nincomplete translations will be filtered out.\n",
            "url": "https://github.com/neulab/word-embeddings-for-nmt",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "ted_talks_iwslt",
        "data": {
            "description": "The core of WIT3 is the TED Talks corpus, that basically redistributes the original content published by the TED Conference website (http://www.ted.com). Since 2007,\nthe TED Conference, based in California, has been posting all video recordings of its talks together with subtitles in English\nand their translations in more than 80 languages. Aside from its cultural and social relevance, this content, which is published under the Creative Commons BYNC-ND license, also represents a precious\nlanguage resource for the machine translation research community, thanks to its size, variety of topics, and covered languages.\nThis effort repurposes the original content in a way which is more convenient for machine translation researchers.\n",
            "url": "https://wit3.fbk.eu/",
            "license": "CC-BY-NC-4.0",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "mr",
                "eu",
                "hr",
                "rup",
                "szl",
                "lo",
                "ms",
                "ht",
                "hy",
                "mg",
                "arq",
                "uk",
                "ku",
                "ig",
                "sr",
                "ug",
                "ne",
                "pt-br",
                "sq",
                "af",
                "km",
                "en",
                "tt",
                "ja",
                "inh",
                "mn",
                "eo",
                "ka",
                "nb",
                "fil",
                "uz",
                "fi",
                "tl",
                "el",
                "tg",
                "bn",
                "si",
                "gu",
                "sk",
                "kn",
                "ar",
                "hup",
                "zh-tw",
                "sl",
                "be",
                "bo",
                "fr",
                "ps",
                "tr",
                "ltg",
                "la",
                "ko",
                "lv",
                "nl",
                "fa",
                "ru",
                "et",
                "vi",
                "pa",
                "my",
                "sw",
                "az",
                "sv",
                "ga",
                "sh",
                "it",
                "da",
                "lt",
                "kk",
                "mk",
                "tlh",
                "he",
                "ceb",
                "bg",
                "fr-ca",
                "ha",
                "ml",
                "mt",
                "as",
                "pt",
                "zh-cn",
                "cnh",
                "ro",
                "hi",
                "es",
                "id",
                "bs",
                "so",
                "cs",
                "te",
                "ky",
                "hu",
                "th",
                "pl",
                "nn",
                "ca",
                "is",
                "ta",
                "de",
                "srp",
                "ast",
                "bi",
                "lb",
                "art-x-bork",
                "am",
                "oc",
                "zh",
                "ur",
                "gl"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "telugu_books",
        "data": {
            "description": "This dataset is created by scraping telugu novels from teluguone.com this dataset can be used for nlp tasks like topic modeling, word embeddings, transfer learning etc\n",
            "url": "https://www.kaggle.com/sudalairajkumar/telugu-nlp",
            "license": "Data files © Original Authors",
            "givenLicense": "unknown",
            "language": [
                "te"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "telugu_news",
        "data": {
            "description": "This dataset contains Telugu language news articles along with respective\ntopic labels (business, editorial, entertainment, nation, sport) extracted from\nthe daily Andhra Jyoti. This dataset could be used to build Classification and Language Models.\n",
            "url": "https://www.kaggle.com/sudalairajkumar/telugu-nlp",
            "license": "Data files © Original Authors",
            "givenLicense": "unknown",
            "language": [
                "te"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "text-classification"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling",
                "multi-class-classification",
                "topic-classification"
            ]
        }
    },
    {
        "id": "tep_en_fa_para",
        "data": {
            "description": "TEP: Tehran English-Persian parallel corpus. The first free Eng-Per corpus, provided by the Natural Language and Text Processing Laboratory, University of Tehran.\n",
            "url": "http://opus.nlpl.eu/TEP.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "fa"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "text2log",
        "data": {
            "description": "The dataset contains about 100,000 simple English sentences selected and filtered from enTenTen15 and their translation into First Order Logic (FOL) Lambda Dependency-based Compositional Semantics using ccg2lambda.\n",
            "url": "https://github.com/alevkov/text2log",
            "license": "none provided",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "textvqa",
        "data": {
            "description": "TextVQA requires models to read and reason about text in images to answer questions about them. \nSpecifically, models need to incorporate a new modality of text present in the images and reason \nover it to answer TextVQA questions. TextVQA dataset contains 45,336 questions over 28,408 images\nfrom the OpenImages dataset.    \n",
            "url": "https://textvqa.org",
            "license": "CC BY 4.0",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "visual-question-answering"
            ],
            "tasks": [
                "visual-question-answering"
            ]
        }
    },
    {
        "id": "thainer",
        "data": {
            "description": "ThaiNER (v1.3) is a 6,456-sentence named entity recognition dataset created from expanding the 2,258-sentence\n[unnamed dataset](http://pioneer.chula.ac.th/~awirote/Data-Nutcha.zip) by\n[Tirasaroj and Aroonmanakun (2012)](http://pioneer.chula.ac.th/~awirote/publications/).\nIt is used to train NER taggers in [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp).\nThe NER tags are annotated by [Tirasaroj and Aroonmanakun (2012)]((http://pioneer.chula.ac.th/~awirote/publications/))\nfor 2,258 sentences and the rest by [@wannaphong](https://github.com/wannaphong/).\nThe POS tags are done by [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp)'s `perceptron` engine trained on `orchid_ud`.\n[@wannaphong](https://github.com/wannaphong/) is now the only maintainer of this dataset.\n",
            "url": "https://github.com/wannaphong/thai-ner/",
            "license": "CC-BY 3.0",
            "givenLicense": "cc-by-3.0",
            "language": [
                "th"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition",
                "part-of-speech-tagging"
            ]
        }
    },
    {
        "id": "thaiqa_squad",
        "data": {
            "description": "`thaiqa_squad` is an open-domain, extractive question answering dataset (4,000 questions in `train` and 74 questions in `dev`) in\n[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) format, originally created by [NECTEC](https://www.nectec.or.th/en/) from\nWikipedia articles and adapted to [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) format by [PyThaiNLP](https://github.com/PyThaiNLP/).\n",
            "url": "https://github.com/PyThaiNLP/thaiqa_squad",
            "license": "",
            "givenLicense": "cc-by-nc-sa-3.0",
            "language": [
                "th"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "thaisum",
        "data": {
            "description": "ThaiSum is a large-scale corpus for Thai text summarization obtained from several online news websites namely Thairath,\nThaiPBS, Prachathai, and The Standard. This dataset consists of over 350,000 article and summary pairs\nwritten by journalists.\n",
            "url": "https://github.com/nakhunchumpolsathien/ThaiSum",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "th"
            ],
            "categories": [
                "summarization",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "thai_toxicity_tweet",
        "data": {
            "description": "Thai Toxicity Tweet Corpus contains 3,300 tweets annotated by humans with guidelines including a 44-word dictionary.\nThe author obtained 2,027 and 1,273 toxic and non-toxic tweets, respectively; these were labeled by three annotators. The result of corpus\nanalysis indicates that tweets that include toxic words are not always toxic. Further, it is more likely that a tweet is toxic, if it contains\ntoxic words indicating their original meaning. Moreover, disagreements in annotation are primarily because of sarcasm, unclear existing\ntarget, and word sense ambiguity.\n\nNotes from data cleaner: The data is included into [huggingface/datasets](https://www.github.com/huggingface/datasets) in Dec 2020.\nBy this time, 506 of the tweets are not available publicly anymore. We denote these by `TWEET_NOT_FOUND` in `tweet_text`. \nProcessing can be found at [this PR](https://github.com/tmu-nlp/ThaiToxicityTweetCorpus/pull/1).\n",
            "url": "https://github.com/tmu-nlp/ThaiToxicityTweetCorpus/",
            "license": "",
            "givenLicense": "cc-by-nc-3.0",
            "language": [
                "th"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "the_pile_books3",
        "data": {
            "description": "This dataset is Shawn Presser's work and is part of EleutherAi/The Pile dataset. This dataset contains all of bibliotik in plain .txt form, aka 197,000 books processed in exactly the same way as did for bookcorpusopen (a.k.a. books1). seems to be similar to OpenAI's mysterious \"books2\" dataset referenced in their papers. Unfortunately OpenAI will not give details, so we know very little about any differences. People suspect it's \"all of libgen\", but it's purely conjecture.\n",
            "url": "https://github.com/soskek/bookcorpus/issues/27#issuecomment-716104208",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "the_pile_openwebtext2",
        "data": {
            "description": "OpenWebText2 is an enhanced version of the original OpenWebTextCorpus covering all Reddit submissions from 2005 up until April 2020, with further months becoming available after the corresponding PushShift dump files are released.\n",
            "url": "https://openwebtext2.readthedocs.io/en/latest/",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "text-scoring"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling",
                "text-scoring-other-rating"
            ]
        }
    },
    {
        "id": "the_pile_stack_exchange",
        "data": {
            "description": "This dataset is part of EleutherAI/The Pile dataset and is a dataset for Language Models from processing stackexchange data dump, which is an anonymized dump of all user-contributed content on the Stack Exchange network.\n",
            "url": "https://github.com/EleutherAI/stackexchange-dataset",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "tilde_model",
        "data": {
            "description": "This is the Tilde MODEL Corpus – Multilingual Open Data for European Languages.\n\nThe data has been collected from sites allowing free use and reuse of its content, as well as from Public Sector web sites. The activities have been undertaken as part of the ODINE Open Data Incubator for Europe, which aims to support the next generation of digital businesses and fast-track the development of new products and services. The corpus includes the following parts:\nTilde MODEL - EESC is a multilingual corpus compiled from document texts of European Economic and Social Committee document portal. Source: http://dm.eesc.europa.eu/\nTilde MODEL - RAPID multilingual parallel corpus is compiled from all press releases of Press Release Database of European Commission released between 1975 and end of 2016 as available from http://europa.eu/rapid/\nTilde MODEL - ECB multilingual parallel corpus is compiled from the multilingual pages of European Central Bank web site http://ebc.europa.eu/\nTilde MODEL - EMA is a corpus compiled from texts of European Medicines Agency document portal as available in http://www.ema.europa.eu/ at the end of 2016\nTilde MODEL - World Bank is a corpus compiled from texts of World Bank as available in http://www.worldbank.org/ in 2017\nTilde MODEL - AirBaltic.com Travel Destinations is a multilingual parallel corpus compiled from description texts of AirBaltic.com travel destinations as available in https://www.airbaltic.com/en/destinations/ in 2017\nTilde MODEL - LiveRiga.com is a multilingual parallel corpus compiled from Riga tourist attractions description texts of http://liveriga.com/ web site in 2017\nTilde MODEL - Lithuanian National Philharmonic Society is a parallel corpus compiled from texts of Lithuanian National Philharmonic Society web site http://www.filharmonija.lt/ in 2017\nTilde MODEL - mupa.hu is a parallel corpus from texts of Müpa Budapest - web site of Hungarian national culture house and concert venue https://www.mupa.hu/en/ compiled in spring of 2017\nTilde MODEL - fold.lv is a parallel corpus from texts of fold.lv portal http://www.fold.lv/en/ of the best of Latvian and foreign creative industries as compiled in spring of 2017\nTilde MODEL - czechtourism.com is a multilingual parallel corpus from texts of http://czechtourism.com/ portal compiled in spring of 2017\n30 languages, 274 bitexts\ntotal number of files: 125\ntotal number of tokens: 1.43G\ntotal number of sentence fragments: 62.44M\n",
            "url": "http://opus.nlpl.eu/TildeMODEL.php",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "bg",
                "cs",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "fi",
                "fr",
                "hr",
                "hu",
                "is",
                "it",
                "lt",
                "lv",
                "mt",
                "nl",
                "no",
                "pl",
                "pt",
                "ro",
                "ru",
                "sk",
                "sl",
                "sq",
                "sr",
                "sv",
                "tr",
                "uk"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "time_dial",
        "data": {
            "description": "TimeDial presents a crowdsourced English challenge set, for temporal commonsense reasoning, formulated\nas a multiple choice cloze task with around 1.5k carefully curated dialogs. The dataset is derived from\nthe DailyDialog (Li et al., 2017), which is a multi-turn dialog corpus.\n\nIn order to establish strong baselines and provide information on future model development, we\nconducted extensive experiments with state-of-the-art LMs. While humans can easily answer these\nquestions (97.8%), the best T5 model variant struggles on this challenge set (73%). Moreover, our\nqualitative error analyses show that the models often rely on shallow, spurious features (particularly text\nmatching), instead of truly doing reasoning over the context.\n",
            "url": "https://github.com/google-research-datasets/timedial",
            "license": "TimeDial dataset is licensed under CC BY-NC-SA 4.0",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "multi-label-classification",
                "text-classification-other-dialog-act-classification"
            ]
        }
    },
    {
        "id": "times_of_india_news_headlines",
        "data": {
            "description": "This news dataset is a persistent historical archive of noteable events in the Indian subcontinent from start-2001 to mid-2020, recorded in realtime by the journalists of India. It contains approximately 3.3 million events published by Times of India. Times Group as a news agency, reaches out a very wide audience across Asia and drawfs every other agency in the quantity of english articles published per day. Due to the heavy daily volume over multiple years, this data offers a deep insight into Indian society, its priorities, events, issues and talking points and how they have unfolded over time. It is possible to chop this dataset into a smaller piece for a more focused analysis, based on one or more facets.\n",
            "url": "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/J7BYRX",
            "license": "",
            "givenLicense": "cc0-1.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation",
                "text-retrieval"
            ],
            "tasks": [
                "document-retrieval",
                "fact-checking-retrieval",
                "text-simplification"
            ]
        }
    },
    {
        "id": "tiny_shakespeare",
        "data": {
            "description": "40,000 lines of Shakespeare from a variety of Shakespeare's plays. Featured in Andrej Karpathy's blog post 'The Unreasonable Effectiveness of Recurrent Neural Networks': http://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n\nTo use for e.g. character modelling:\n\n```\nd = datasets.load_dataset(name='tiny_shakespeare')['train']\nd = d.map(lambda x: datasets.Value('strings').unicode_split(x['text'], 'UTF-8'))\n# train split includes vocabulary for other splits\nvocabulary = sorted(set(next(iter(d)).numpy()))\nd = d.map(lambda x: {'cur_char': x[:-1], 'next_char': x[1:]})\nd = d.unbatch()\nseq_len = 100\nbatch_size = 2\nd = d.batch(seq_len)\nd = d.batch(batch_size)\n```\n",
            "url": "https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "tlc",
        "data": {
            "description": "Thai Literature Corpora (TLC): Corpora of machine-ingestible Thai classical literature texts.\n\nRelease: 6/25/19\n\nIt consists of two datasets:\n\n## TLC set\nIt is texts from [Vajirayana Digital Library](https://vajirayana.org/), stored by chapters and stanzas (non-tokenized).\n\ntlc v.2.0 (6/17/19 : a total of 34 documents, 292,270 lines, 31,790,734 characters)\ntlc v.1.0 (6/11/19 : a total of 25 documents, 113,981 lines, 28,775,761 characters)\n\n## TNHC set\nIt is texts from Thai National Historical Corpus, stored by lines (manually tokenized).\n\ntnhc v.1.0 (6/25/19 : a total of 47 documents, 756,478 lines, 13,361,142 characters)\n",
            "url": "https://attapol.github.io/tlc.html",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "th"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "tmu_gfm_dataset",
        "data": {
            "description": "A dataset for GEC metrics with manual evaluations of grammaticality, fluency, and meaning preservation for system outputs. More detail about the creation of the dataset can be found in Yoshimura et al. (2020).\n",
            "url": "https://github.com/tmu-nlp/TMU-GFM-Dataset",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-grammatical-error-correction"
            ]
        }
    },
    {
        "id": "tne",
        "data": {
            "description": "TNE is an NLU task, which focus on relations between noun phrases (NPs) that can be mediated via prepositions.\nThe dataset contains 5,497 documents, annotated exhaustively with all possible links between the NPs in each document.\n",
            "url": "https://yanaiela.github.io/TNE/",
            "license": "MIT",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "text-retrieval"
            ],
            "tasks": [
                "document-retrieval"
            ]
        }
    },
    {
        "id": "told-br",
        "data": {
            "description": "ToLD-Br is the biggest dataset for toxic tweets in Brazilian Portuguese, crowdsourced\nby 42 annotators selected from a pool of 129 volunteers. Annotators were selected aiming\nto create a plural group in terms of demographics (ethnicity, sexual orientation, age, gender).\nEach tweet was labeled by three annotators in 6 possible categories:\nLGBTQ+phobia,Xenophobia, Obscene, Insult, Misogyny and Racism.\n",
            "url": "https://github.com/JAugusto97/ToLD-Br",
            "license": "https://github.com/JAugusto97/ToLD-Br/blob/main/LICENSE_ToLD-Br.txt ",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "pt-BR"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-hate-speech-detection"
            ]
        }
    },
    {
        "id": "totto",
        "data": {
            "description": "ToTTo is an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description.\n",
            "url": "",
            "license": "",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "table-to-text"
            ],
            "tasks": []
        }
    },
    {
        "id": "trec",
        "data": {
            "description": "The Text REtrieval Conference (TREC) Question Classification dataset contains 5500 labeled questions in training set and another 500 for test set. The dataset has 6 labels, 47 level-2 labels. Average length of each sentence is 10, vocabulary size of 8700.\n\nData are collected from four sources: 4,500 English questions published by USC (Hovy et al., 2001), about 500 manually constructed questions for a few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions from TREC 10 which serves as the test set.\n",
            "url": "https://cogcomp.seas.upenn.edu/Data/QA/QC/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "trivia_qa",
        "data": {
            "description": "TriviaqQA is a reading comprehension dataset containing over 650K\nquestion-answer-evidence triples. TriviaqQA includes 95K question-answer\npairs authored by trivia enthusiasts and independently gathered evidence\ndocuments, six per question on average, that provide high quality distant\nsupervision for answering the questions.\n",
            "url": "http://nlp.cs.washington.edu/triviaqa/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering",
                "text2text-generation"
            ],
            "tasks": [
                "open-domain-qa",
                "open-domain-abstractive-qa",
                "extractive-qa",
                "abstractive-qa"
            ]
        }
    },
    {
        "id": "truthful_qa",
        "data": {
            "description": "TruthfulQA is a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. Questions are\ncrafted so that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts.\n",
            "url": "https://github.com/sylinrl/TruthfulQA",
            "license": "Apache License 2.0",
            "givenLicense": "apache-2.0",
            "language": [
                "en"
            ],
            "categories": [
                "multiple-choice",
                "text-generation",
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa",
                "language-modeling",
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "tsac",
        "data": {
            "description": "Tunisian Sentiment Analysis Corpus.\n\nAbout 17k user comments manually annotated to positive and negative polarities. This corpus is collected from Facebook users comments written on official pages of Tunisian radios and TV channels namely Mosaique FM, JawhraFM, Shemes FM, HiwarElttounsi TV and Nessma TV. The corpus is collected from a period spanning January 2015 until June 2016.\n",
            "url": "https://github.com/fbougares/TSAC",
            "license": "",
            "givenLicense": "lgpl-3.0",
            "language": [
                "aeb"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "ttc4900",
        "data": {
            "description": "The data set is taken from kemik group\nhttp://www.kemik.yildiz.edu.tr/\nThe data are pre-processed for the text categorization, collocations are found, character set is corrected, and so forth.\nWe named TTC4900 by mimicking the name convention of TTC 3600 dataset shared by the study http://journals.sagepub.com/doi/abs/10.1177/0165551515620551\n\nIf you use the dataset in a paper, please refer https://www.kaggle.com/savasy/ttc4900 as footnote and cite one of the papers as follows:\n\n- A Comparison of Different Approaches to Document Representation in Turkish Language, SDU Journal of Natural and Applied Science, Vol 22, Issue 2, 2018\n- A comparative analysis of text classification for Turkish language, Pamukkale University Journal of Engineering Science Volume 25 Issue 5, 2018\n- A Knowledge-poor Approach to Turkish Text Categorization with a Comparative Analysis, Proceedings of CICLING 2014, Springer LNCS, Nepal, 2014.\n",
            "url": "https://www.kaggle.com/savasy/ttc4900",
            "license": "CC0: Public Domain",
            "givenLicense": "unknown",
            "language": [
                "tr"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-news-category-classification"
            ]
        }
    },
    {
        "id": "tunizi",
        "data": {
            "description": "On social media, Arabic speakers tend to express themselves in their own local dialect. To do so, Tunisians use \"Tunisian Arabizi\", which consists in supplementing numerals to the Latin script rather than the Arabic alphabet. TUNIZI is the first Tunisian Arabizi Dataset including 3K sentences, balanced, covering different topics, preprocessed and annotated as positive and negative.\n",
            "url": "https://github.com/chaymafourati/TUNIZI-Sentiment-Analysis-Tunisian-Arabizi-Dataset",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "aeb"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "tuple_ie",
        "data": {
            "description": "The TupleInf Open IE dataset contains Open IE tuples extracted from 263K sentences that were used by the solver in “Answering Complex Questions Using Open Information Extraction” (referred as Tuple KB, T). These sentences were collected from a large Web corpus using training questions from 4th and 8th grade as queries. This dataset contains 156K sentences collected for 4th grade questions and 107K sentences for 8th grade questions. Each sentence is followed by the Open IE v4 tuples using their simple format.\n",
            "url": "https://allenai.org/data/tuple-ie",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "other"
            ],
            "tasks": [
                "other-other-open-information-extraction"
            ]
        }
    },
    {
        "id": "turk",
        "data": {
            "description": "TURKCorpus is a dataset for evaluating sentence simplification systems that focus on lexical paraphrasing,\nas described in \"Optimizing Statistical Machine Translation for Text Simplification\". The corpus is composed of 2000 validation and 359 test original sentences that were each simplified 8 times by different annotators.\n",
            "url": "https://github.com/cocoxu/simplification",
            "license": "GNU General Public License v3.0",
            "givenLicense": "gpl-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text-simplification"
            ]
        }
    },
    {
        "id": "turkic_xwmt",
        "data": {
            "description": "A Large-Scale Study of Machine Translation in Turkic Languages\n",
            "url": "https://github.com/turkicinterlingua/til-mt",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "az",
                "ba",
                "en",
                "kaa",
                "kk",
                "ky",
                "ru",
                "sah",
                "tr",
                "uz"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "turkish_movie_sentiment",
        "data": {
            "description": "This data set is a dataset from kaggle consisting of Turkish movie reviews and scored between 0-5.\n",
            "url": "https://www.kaggle.com/mustfkeskin/turkish-movie-sentiment-analysis-dataset",
            "license": "CC0: Public Domain",
            "givenLicense": "unknown",
            "language": [
                "tr"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification",
                "sentiment-scoring"
            ]
        }
    },
    {
        "id": "turkish_ner",
        "data": {
            "description": "Turkish Wikipedia Named-Entity Recognition and Text Categorization\n(TWNERTC) dataset is a collection of automatically categorized and annotated\nsentences obtained from Wikipedia. The authors constructed large-scale\ngazetteers by using a graph crawler algorithm to extract\nrelevant entity and domain information\nfrom a semantic knowledge base, Freebase.\nThe constructed gazetteers contains approximately\n300K entities with thousands of fine-grained entity types\nunder 77 different domains.\n",
            "url": "https://data.mendeley.com/datasets/cdcztymf4k/1",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "tr"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "turkish_product_reviews",
        "data": {
            "description": "\nTurkish Product Reviews.\nThis repository contains 235.165 product reviews collected online. There are 220.284 positive, 14881 negative reviews.\n",
            "url": "https://github.com/fthbrmnby/turkish-text-data",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "tr"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "turkish_shrinked_ner",
        "data": {
            "description": "Shrinked version (48 entity type) of the turkish_ner.\n\nOriginal turkish_ner dataset: Automatically annotated Turkish corpus for named entity recognition and text categorization using large-scale gazetteers. The constructed gazetteers contains approximately 300K entities with thousands of fine-grained entity types under 25 different domains.\n\nShrinked entity types are: academic, academic_person, aircraft, album_person, anatomy, animal, architect_person, capital, chemical, clothes, country, culture, currency, date, food, genre, government, government_person, language, location, material, measure, medical, military, military_person, nation, newspaper, organization, organization_person, person, production_art_music, production_art_music_person, quantity, religion, science, shape, ship, software, space, space_person, sport, sport_name, sport_person, structure, subject, tech, train, vehicle\n",
            "url": "https://www.kaggle.com/behcetsenturk/shrinked-twnertc-turkish-ner-data-by-kuzgunlar",
            "license": "Attribution 4.0 International (CC BY 4.0)",
            "givenLicense": "cc-by-4.0",
            "language": [
                "tr"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "turku_ner_corpus",
        "data": {
            "description": "An open, broad-coverage corpus for Finnish named entity recognition presented in Luoma et al. (2020) A Broad-coverage Corpus for Finnish Named Entity Recognition.\n",
            "url": "https://github.com/klintan/swedish-ner-corpus",
            "license": "",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "fi"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "tweet_eval",
        "data": {
            "description": "TweetEval consists of seven heterogenous tasks in Twitter, all framed as multi-class tweet classification. All tasks have been unified into the same benchmark, with each dataset presented in the same format and with fixed training, validation and test splits.\n",
            "url": "https://github.com/cardiffnlp/tweeteval",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification",
                "multi-class-classification",
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "tweet_qa",
        "data": {
            "description": "TweetQA is the first dataset for QA on social media data by leveraging news media and crowdsourcing.\n",
            "url": "https://tweetqa.github.io/",
            "license": "CC BY-SA 4.0",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "tweets_ar_en_parallel",
        "data": {
            "description": "    Twitter users often post parallel tweets—tweets that contain the same content but are\n    written in different languages. Parallel tweets can be an important resource for developing\n    machine translation (MT) systems among other natural language processing (NLP) tasks. This\n    resource is a result of a generic method for collecting parallel tweets. Using the method,\n    we compiled a bilingual corpus of English-Arabic parallel tweets and a list of Twitter accounts\n    who post English-Arabic tweets regularly. Additionally, we annotate a subset of Twitter accounts\n    with their countries of origin and topic of interest, which provides insights about the population\n    who post parallel tweets.\n",
            "url": "https://alt.qcri.org/resources/bilingual_corpus_of_parallel_tweets",
            "license": "",
            "givenLicense": "apache-2.0",
            "language": [
                "ar",
                "en"
            ],
            "categories": [
                "translation"
            ],
            "tasks": [
                "translation-other-tweets-translation"
            ]
        }
    },
    {
        "id": "tweets_hate_speech_detection",
        "data": {
            "description": "The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n\nFormally, given a training sample of tweets and labels, where label ‘1’ denotes the tweet is racist/sexist and label ‘0’ denotes the tweet is not racist/sexist, your objective is to predict the labels on the given test dataset.\n",
            "url": "https://github.com/sharmaroshan/Twitter-Sentiment-Analysis",
            "license": "",
            "givenLicense": "gpl-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "twi_text_c3",
        "data": {
            "description": "Twi Text C3 is the largest Twi texts collected and used to train FastText embeddings in the\nYorubaTwi Embedding paper: https://www.aclweb.org/anthology/2020.lrec-1.335/\n",
            "url": "https://www.aclweb.org/anthology/2020.lrec-1.335/",
            "license": "",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "tw"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "twi_wordsim353",
        "data": {
            "description": "A translation of the word pair similarity dataset wordsim-353 to Twi.\n\nThe dataset was presented in the paper\nAlabi et al.: Massive vs. Curated Embeddings for Low-Resourced\nLanguages: the Case of Yorùbá and Twi (LREC 2020).\n",
            "url": "https://github.com/ajesujoba/YorubaTwi-Embedding",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "tw"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "semantic-similarity-scoring"
            ]
        }
    },
    {
        "id": "tydiqa",
        "data": {
            "description": "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\nexpresses -- such that we expect models performing well on this set to generalize across a large number of the languages\nin the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic\ninformation-seeking task and avoid priming effects, questions are written by people who want to know the answer, but\ndon’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without\nthe use of translation (unlike MLQA and XQuAD).\n",
            "url": "https://github.com/google-research-datasets/tydiqa",
            "license": "",
            "givenLicense": "apache-2.0",
            "language": [
                "en",
                "ar",
                "bn",
                "fi",
                "id",
                "ja",
                "sw",
                "ko",
                "ru",
                "te",
                "th"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "ubuntu_dialogs_corpus",
        "data": {
            "description": "Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter.\n",
            "url": "https://github.com/rkadlec/ubuntu-ranking-dataset-creator",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "udhr",
        "data": {
            "description": "The Universal Declaration of Human Rights (UDHR) is a milestone document in the history of human rights. Drafted by\nrepresentatives with different legal and cultural backgrounds from all regions of the world, it set out, for the\nfirst time, fundamental human rights to be universally protected. The Declaration was adopted by the UN General\nAssembly in Paris on 10 December 1948 during its 183rd plenary meeting. The dataset includes translations of the\ndocument in 464+ languages and dialects.\n\n© 1996 – 2009 The Office of the High Commissioner for Human Rights\n\nThis plain text version prepared by the “UDHR in Unicode” project, https://www.unicode.org/udhr.\n",
            "url": "https://www.ohchr.org/en/universal-declaration-of-human-rights",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "aa",
                "ab",
                "ace",
                "acu",
                "ada",
                "ady",
                "af",
                "agr",
                "aii",
                "ajg",
                "als",
                "alt",
                "am",
                "amc",
                "ame",
                "ami",
                "amr",
                "ar",
                "arl",
                "arn",
                "ast",
                "auc",
                "ay",
                "az",
                "az-Cyrl",
                "az-Latn",
                "ban",
                "bax",
                "bba",
                "bci",
                "be",
                "bem",
                "bfa",
                "bg",
                "bho",
                "bi",
                "bik",
                "bin",
                "blt",
                "bm",
                "bn",
                "bo",
                "boa",
                "br",
                "bs",
                "bs-Cyrl",
                "bs-Latn",
                "buc",
                "bug",
                "bum",
                "ca",
                "cab",
                "cak",
                "cbi",
                "cbr",
                "cbs",
                "cbt",
                "cbu",
                "ccp",
                "ceb",
                "cfm",
                "ch",
                "chj",
                "chk",
                "chr",
                "cic",
                "cjk",
                "cjs",
                "cjy",
                "ckb",
                "ckb-Latn",
                "cnh",
                "cni",
                "cnr",
                "co",
                "cof",
                "cot",
                "cpu",
                "crh",
                "cri",
                "crs",
                "cs",
                "csa",
                "csw",
                "ctd",
                "cy",
                "da",
                "dag",
                "ddn",
                "de",
                "de-1901",
                "de-1996",
                "dga",
                "dip",
                "duu",
                "dv",
                "dyo",
                "dyu",
                "dz",
                "ee",
                "el",
                "el-monoton",
                "el-polyton",
                "en",
                "eo",
                "es",
                "ese",
                "et",
                "eu",
                "eve",
                "evn",
                "fa",
                "fa-AF",
                "fat",
                "fi",
                "fj",
                "fkv",
                "fo",
                "fon",
                "fr",
                "fuf",
                "fuf-Adlm",
                "fur",
                "fuv",
                "fvr",
                "fy",
                "ga",
                "gaa",
                "gag",
                "gan",
                "gd",
                "gjn",
                "gkp",
                "gl",
                "gld",
                "gn",
                "gsw",
                "gu",
                "guc",
                "guu",
                "gv",
                "gyr",
                "ha",
                "hak",
                "ha-NE",
                "ha-NG",
                "haw",
                "he",
                "hi",
                "hil",
                "hlt",
                "hmn",
                "hms",
                "hna",
                "hni",
                "hnj",
                "hns",
                "hr",
                "hsb",
                "hsn",
                "ht",
                "hu",
                "hus",
                "huu",
                "hy",
                "ia",
                "ibb",
                "id",
                "idu",
                "ig",
                "ii",
                "ijs",
                "ilo",
                "io",
                "is",
                "it",
                "iu",
                "ja",
                "jiv",
                "jv",
                "jv-Java",
                "ka",
                "kaa",
                "kbd",
                "kbp",
                "kde",
                "kdh",
                "kea",
                "kek",
                "kg",
                "kg-AO",
                "kha",
                "kjh",
                "kk",
                "kkh",
                "kkh-Lana",
                "kl",
                "km",
                "kmb",
                "kn",
                "ko",
                "koi",
                "koo",
                "kqn",
                "kqs",
                "kr",
                "kri",
                "krl",
                "ktu",
                "ku",
                "kwi",
                "ky",
                "la",
                "lad",
                "lah",
                "lb",
                "lg",
                "lia",
                "lij",
                "lld",
                "ln",
                "lns",
                "lo",
                "lob",
                "lot",
                "loz",
                "lt",
                "lua",
                "lue",
                "lun",
                "lus",
                "lv",
                "mad",
                "mag",
                "mai",
                "mam",
                "man",
                "maz",
                "mcd",
                "mcf",
                "men",
                "mfq",
                "mg",
                "mh",
                "mi",
                "mic",
                "min",
                "miq",
                "mk",
                "ml",
                "mn",
                "mn-Cyrl",
                "mnw",
                "mor",
                "mos",
                "mr",
                "mt",
                "mto",
                "mxi",
                "mxv",
                "my",
                "mzi",
                "nan",
                "nb",
                "nba",
                "nds",
                "ne",
                "ng",
                "nhn",
                "nio",
                "niu",
                "niv",
                "njo",
                "nku",
                "nl",
                "nn",
                "not",
                "nr",
                "nso",
                "nv",
                "ny",
                "nym",
                "nyn",
                "nzi",
                "oaa",
                "oc",
                "ojb",
                "oki",
                "om",
                "orh",
                "os",
                "ote",
                "pa",
                "pam",
                "pap",
                "pau",
                "pbb",
                "pcd",
                "pcm",
                "pis",
                "piu",
                "pl",
                "pon",
                "pov",
                "ppl",
                "prq",
                "ps",
                "pt",
                "pt-BR",
                "pt-PT",
                "qu",
                "quc",
                "qug",
                "quh",
                "quy",
                "qva",
                "qvc",
                "qvh",
                "qvm",
                "qvn",
                "qwh",
                "qxn",
                "qxu",
                "rar",
                "rgn",
                "rm",
                "rmn",
                "rm-puter",
                "rm-rumgr",
                "rm-surmiran",
                "rm-sursilv",
                "rm-sutsilv",
                "rm-vallader",
                "rn",
                "ro",
                "ru",
                "rup",
                "rw",
                "sa",
                "sa-Gran",
                "sah",
                "sc",
                "sco",
                "se",
                "sey",
                "sg",
                "shk",
                "shn",
                "shp",
                "si",
                "sk",
                "skr",
                "sl",
                "slr",
                "sm",
                "sn",
                "snk",
                "snn",
                "so",
                "sr",
                "sr-Cyrl",
                "sr-Latn",
                "srr",
                "ss",
                "st",
                "su",
                "suk",
                "sus",
                "sv",
                "sw",
                "swb",
                "ta",
                "taj",
                "ta-LK",
                "tbz",
                "tca",
                "tdt",
                "te",
                "tem",
                "tet",
                "tg",
                "th",
                "ti",
                "tiv",
                "tk",
                "tk-Cyrl",
                "tk-Latn",
                "tl",
                "tly",
                "tn",
                "to",
                "tob",
                "toi",
                "toj",
                "top",
                "tpi",
                "tr",
                "ts",
                "tsz",
                "tt",
                "tw-akuapem",
                "tw-asante",
                "ty",
                "tyv",
                "tzh",
                "tzm",
                "tzo",
                "udu",
                "ug",
                "ug-Arab",
                "ug-Latn",
                "uk",
                "umb",
                "und",
                "ur",
                "ura",
                "uz",
                "uz-Cyrl",
                "uz-Latn",
                "vai",
                "ve",
                "vec",
                "vep",
                "vi",
                "vi-Hani",
                "vmw",
                "wa",
                "war",
                "wo",
                "wuu",
                "wwa",
                "xh",
                "xsm",
                "yad",
                "yao",
                "yap",
                "yi",
                "ykg",
                "yo",
                "yrk",
                "yua",
                "yue",
                "za",
                "zam",
                "zdj",
                "zgh",
                "zh",
                "zh-Hant",
                "zlm",
                "zlm-Arab",
                "zlm-Latn",
                "zro",
                "ztu",
                "zu"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "um005",
        "data": {
            "description": "UMC005 English-Urdu is a parallel corpus of texts in English and Urdu language with sentence alignments. The corpus can be used for experiments with statistical machine translation.\n\nThe texts come from four different sources:\n- Quran\n- Bible\n- Penn Treebank (Wall Street Journal)\n- Emille corpus\n\nThe authors provide the religious texts of Quran and Bible for direct download. Because of licensing reasons, Penn and Emille texts cannot be redistributed freely. However, if you already hold a license for the original corpora, we are able to provide scripts that will recreate our data on your disk. Our modifications include but are not limited to the following:\n\n- Correction of Urdu translations and manual sentence alignment of the Emille texts.\n- Manually corrected sentence alignment of the other corpora.\n- Our data split (training-development-test) so that our published experiments can be reproduced.\n- Tokenization (optional, but needed to reproduce our experiments).\n- Normalization (optional) of e.g. European vs. Urdu numerals, European vs. Urdu punctuation, removal of Urdu diacritics.\n",
            "url": "http://ufal.ms.mff.cuni.cz/umc/005-en-ur/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "ur"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "un_ga",
        "data": {
            "description": "United nations general assembly resolutions: A six-language parallel corpus.\nThis is a collection of translated documents from the United Nations originally compiled into a translation memory by Alexandre Rafalovitch, Robert Dale (see http://uncorpora.org).\n6 languages, 15 bitexts\ntotal number of files: 6\ntotal number of tokens: 18.87M\ntotal number of sentence fragments: 0.44M\n",
            "url": "http://opus.nlpl.eu/UN.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar",
                "en",
                "es",
                "fr",
                "ru",
                "zh"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "universal_dependencies",
        "data": {
            "description": "Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008).\n",
            "url": "https://universaldependencies.org/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "no",
                "af",
                "aii",
                "ajp",
                "akk",
                "am",
                "apu",
                "aqz",
                "ar",
                "be",
                "bg",
                "bho",
                "bm",
                "br",
                "bxr",
                "ca",
                "ckt",
                "cop",
                "cs",
                "cu",
                "cy",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "eu",
                "fa",
                "fi",
                "fo",
                "fr",
                "fro",
                "ga",
                "gd",
                "gl",
                "got",
                "grc",
                "gsw",
                "gun",
                "gv",
                "he",
                "hi",
                "hr",
                "hsb",
                "hu",
                "hy",
                "id",
                "is",
                "it",
                "ja",
                "kfm",
                "kk",
                "kmr",
                "ko",
                "koi",
                "kpv",
                "krl",
                "la",
                "lt",
                "lv",
                "lzh",
                "mdf",
                "mr",
                "mt",
                "myu",
                "myv",
                "nl",
                "nyq",
                "olo",
                "orv",
                "otk",
                "pcm",
                "pl",
                "pt",
                "ro",
                "ru",
                "sa",
                "sk",
                "sl",
                "sme",
                "sms",
                "soj",
                "sq",
                "sr",
                "sv",
                "swl",
                "ta",
                "te",
                "th",
                "tl",
                "tpn",
                "tr",
                "ug",
                "uk",
                "ur",
                "vi",
                "wbp",
                "wo",
                "yo",
                "yue",
                "zh"
            ],
            "categories": [
                "token-classification",
                "natural-language-processing"
            ],
            "tasks": [
                "parsing",
                "token-classification-other-constituency-parsing",
                "token-classification-other-dependency-parsing"
            ]
        }
    },
    {
        "id": "universal_morphologies",
        "data": {
            "description": "The Universal Morphology (UniMorph) project is a collaborative effort to improve how NLP handles complex morphology in the world’s languages.\nThe goal of UniMorph is to annotate morphological data in a universal schema that allows an inflected word from any language to be defined by its lexical meaning,\ntypically carried by the lemma, and by a rendering of its inflectional form in terms of a bundle of morphological features from our schema.\nThe specification of the schema is described in Sylak-Glassman (2016).\n",
            "url": "https://unimorph.github.io/",
            "license": "CC-BY-SA-3.0",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "ady",
                "ang",
                "ar",
                "arn",
                "ast",
                "az",
                "ba",
                "be",
                "bg",
                "bn",
                "bo",
                "br",
                "ca",
                "ckb",
                "crh",
                "cs",
                "csb",
                "cu",
                "cy",
                "da",
                "de",
                "dsb",
                "el",
                "en",
                "es",
                "et",
                "eu",
                "fa",
                "fi",
                "fo",
                "fr",
                "frm",
                "fro",
                "frr",
                "fur",
                "fy",
                "ga",
                "gal",
                "gd",
                "gmh",
                "gml",
                "got",
                "grc",
                "gv",
                "hai",
                "he",
                "hi",
                "hu",
                "hy",
                "is",
                "it",
                "izh",
                "ka",
                "kbd",
                "kjh",
                "kk",
                "kl",
                "klr",
                "kmr",
                "kn",
                "krl",
                "kw",
                "la",
                "liv",
                "lld",
                "lt",
                "lud",
                "lv",
                "mk",
                "mt",
                "mwf",
                "nap",
                "nb",
                "nds",
                "nl",
                "nn",
                "nv",
                "oc",
                "olo",
                "osx",
                "pl",
                "ps",
                "pt",
                "qu",
                "ro",
                "ru",
                "sa",
                "sga",
                "sh",
                "sl",
                "sme",
                "sq",
                "sv",
                "swc",
                "syc",
                "te",
                "tg",
                "tk",
                "tr",
                "tt",
                "uk",
                "ur",
                "uz",
                "vec",
                "vep",
                "vot",
                "xcl",
                "xno",
                "yi",
                "zu"
            ],
            "categories": [
                "token-classification",
                "text-classification"
            ],
            "tasks": [
                "multi-class-classification",
                "multi-label-classification",
                "token-classification-other-morphology"
            ]
        }
    },
    {
        "id": "un_multi",
        "data": {
            "description": "This is a collection of translated documents from the United Nations. This corpus is available in all 6 official languages of the UN, consisting of around 300 million words per language\n",
            "url": "http://www.euromatrixplus.net/multi-un/",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar",
                "de",
                "en",
                "es",
                "fr",
                "ru",
                "zh"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "un_pc",
        "data": {
            "description": "This parallel corpus consists of manually translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish.\n",
            "url": "http://opus.nlpl.eu/UNPC.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar",
                "en",
                "es",
                "fr",
                "ru",
                "zh"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "urdu_fake_news",
        "data": {
            "description": "\nUrdu fake news datasets that contain news of 5 different news domains.\nThese domains are Sports, Health, Technology, Entertainment, and Business.\nThe real news are collected by combining manual approaches.\n",
            "url": "https://github.com/MaazAmjad/Datasets-for-Urdu-news",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ur"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "fact-checking",
                "intent-classification"
            ]
        }
    },
    {
        "id": "urdu_sentiment_corpus",
        "data": {
            "description": "\n“Urdu Sentiment Corpus” (USC) shares the dat of Urdu tweets for the sentiment analysis and polarity detection.\nThe dataset is consisting of tweets and overall, the dataset is comprising over 17, 185 tokens\nwith 52% records as positive, and 48 % records as negative.\n",
            "url": "https://github.com/MuhammadYaseenKhan/Urdu-Sentiment-Corpus",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ur"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "vctk",
        "data": {
            "description": "",
            "url": "https://datashare.ed.ac.uk/handle/10283/3443",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "automatic-speech-recognition"
            ],
            "tasks": []
        }
    },
    {
        "id": "visual_genome",
        "data": {
            "description": "Visual Genome enable to model objects and relationships between objects.\nThey collect dense annotations of objects, attributes, and relationships within each image.\nSpecifically, the dataset contains over 108K images where each image has an average of 35 objects, 26 attributes, and 21 pairwise relationships between objects.\n",
            "url": "https://visualgenome.org/",
            "license": "Creative Commons Attribution 4.0 International License",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "image-to-text",
                "object-detection",
                "visual-question-answering"
            ],
            "tasks": [
                "image-captioning"
            ]
        }
    },
    {
        "id": "vivos",
        "data": {
            "description": "VIVOS is a free Vietnamese speech corpus consisting of 15 hours of recording speech prepared for\nVietnamese Automatic Speech Recognition task.\nThe corpus was prepared by AILAB, a computer science lab of VNUHCM - University of Science, with Prof. Vu Hai Quan is the head of.\nWe publish this corpus in hope to attract more scientists to solve Vietnamese speech recognition problems.\n",
            "url": "https://ailab.hcmus.edu.vn/vivos",
            "license": "cc-by-sa-4.0",
            "givenLicense": "cc-by-nc-sa-4.0",
            "language": [
                "vi"
            ],
            "categories": [
                "automatic-speech-recognition"
            ],
            "tasks": []
        }
    },
    {
        "id": "web_nlg",
        "data": {
            "description": "The WebNLG challenge consists in mapping data to text. The training data consists\nof Data/Text pairs where the data is a set of triples extracted from DBpedia and the text is a verbalisation\nof these triples. For instance, given the 3 DBpedia triples shown in (a), the aim is to generate a text such as (b).\n\na. (John_E_Blaha birthDate 1942_08_26) (John_E_Blaha birthPlace San_Antonio) (John_E_Blaha occupation Fighter_pilot)\nb. John E Blaha, born in San Antonio on 1942-08-26, worked as a fighter pilot\n\nAs the example illustrates, the task involves specific NLG subtasks such as sentence segmentation\n(how to chunk the input data into sentences), lexicalisation (of the DBpedia properties),\naggregation (how to avoid repetitions) and surface realisation\n(how to build a syntactically correct and natural sounding text).\n",
            "url": "https://webnlg-challenge.loria.fr/",
            "license": "",
            "givenLicense": "cc-by-sa-3.0,cc-by-nc-sa-4.0,gfdl",
            "language": [
                "en",
                "ru"
            ],
            "categories": [
                "tabular-to-text"
            ],
            "tasks": [
                "rdf-to-text"
            ]
        }
    },
    {
        "id": "web_of_science",
        "data": {
            "description": "The Web Of Science (WOS) dataset is a collection of data  of published papers\navailable from the Web of Science. WOS has been released in three versions: WOS-46985, WOS-11967 and WOS-5736. WOS-46985 is the\nfull dataset. WOS-11967 and WOS-5736 are two subsets of WOS-46985.\n\nWeb of Science Dataset WOS-5736: This dataset contains 5,736 documents with 11 categories which include 3 parents categories.",
            "url": "https://data.mendeley.com/datasets/9rw3vkcfy4/6",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "web_questions",
        "data": {
            "description": "This dataset consists of 6,642 question/answer pairs.\nThe questions are supposed to be answerable by Freebase, a large knowledge graph.\nThe questions are mostly centered around a single named entity.\nThe questions are popular ones asked on the web (at least in 2013).\n",
            "url": "https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "weibo_ner",
        "data": {
            "description": "Tags: PER(人名), LOC(地点名), GPE(行政区名), ORG(机构名)\nLabel\tTag\tMeaning\nPER\tPER.NAM\t名字（张三）\nPER.NOM\t代称、类别名（穷人）\nLOC\tLOC.NAM\t特指名称（紫玉山庄）\nLOC.NOM\t泛称（大峡谷、宾馆）\nGPE\tGPE.NAM\t行政区的名称（北京）\nORG\tORG.NAM\t特定机构名称（通惠医院）\nORG.NOM\t泛指名称、统称（文艺公司）\n",
            "url": "https://github.com/OYE93/Chinese-NLP-Corpus/tree/master/NER/Weibo",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "zh"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "wider_face",
        "data": {
            "description": "WIDER FACE dataset is a face detection benchmark dataset, of which images are\nselected from the publicly available WIDER dataset. We choose 32,203 images and\nlabel 393,703 faces with a high degree of variability in scale, pose and\nocclusion as depicted in the sample images. WIDER FACE dataset is organized\nbased on 61 event classes. For each event class, we randomly select 40%/10%/50%\ndata as training, validation and testing sets. We adopt the same evaluation\nmetric employed in the PASCAL VOC dataset. Similar to MALF and Caltech datasets,\nwe do not release bounding box ground truth for the test images. Users are\nrequired to submit final prediction files, which we shall proceed to evaluate.\n",
            "url": "http://shuoyang1213.me/WIDERFACE/",
            "license": "Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)",
            "givenLicense": "cc-by-nc-nd-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "object-detection"
            ],
            "tasks": [
                "face-detection"
            ]
        }
    },
    {
        "id": "wiki40b",
        "data": {
            "description": "\nClean-up text for 40+ Wikipedia languages editions of pages\ncorrespond to entities. The datasets have train/dev/test splits per language.\nThe dataset is cleaned up by page filtering to remove disambiguation pages,\nredirect pages, deleted pages, and non-entity pages. Each example contains the\nwikidata id of the entity, and the full Wikipedia article after page processing\nthat removes non-content sections and structured objects.\n",
            "url": "https://research.google/pubs/pub49029/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "wikiann",
        "data": {
            "description": "WikiANN (sometimes called PAN-X) is a multilingual named entity recognition dataset consisting of Wikipedia articles annotated with LOC (location), PER (person), and ORG (organisation) tags in the IOB2 format. This version corresponds to the balanced train, dev, and test splits of Rahimi et al. (2019), which supports 176 of the 282 languages from the original WikiANN corpus.",
            "url": "https://github.com/afshinrahimi/mmner",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "no",
                "ace",
                "af",
                "als",
                "am",
                "an",
                "ang",
                "ar",
                "arc",
                "arz",
                "as",
                "ast",
                "ay",
                "az",
                "ba",
                "bar",
                "be",
                "be-tarask",
                "bg",
                "bh",
                "bn",
                "bo",
                "br",
                "bs",
                "ca",
                "cbk",
                "cdo",
                "ce",
                "ceb",
                "ckb",
                "co",
                "crh",
                "cs",
                "csb",
                "cv",
                "cy",
                "da",
                "de",
                "diq",
                "dv",
                "el",
                "eml",
                "en",
                "en-basiceng",
                "eo",
                "es",
                "et",
                "eu",
                "ext",
                "fa",
                "fi",
                "fo",
                "fr",
                "frr",
                "fur",
                "fy",
                "ga",
                "gan",
                "gd",
                "gl",
                "gn",
                "gu",
                "hak",
                "he",
                "hi",
                "hr",
                "hsb",
                "hu",
                "hy",
                "ia",
                "id",
                "ig",
                "ilo",
                "io",
                "is",
                "it",
                "ja",
                "jbo",
                "jv",
                "jv-x-bms",
                "ka",
                "kk",
                "km",
                "kn",
                "ko",
                "ksh",
                "ku",
                "ky",
                "la",
                "lb",
                "li",
                "lij",
                "lmo",
                "ln",
                "lt",
                "lv",
                "lzh",
                "mg",
                "mhr",
                "mi",
                "min",
                "mk",
                "ml",
                "mn",
                "mr",
                "ms",
                "mt",
                "mwl",
                "my",
                "mzn",
                "nan",
                "nap",
                "nds",
                "ne",
                "nl",
                "nn",
                "nov",
                "oc",
                "or",
                "os",
                "pa",
                "pdc",
                "pl",
                "pms",
                "pnb",
                "ps",
                "pt",
                "qu",
                "rm",
                "ro",
                "ru",
                "rw",
                "sa",
                "sah",
                "scn",
                "sco",
                "sd",
                "sgs",
                "sh",
                "si",
                "sk",
                "sl",
                "so",
                "sq",
                "sr",
                "su",
                "sv",
                "sw",
                "szl",
                "ta",
                "te",
                "tg",
                "th",
                "tk",
                "tl",
                "tr",
                "tt",
                "ug",
                "uk",
                "ur",
                "uz",
                "vec",
                "vep",
                "vi",
                "vls",
                "vo",
                "vro",
                "wa",
                "war",
                "wuu",
                "xmf",
                "yi",
                "yo",
                "yue",
                "zea",
                "zh"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "wiki_asp",
        "data": {
            "description": "WikiAsp is a multi-domain, aspect-based summarization dataset in the encyclopedic\ndomain. In this task, models are asked to summarize cited reference documents of a\nWikipedia article into aspect-based summaries. Each of the 20 domains include 10\ndomain-specific pre-defined aspects.\n",
            "url": "https://github.com/neulab/wikiasp",
            "license": "CC BY-SA 4.0",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "summarization-other-aspect-based-summarization"
            ]
        }
    },
    {
        "id": "wiki_atomic_edits",
        "data": {
            "description": "A dataset of atomic wikipedia edits containing insertions and deletions of a contiguous chunk of text in a sentence. This dataset contains ~43 million edits across 8 languages.\n\nAn atomic edit is defined as an edit e applied to a natural language expression S as the insertion, deletion, or substitution of a sub-expression P such that both the original expression S and the resulting expression e(S) are well-formed semantic constituents (MacCartney, 2009). In this corpus, we release such atomic insertions and deletions made to sentences in wikipedia.\n",
            "url": "https://github.com/google-research-datasets/wiki-atomic-edits",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "de",
                "en",
                "es",
                "fr",
                "it",
                "jp",
                "ru",
                "zh"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": []
        }
    },
    {
        "id": "wiki_auto",
        "data": {
            "description": "WikiAuto provides a set of aligned sentences from English Wikipedia and Simple English Wikipedia\nas a resource to train sentence simplification systems. The authors first crowd-sourced a set of manual alignments\nbetween sentences in a subset of the Simple English Wikipedia and their corresponding versions in English Wikipedia\n(this corresponds to the `manual` config), then trained a neural CRF system to predict these alignments.\nThe trained model was then applied to the other articles in Simple English Wikipedia with an English counterpart to\ncreate a larger corpus of aligned sentences (corresponding to the `auto`, `auto_acl`, `auto_full_no_split`, and `auto_full_with_split`  configs here).\n",
            "url": "https://github.com/chaojiang06/wiki-auto",
            "license": "CC-BY-SA 3.0",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text-simplification"
            ]
        }
    },
    {
        "id": "wiki_bio",
        "data": {
            "description": "This dataset gathers 728,321 biographies from wikipedia. It aims at evaluating text generation\nalgorithms. For each article, we provide the first paragraph and the infobox (both tokenized).\nFor each article, we extracted the first paragraph (text), the infobox (structured data). Each\ninfobox is encoded as a list of (field name, field value) pairs. We used Stanford CoreNLP\n(http://stanfordnlp.github.io/CoreNLP/) to preprocess the data, i.e. we broke the text into\nsentences and tokenized both the text and the field values. The dataset was randomly split in\nthree subsets train (80%), valid (10%), test (10%).\n",
            "url": "https://github.com/DavidGrangier/wikipedia-biography-dataset",
            "license": "CC BY-SA 3.0",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "table-to-text"
            ],
            "tasks": []
        }
    },
    {
        "id": "wikicorpus",
        "data": {
            "description": "The Wikicorpus is a trilingual corpus (Catalan, Spanish, English) that contains large portions of the Wikipedia (based on a 2006 dump) and has been automatically enriched with linguistic information. \nIn its present version, it contains over 750 million words.\n",
            "url": "https://www.cs.upc.edu/~nlp/wikicorpus/",
            "license": "GNU Free Documentation License",
            "givenLicense": "gfdl",
            "language": [
                "ca",
                "en",
                "es"
            ],
            "categories": [
                "fill-mask",
                "text-classification",
                "text-generation",
                "token-classification"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling",
                "part-of-speech-tagging",
                "text-classification-other-word-sense-disambiguation",
                "token-classification-other-lemmatization"
            ]
        }
    },
    {
        "id": "wiki_dpr",
        "data": {
            "description": "\nThis is the wikipedia split used to evaluate the Dense Passage Retrieval (DPR) model.\nIt contains 21M passages from wikipedia along with their DPR embeddings.\nThe wikipedia articles were split into multiple, disjoint text blocks of 100 words as passages.\n",
            "url": "https://github.com/facebookresearch/DPR",
            "license": "",
            "givenLicense": "cc-by-sa-3.0,gfdl",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "other-text-search"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling",
                "other-neural-search"
            ]
        }
    },
    {
        "id": "wiki_hop",
        "data": {
            "description": "WikiHop is open-domain and based on Wikipedia articles; the goal is to recover Wikidata information by hopping through documents. The goal is to answer text understanding queries by combining multiple facts that are spread across different documents.\n",
            "url": "http://qangaroo.cs.ucl.ac.uk/",
            "license": "",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa",
                "question-answering-other-multi-hop"
            ]
        }
    },
    {
        "id": "wikihow",
        "data": {
            "description": "WikiHow is a new large-scale dataset using the online WikiHow\n(http://www.wikihow.com/) knowledge base.\n\nThere are two features:\n  - text: wikihow answers texts.\n  - headline: bold lines as summary.\n\nThere are two separate versions:\n  - all: consisting of the concatenation of all paragraphs as the articles and\n         the bold lines as the reference summaries.\n  - sep: consisting of each paragraph and its summary.\n\nDownload \"wikihowAll.csv\" and \"wikihowSep.csv\" from\nhttps://github.com/mahnazkoupaee/WikiHow-Dataset and place them in manual folder\nhttps://www.tensorflow.org/datasets/api_docs/python/tfds/download/DownloadConfig.\nTrain/validation/test splits are provided by the authors.\nPreprocessing is applied to remove short articles\n(abstract length < 0.75 article length) and clean up extra commas.\n",
            "url": "https://github.com/mahnazkoupaee/WikiHow-Dataset",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "wiki_lingua",
        "data": {
            "description": "WikiLingua is a large-scale multilingual dataset for the evaluation of\ncrosslingual abstractive summarization systems. The dataset includes ~770k\narticle and summary pairs in 18 languages from WikiHow. The gold-standard\narticle-summary alignments across languages was done by aligning the images\nthat are used to describe each how-to step in an article.\n",
            "url": "https://github.com/esdurmus/Wikilingua",
            "license": "CC BY-NC-SA 3.0",
            "givenLicense": "cc-by-3.0",
            "language": [
                "ar",
                "cs",
                "de",
                "en",
                "es",
                "fr",
                "hi",
                "id",
                "it",
                "ja",
                "ko",
                "nl",
                "pt",
                "ru",
                "th",
                "tr",
                "vi",
                "zh"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": []
        }
    },
    {
        "id": "wiki_movies",
        "data": {
            "description": "The WikiMovies dataset consists of roughly 100k (templated) questions over 75k entities based on questions with answers in the open movie database (OMDb).\n",
            "url": "https://research.fb.com/downloads/babi/",
            "license": "Creative Commons Public License (CCPL)",
            "givenLicense": "cc-by-3.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "closed-domain-qa"
            ]
        }
    },
    {
        "id": "wikipedia",
        "data": {
            "description": "Wikipedia dataset containing cleaned articles of all languages.\nThe datasets are built from the Wikipedia dump\n(https://dumps.wikimedia.org/) with one split per language. Each example\ncontains the content of one full Wikipedia article with cleaning to strip\nmarkdown and unwanted sections (references, etc.).\n",
            "url": "https://dumps.wikimedia.org",
            "license": "",
            "givenLicense": "cc-by-sa-3.0,gfdl",
            "language": [
                "no",
                "aa",
                "ab",
                "ace",
                "af",
                "ak",
                "als",
                "am",
                "an",
                "ang",
                "ar",
                "arc",
                "arz",
                "as",
                "ast",
                "atj",
                "av",
                "ay",
                "az",
                "azb",
                "ba",
                "bar",
                "bcl",
                "be",
                "bg",
                "bh",
                "bi",
                "bjn",
                "bm",
                "bn",
                "bo",
                "bpy",
                "br",
                "bs",
                "bug",
                "bxr",
                "ca",
                "cbk",
                "cdo",
                "ce",
                "ceb",
                "ch",
                "cho",
                "chr",
                "chy",
                "ckb",
                "co",
                "cr",
                "crh",
                "cs",
                "csb",
                "cu",
                "cv",
                "cy",
                "da",
                "de",
                "din",
                "diq",
                "dsb",
                "dty",
                "dv",
                "dz",
                "ee",
                "el",
                "eml",
                "en",
                "eo",
                "es",
                "et",
                "eu",
                "ext",
                "fa",
                "ff",
                "fi",
                "fj",
                "fo",
                "fr",
                "frp",
                "frr",
                "fur",
                "fy",
                "ga",
                "gag",
                "gan",
                "gd",
                "gl",
                "glk",
                "gn",
                "gom",
                "gor",
                "got",
                "gu",
                "gv",
                "ha",
                "hak",
                "haw",
                "he",
                "hi",
                "hif",
                "ho",
                "hr",
                "hsb",
                "ht",
                "hu",
                "hy",
                "ia",
                "id",
                "ie",
                "ig",
                "ii",
                "ik",
                "ilo",
                "inh",
                "io",
                "is",
                "it",
                "iu",
                "ja",
                "jam",
                "jbo",
                "jv",
                "ka",
                "kaa",
                "kab",
                "kbd",
                "kbp",
                "kg",
                "ki",
                "kj",
                "kk",
                "kl",
                "km",
                "kn",
                "ko",
                "koi",
                "krc",
                "ks",
                "ksh",
                "ku",
                "kv",
                "kw",
                "ky",
                "la",
                "lad",
                "lb",
                "lbe",
                "lez",
                "lfn",
                "lg",
                "li",
                "lij",
                "lmo",
                "ln",
                "lo",
                "lrc",
                "lt",
                "ltg",
                "lv",
                "lzh",
                "mai",
                "mdf",
                "mg",
                "mh",
                "mhr",
                "mi",
                "min",
                "mk",
                "ml",
                "mn",
                "mr",
                "mrj",
                "ms",
                "mt",
                "mus",
                "mwl",
                "my",
                "myv",
                "mzn",
                "na",
                "nah",
                "nan",
                "nap",
                "nds",
                "nds-nl",
                "ne",
                "new",
                "ng",
                "nl",
                "nn",
                "nov",
                "nrf",
                "nso",
                "nv",
                "ny",
                "oc",
                "olo",
                "om",
                "or",
                "os",
                "pa",
                "pag",
                "pam",
                "pap",
                "pcd",
                "pdc",
                "pfl",
                "pi",
                "pih",
                "pl",
                "pms",
                "pnb",
                "pnt",
                "ps",
                "pt",
                "qu",
                "rm",
                "rmy",
                "rn",
                "ro",
                "ru",
                "rue",
                "rup",
                "rw",
                "sa",
                "sah",
                "sat",
                "sc",
                "scn",
                "sco",
                "sd",
                "se",
                "sg",
                "sgs",
                "sh",
                "si",
                "simple",
                "sk",
                "sl",
                "sm",
                "sn",
                "so",
                "sq",
                "sr",
                "srn",
                "ss",
                "st",
                "stq",
                "su",
                "sv",
                "sw",
                "szl",
                "ta",
                "tcy",
                "tdt",
                "te",
                "tg",
                "th",
                "ti",
                "tk",
                "tl",
                "tn",
                "to",
                "tpi",
                "tr",
                "ts",
                "tt",
                "tum",
                "tw",
                "ty",
                "tyv",
                "udm",
                "ug",
                "uk",
                "unknown",
                "ur",
                "uz",
                "ve",
                "vec",
                "vep",
                "vi",
                "vls",
                "vo",
                "vro",
                "wa",
                "war",
                "wo",
                "wuu",
                "xal",
                "xh",
                "xmf",
                "yi",
                "yo",
                "yue",
                "za",
                "zea",
                "zh",
                "zu"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "wiki_qa",
        "data": {
            "description": "Wiki Question Answering corpus from Microsoft\n",
            "url": "https://www.microsoft.com/en-us/download/details.aspx?id=52419",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "wiki_qa_ar",
        "data": {
            "description": "Arabic Version of WikiQA by automatic automatic machine translators and crowdsourced the selection of the best one to be incorporated into the corpus\n",
            "url": "https://github.com/qcri/WikiQAar",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "ar"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "wiki_snippets",
        "data": {
            "description": "Wikipedia version split into plain text snippets for dense semantic indexing.\n",
            "url": "https://dumps.wikimedia.org",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "sequence-modeling",
                "other"
            ],
            "tasks": [
                "language-modeling",
                "other-text-search"
            ]
        }
    },
    {
        "id": "wiki_source",
        "data": {
            "description": "2 languages, total number of files: 132\ntotal number of tokens: 1.80M\ntotal number of sentence fragments: 78.36k\n",
            "url": "http://opus.nlpl.eu/WikiSource.php",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "sv"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "wiki_split",
        "data": {
            "description": "One million English sentences, each split into two sentences that together preserve the original meaning, extracted from Wikipedia \nGoogle's WikiSplit dataset was constructed automatically from the publicly available Wikipedia revision history. Although \nthe dataset contains some inherent noise, it can serve as valuable training data for models that split or merge sentences.\n",
            "url": "https://dataset-homepage/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "wikisql",
        "data": {
            "description": "A large crowd-sourced dataset for developing natural language interfaces for relational databases\n",
            "url": "https://github.com/salesforce/WikiSQL",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "wiki_summary",
        "data": {
            "description": "The dataset extracted from Persian Wikipedia into the form of articles and highlights and cleaned the dataset into pairs of articles and highlights and reduced the articles' length (only version 1.0.0) and highlights' length to a maximum of 512 and 128, respectively, suitable for parsBERT.\n",
            "url": "https://github.com/m3hrdadfi/wiki-summary",
            "license": "",
            "givenLicense": "apache-2.0",
            "language": [
                "fa"
            ],
            "categories": [
                "text2text-generation",
                "translation",
                "question-answering"
            ],
            "tasks": [
                "abstractive-qa",
                "explanation-generation",
                "extractive-qa",
                "open-domain-qa",
                "open-domain-abstractive-qa",
                "summarization",
                "text-simplification"
            ]
        }
    },
    {
        "id": "wikitablequestions",
        "data": {
            "description": "This WikiTableQuestions dataset is a large-scale dataset for the task of question answering on semi-structured tables.\n",
            "url": "https://nlp.stanford.edu/software/sempre/wikitable",
            "license": "Creative Commons Attribution Share Alike 4.0 International",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "question-answering-other-table-question-answering"
            ]
        }
    },
    {
        "id": "wikitext",
        "data": {
            "description": " The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike\n License.\n",
            "url": "https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/",
            "license": "Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)",
            "givenLicense": "cc-by-sa-3.0,gfdl",
            "language": [
                "en"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "wikitext_tl39",
        "data": {
            "description": "Large scale, unlabeled text dataset with 39 Million tokens in the training set. Inspired by the original WikiText Long Term Dependency dataset (Merity et al., 2016). TL means \"Tagalog.\" Originally published in Cruz & Cheng (2019).\n",
            "url": "https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks",
            "license": "GPL-3.0",
            "givenLicense": "gpl-3.0",
            "language": [
                "fil",
                "tl"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "wili_2018",
        "data": {
            "description": "It is a benchmark dataset for language identification and contains 235000 paragraphs of 235 languages\n",
            "url": "https://zenodo.org/record/841984",
            "license": "ODC Open Database License v1.0",
            "givenLicense": "odbl",
            "language": [
                "af",
                "am",
                "an",
                "as",
                "av",
                "ay",
                "bs",
                "ce",
                "co",
                "cv",
                "dv",
                "eo",
                "gl",
                "gn",
                "gu",
                "ha",
                "hr",
                "ht",
                "ia",
                "id",
                "ie",
                "ig",
                "io",
                "ja",
                "jv",
                "km",
                "ko",
                "ku",
                "kv",
                "ky",
                "lb",
                "lg",
                "li",
                "ln",
                "lo",
                "mg",
                "mi",
                "ml",
                "mn",
                "mr",
                "ms",
                "my",
                "ne",
                "om",
                "or",
                "os",
                "pa",
                "rm",
                "rw",
                "sc",
                "sd",
                "si",
                "sk",
                "sn",
                "so",
                "sr",
                "su",
                "sw",
                "ta",
                "th",
                "tl",
                "tn",
                "to",
                "ug",
                "vi",
                "vo",
                "wa",
                "wo",
                "xh",
                "yo",
                "zh",
                "ar",
                "az",
                "ba",
                "be",
                "bg",
                "bn",
                "bo",
                "br",
                "ca",
                "cs",
                "cy",
                "da",
                "de",
                "el",
                "en",
                "es",
                "et",
                "eu",
                "fa",
                "fi",
                "fo",
                "fr",
                "fy",
                "ga",
                "gd",
                "gv",
                "he",
                "hi",
                "hu",
                "hy",
                "is",
                "it",
                "ka",
                "kk",
                "kn",
                "kw",
                "la",
                "lt",
                "lv",
                "mk",
                "mt",
                "nb",
                "nl",
                "nn",
                "nv",
                "oc",
                "pl",
                "ps",
                "pt",
                "qu",
                "ro",
                "ru",
                "sa",
                "sh",
                "sl",
                "sq",
                "sv",
                "te",
                "tg",
                "tk",
                "tr",
                "tt",
                "uk",
                "ur",
                "uz",
                "yi",
                "ace",
                "als",
                "ang",
                "arz",
                "ast",
                "azb",
                "bar",
                "bcl",
                "bho",
                "bjn",
                "bpy",
                "bxr",
                "cbk",
                "cdo",
                "ceb",
                "chr",
                "ckb",
                "crh",
                "csb",
                "diq",
                "dsb",
                "dty",
                "egl",
                "ext",
                "frp",
                "fur",
                "gag",
                "glk",
                "hak",
                "hif",
                "hsb",
                "ilo",
                "jam",
                "jbo",
                "kaa",
                "kab",
                "kbd",
                "koi",
                "kok",
                "krc",
                "ksh",
                "lad",
                "lez",
                "lij",
                "lmo",
                "lrc",
                "ltg",
                "lzh",
                "mai",
                "mdf",
                "mhr",
                "min",
                "mrj",
                "mwl",
                "myv",
                "mzn",
                "nan",
                "nap",
                "nci",
                "nds",
                "new",
                "nrm",
                "nso",
                "olo",
                "pag",
                "pam",
                "pap",
                "pcd",
                "pdc",
                "pfl",
                "pnb",
                "rue",
                "rup",
                "sah",
                "scn",
                "sco",
                "sgs",
                "sme",
                "srn",
                "stq",
                "szl",
                "tcy",
                "tet",
                "tyv",
                "udm",
                "vec",
                "vep",
                "vls",
                "vro",
                "war",
                "wuu",
                "xmf",
                "zea",
                "other-roa-tara",
                "other-zh-yue",
                "other-map-bms",
                "other-nds-nl",
                "other-be-tarask"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-language-identification"
            ]
        }
    },
    {
        "id": "wi_locness",
        "data": {
            "description": "Write & Improve (Yannakoudakis et al., 2018) is an online web platform that assists non-native\nEnglish students with their writing. Specifically, students from around the world submit letters,\nstories, articles and essays in response to various prompts, and the W&I system provides instant\nfeedback. Since W&I went live in 2014, W&I annotators have manually annotated some of these\nsubmissions and assigned them a CEFR level.\n",
            "url": "https://www.cl.cam.ac.uk/research/nl/bea2019st/#data",
            "license": "",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "text2text-generation"
            ],
            "tasks": [
                "text2text-generation-other-grammatical-error-correction"
            ]
        }
    },
    {
        "id": "wino_bias",
        "data": {
            "description": "WinoBias, a Winograd-schema dataset for coreference resolution focused on gender bias.\nThe corpus contains Winograd-schema style sentences with entities corresponding to people\nreferred by their occupation (e.g. the nurse, the doctor, the carpenter).\n",
            "url": "https://uclanlp.github.io/corefBias/overview",
            "license": "MIT License (https://github.com/uclanlp/corefBias/blob/master/LICENSE)",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "coreference-resolution"
            ]
        }
    },
    {
        "id": "winograd_wsc",
        "data": {
            "description": "A Winograd schema is a pair of sentences that differ in only one or two words and that contain an ambiguity that is\nresolved in opposite ways in the two sentences and requires the use of world knowledge and reasoning for its\nresolution. The schema takes its name from a well-known example by Terry Winograd:\n\n> The city councilmen refused the demonstrators a permit because they [feared/advocated] violence.\n\nIf the word is ``feared'', then ``they'' presumably refers to the city council; if it is ``advocated'' then ``they''\npresumably refers to the demonstrators.\n",
            "url": "https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "multiple-choice"
            ],
            "tasks": [
                "coreference-resolution"
            ]
        }
    },
    {
        "id": "winogrande",
        "data": {
            "description": "WinoGrande is a new collection of 44k problems, inspired by Winograd Schema Challenge (Levesque, Davis, and Morgenstern\n 2011), but adjusted to improve the scale and robustness against the dataset-specific bias. Formulated as a\nfill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires\ncommonsense reasoning.\n",
            "url": "https://leaderboard.allenai.org/winogrande/submissions/get-started",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "wiqa",
        "data": {
            "description": "The WIQA dataset V1 has 39705 questions containing a perturbation and a possible effect in the context of a paragraph. \nThe dataset is split into 29808 train questions, 6894 dev questions and 3003 test questions.\n",
            "url": "https://allenai.org/data/wiqa",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "wisesight1000",
        "data": {
            "description": "\n",
            "url": "https://github.com/PyThaiNLP/wisesight-sentiment",
            "license": "CC-0 3.0",
            "givenLicense": "cc0-1.0",
            "language": [
                "th"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "token-classification-other-word-tokenization"
            ]
        }
    },
    {
        "id": "wisesight_sentiment",
        "data": {
            "description": "Wisesight Sentiment Corpus: Social media messages in Thai language with sentiment category (positive, neutral, negative, question)\n* Released to public domain under Creative Commons Zero v1.0 Universal license.\n* Category (Labels): {\"pos\": 0, \"neu\": 1, \"neg\": 2, \"q\": 3}\n* Size: 26,737 messages\n* Language: Central Thai\n* Style: Informal and conversational. With some news headlines and advertisement.\n* Time period: Around 2016 to early 2019. With small amount from other period.\n* Domains: Mixed. Majority are consumer products and services (restaurants, cosmetics, drinks, car, hotels), with some current affairs.\n* Privacy:\n    * Only messages that made available to the public on the internet (websites, blogs, social network sites).\n    * For Facebook, this means the public comments (everyone can see) that made on a public page.\n    * Private/protected messages and messages in groups, chat, and inbox are not included.\n* Alternations and modifications:\n    * Keep in mind that this corpus does not statistically represent anything in the language register.\n    * Large amount of messages are not in their original form. Personal data are removed or masked.\n    * Duplicated, leading, and trailing whitespaces are removed. Other punctuations, symbols, and emojis are kept intact.\n    (Mis)spellings are kept intact.\n    * Messages longer than 2,000 characters are removed.\n    * Long non-Thai messages are removed. Duplicated message (exact match) are removed.\n* More characteristics of the data can be explore: https://github.com/PyThaiNLP/wisesight-sentiment/blob/master/exploration.ipynb\n",
            "url": "https://github.com/PyThaiNLP/wisesight-sentiment",
            "license": "",
            "givenLicense": "cc0-1.0",
            "language": [
                "th"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "wmt14",
        "data": {
            "description": "Translate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `datasets.translate.wmt.WmtConfig`.\n\n```\nconfig = datasets.wmt.WmtConfig(\n    version=\"0.0.1\",\n    language_pair=(\"fr\", \"de\"),\n    subsets={\n        datasets.Split.TRAIN: [\"commoncrawl_frde\"],\n        datasets.Split.VALIDATION: [\"euelections_dev2019\"],\n    },\n)\nbuilder = datasets.builder(\"wmt_translate\", config=config)\n```\n\n",
            "url": "http://www.statmt.org/wmt14/translation-task.html",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "wmt15",
        "data": {
            "description": "Translate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `datasets.translate.wmt.WmtConfig`.\n\n```\nconfig = datasets.wmt.WmtConfig(\n    version=\"0.0.1\",\n    language_pair=(\"fr\", \"de\"),\n    subsets={\n        datasets.Split.TRAIN: [\"commoncrawl_frde\"],\n        datasets.Split.VALIDATION: [\"euelections_dev2019\"],\n    },\n)\nbuilder = datasets.builder(\"wmt_translate\", config=config)\n```\n\n",
            "url": "http://www.statmt.org/wmt15/translation-task.html",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "wmt16",
        "data": {
            "description": "Translate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `datasets.translate.wmt.WmtConfig`.\n\n```\nconfig = datasets.wmt.WmtConfig(\n    version=\"0.0.1\",\n    language_pair=(\"fr\", \"de\"),\n    subsets={\n        datasets.Split.TRAIN: [\"commoncrawl_frde\"],\n        datasets.Split.VALIDATION: [\"euelections_dev2019\"],\n    },\n)\nbuilder = datasets.builder(\"wmt_translate\", config=config)\n```\n\n",
            "url": "http://www.statmt.org/wmt16/translation-task.html",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "wmt17",
        "data": {
            "description": "Translate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `datasets.translate.wmt.WmtConfig`.\n\n```\nconfig = datasets.wmt.WmtConfig(\n    version=\"0.0.1\",\n    language_pair=(\"fr\", \"de\"),\n    subsets={\n        datasets.Split.TRAIN: [\"commoncrawl_frde\"],\n        datasets.Split.VALIDATION: [\"euelections_dev2019\"],\n    },\n)\nbuilder = datasets.builder(\"wmt_translate\", config=config)\n```\n\n",
            "url": "http://www.statmt.org/wmt17/translation-task.html",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "wmt18",
        "data": {
            "description": "Translate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `datasets.translate.wmt.WmtConfig`.\n\n```\nconfig = datasets.wmt.WmtConfig(\n    version=\"0.0.1\",\n    language_pair=(\"fr\", \"de\"),\n    subsets={\n        datasets.Split.TRAIN: [\"commoncrawl_frde\"],\n        datasets.Split.VALIDATION: [\"euelections_dev2019\"],\n    },\n)\nbuilder = datasets.builder(\"wmt_translate\", config=config)\n```\n\n",
            "url": "http://www.statmt.org/wmt18/translation-task.html",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "wmt19",
        "data": {
            "description": "Translate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `datasets.translate.wmt.WmtConfig`.\n\n```\nconfig = datasets.wmt.WmtConfig(\n    version=\"0.0.1\",\n    language_pair=(\"fr\", \"de\"),\n    subsets={\n        datasets.Split.TRAIN: [\"commoncrawl_frde\"],\n        datasets.Split.VALIDATION: [\"euelections_dev2019\"],\n    },\n)\nbuilder = datasets.builder(\"wmt_translate\", config=config)\n```\n\n",
            "url": "http://www.statmt.org/wmt19/translation-task.html",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "wmt20_mlqe_task1",
        "data": {
            "description": "This shared task (part of WMT20) will build on its previous editions\nto further examine automatic methods for estimating the quality\nof neural machine translation output at run-time, without relying\non reference translations. As in previous years, we cover estimation\nat various levels. Important elements introduced this year include: a new\ntask where sentences are annotated with Direct Assessment (DA)\nscores instead of labels based on post-editing; a new multilingual\nsentence-level dataset mainly from Wikipedia articles, where the\nsource articles can be retrieved for document-wide context; the\navailability of NMT models to explore system-internal information for the task.\n\nTask 1 uses Wikipedia data for 6 language pairs that includes high-resource\nEnglish--German (En-De) and English--Chinese (En-Zh), medium-resource\nRomanian--English (Ro-En) and Estonian--English (Et-En), and low-resource\nSinhalese--English (Si-En) and Nepalese--English (Ne-En), as well as a\ndataset with a combination of Wikipedia articles and Reddit articles\nfor Russian-English (En-Ru). The datasets were collected by translating\nsentences sampled from source language articles using state-of-the-art NMT\nmodels built using the fairseq toolkit and annotated with Direct Assessment (DA)\nscores by professional translators. Each sentence was annotated following the\nFLORES setup, which presents a form of DA, where at least three professional\ntranslators rate each sentence from 0-100 according to the perceived translation\nquality. DA scores are standardised using the z-score by rater. Participating systems\nare required to score sentences according to z-standardised DA scores.\n",
            "url": "http://www.statmt.org/wmt20/quality-estimation-task.html",
            "license": "Unknown",
            "givenLicense": "unknown",
            "language": [
                "de",
                "en",
                "et",
                "ne",
                "ro",
                "ru",
                "si",
                "zh"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "wmt20_mlqe_task2",
        "data": {
            "description": "This shared task (part of WMT20) will build on its previous editions\nto further examine automatic methods for estimating the quality\nof neural machine translation output at run-time, without relying\non reference translations. As in previous years, we cover estimation\nat various levels. Important elements introduced this year include: a new\ntask where sentences are annotated with Direct Assessment (DA)\nscores instead of labels based on post-editing; a new multilingual\nsentence-level dataset mainly from Wikipedia articles, where the\nsource articles can be retrieved for document-wide context; the\navailability of NMT models to explore system-internal information for the task.\n\nTask 2 evaluates the application of QE for post-editing purposes. It consists of predicting:\n- A/ Word-level tags. This is done both on source side (to detect which words caused errors)\nand target side (to detect mistranslated or missing words).\n  - A1/ Each token is tagged as either `OK` or `BAD`. Additionally,\n  each gap between two words is tagged as `BAD` if one or more\n  missing words should have been there, and `OK` otherwise. Note\n  that number of tags for each target sentence is 2*N+1, where\n  N is the number of tokens in the sentence.\n  - A2/ Tokens are tagged as `OK` if they were correctly\n  translated, and `BAD` otherwise. Gaps are not tagged.\n- B/ Sentence-level HTER scores. HTER (Human Translation Error Rate)\nis the ratio between the number of edits (insertions/deletions/replacements)\nneeded and the reference translation length.\n",
            "url": "http://www.statmt.org/wmt20/quality-estimation-task.html",
            "license": "Unknown",
            "givenLicense": "unknown",
            "language": [
                "de",
                "en",
                "zh"
            ],
            "categories": [
                "translation",
                "text-classification"
            ],
            "tasks": [
                "text-classification-other-translation-quality-estimation"
            ]
        }
    },
    {
        "id": "wmt20_mlqe_task3",
        "data": {
            "description": "This shared task (part of WMT20) will build on its previous editions\nto further examine automatic methods for estimating the quality\nof neural machine translation output at run-time, without relying\non reference translations. As in previous years, we cover estimation\nat various levels. Important elements introduced this year include: a new\ntask where sentences are annotated with Direct Assessment (DA)\nscores instead of labels based on post-editing; a new multilingual\nsentence-level dataset mainly from Wikipedia articles, where the\nsource articles can be retrieved for document-wide context; the\navailability of NMT models to explore system-internal information for the task.\n\nThe goal of this task 3 is to predict document-level quality scores as well as fine-grained annotations.\n",
            "url": "http://www.statmt.org/wmt20/quality-estimation-task.html",
            "license": "Unknown",
            "givenLicense": "unknown",
            "language": [
                "en",
                "fr"
            ],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "wmt_t2t",
        "data": {
            "description": "Translate dataset based on the data from statmt.org.\n\nVersions exists for the different years using a combination of multiple data\nsources. The base `wmt_translate` allows you to create your own config to choose\nyour own data/language pair by creating a custom `datasets.translate.wmt.WmtConfig`.\n\n```\nconfig = datasets.wmt.WmtConfig(\n    version=\"0.0.1\",\n    language_pair=(\"fr\", \"de\"),\n    subsets={\n        datasets.Split.TRAIN: [\"commoncrawl_frde\"],\n        datasets.Split.VALIDATION: [\"euelections_dev2019\"],\n    },\n)\nbuilder = datasets.builder(\"wmt_translate\", config=config)\n```\n\n",
            "url": "https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/translate_ende.py",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [
                "translation"
            ],
            "tasks": []
        }
    },
    {
        "id": "wnut_17",
        "data": {
            "description": "WNUT 17: Emerging and Rare entity recognition\n\nThis shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions.\nNamed entities form the basis of many modern approaches to other tasks (like event clustering and summarisation),\nbut recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms.\nTake for example the tweet “so.. kktny in 30 mins?” - even human experts find entity kktny hard to detect and resolve.\nThis task will evaluate the ability to detect and classify novel, emerging, singleton named entities in noisy text.\n\nThe goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities.\n",
            "url": "http://noisy-text.github.io/2017/emerging-rare-entities.html",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "wongnai_reviews",
        "data": {
            "description": "Wongnai's review dataset contains restaurant reviews and ratings, mainly in Thai language.\nThe reviews are in 5 classes ranging from 1 to 5 stars.\n",
            "url": "https://github.com/wongnai/wongnai-corpus",
            "license": "LGPL-3.0",
            "givenLicense": "lgpl-3.0",
            "language": [
                "th"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "woz_dialogue",
        "data": {
            "description": "Wizard-of-Oz (WOZ) is a dataset for training task-oriented dialogue systems. The dataset is designed around the task of finding a restaurant in the Cambridge, UK area. There are three informable slots (food, pricerange,area) that users can use to constrain the search and six requestable slots (address, phone, postcode plus the three informable slots) that the user can ask a value for once a restaurant has been offered.\n",
            "url": "https://github.com/nmrksic/neural-belief-tracker/tree/master/data/woz",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "de",
                "en",
                "it"
            ],
            "categories": [
                "text-generation",
                "fill-mask",
                "token-classification",
                "text-classification",
                "natural-language-processing"
            ],
            "tasks": [
                "dialogue-modeling",
                "multi-class-classification",
                "parsing"
            ]
        }
    },
    {
        "id": "wrbsc",
        "data": {
            "description": "WUT Relations Between Sentences Corpus contains 2827 pairs of related sentences.\nRelationships are derived from Cross-document Structure Theory (CST), which enables multi-document summarization through identification of cross-document rhetorical relationships within a cluster of related documents.\nEvery relation was marked by at least 3 annotators.\n",
            "url": "https://clarin-pl.eu/dspace/handle/11321/305",
            "license": "Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0)",
            "givenLicense": "cc-by-sa-3.0",
            "language": [
                "pl"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "semantic-similarity-classification"
            ]
        }
    },
    {
        "id": "xcopa",
        "data": {
            "description": "  XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning\nThe Cross-lingual Choice of Plausible Alternatives dataset is a benchmark to evaluate the ability of machine learning models to transfer commonsense reasoning across\nlanguages. The dataset is the translation and reannotation of the English COPA (Roemmele et al. 2011) and covers 11 languages from 11 families and several areas around\nthe globe. The dataset is challenging as it requires both the command of world knowledge and the ability to generalise to new languages. All the details about the\ncreation of XCOPA and the implementation of the baselines are available in the paper.\n\nXcopa language et",
            "url": "https://github.com/cambridgeltl/xcopa",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "et",
                "ht",
                "id",
                "it",
                "qu",
                "sw",
                "ta",
                "th",
                "tr",
                "vi",
                "zh"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "xcsr",
        "data": {
            "description": "To evaluate multi-lingual language models (ML-LMs) for commonsense reasoning in a cross-lingual zero-shot transfer setting (X-CSR), i.e., training in English and test in other languages, we create two benchmark datasets, namely X-CSQA and X-CODAH. Specifically, we automatically translate the original CSQA and CODAH datasets, which only have English versions, to 15 other languages, forming development and test sets for studying X-CSR. As our goal is to evaluate different ML-LMs in a unified evaluation protocol for X-CSR, we argue that such translated examples, although might contain noise, can serve as a starting benchmark for us to obtain meaningful analysis, before more human-translated datasets will be available in the future.\n",
            "url": "https://inklab.usc.edu//XCSR/",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "en",
                "zh",
                "de",
                "es",
                "fr",
                "it",
                "ja",
                "nl",
                "pl",
                "pt",
                "ru",
                "ar",
                "vi",
                "hi",
                "sw",
                "ur"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "multiple-choice-qa"
            ]
        }
    },
    {
        "id": "xed_en_fi",
        "data": {
            "description": "A multilingual fine-grained emotion dataset. The dataset consists of human annotated Finnish (25k) and English sentences (30k). Plutchik’s\ncore emotions are used to annotate the dataset with the addition of neutral to create a multilabel multiclass\ndataset. The dataset is carefully evaluated using language-specific BERT models and SVMs to\nshow that XED performs on par with other similar datasets and is therefore a useful tool for\nsentiment analysis and emotion detection.\n",
            "url": "",
            "license": "License: Creative Commons Attribution 4.0 International License (CC-BY)",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en",
                "fi"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "intent-classification",
                "multi-class-classification",
                "multi-label-classification",
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "xglue",
        "data": {
            "description": "XGLUE is a new benchmark dataset to evaluate the performance of cross-lingual pre-trained\nmodels with respect to cross-lingual natural language understanding and generation.\nThe benchmark is composed of the following 11 tasks:\n- NER\n- POS Tagging (POS)\n- News Classification (NC)\n- MLQA\n- XNLI\n- PAWS-X\n- Query-Ad Matching (QADSM)\n- Web Page Ranking (WPR)\n- QA Matching (QAM)\n- Question Generation (QG)\n- News Title Generation (NTG)\n\nFor more information, please take a look at https://microsoft.github.io/XGLUE/.\n",
            "url": "https://www.clips.uantwerpen.be/conll2003/ner/",
            "license": "",
            "givenLicense": "cc-by-nc-4.0,cc-by-sa-4.0,other",
            "language": [
                "ar",
                "bg",
                "de",
                "el",
                "en",
                "es",
                "fr",
                "hi",
                "it",
                "nl",
                "pl",
                "pt",
                "ru",
                "sw",
                "th",
                "tr",
                "ur",
                "vi",
                "zh"
            ],
            "categories": [
                "question-answering",
                "summarization",
                "text-classification",
                "text2text-generation",
                "token-classification",
                "natural-language-processing"
            ],
            "tasks": [
                "acceptability-classification",
                "extractive-qa",
                "named-entity-recognition",
                "natural-language-inference",
                "news-articles-headline-generation",
                "open-domain-qa",
                "parsing",
                "text-classification-other-paraphrase-identification",
                "text2text-generation-other-question-answering",
                "topic-classification"
            ]
        }
    },
    {
        "id": "xnli",
        "data": {
            "description": "XNLI is a subset of a few thousand examples from MNLI which has been translated\ninto a 14 different languages (some low-ish resource). As with MNLI, the goal is\nto predict textual entailment (does sentence A imply/contradict/neither sentence\nB) and is a classification task (given two sentences, predict one of three\nlabels).\n",
            "url": "https://www.nyu.edu/projects/bowman/xnli/",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "xor_tydi_qa",
        "data": {
            "description": "    XOR-TyDi QA brings together for the first time information-seeking questions,\n    open-retrieval QA, and multilingual QA to create a multilingual open-retrieval\n    QA dataset that enables cross-lingual answer retrieval. It consists of questions\n    written by information-seeking native speakers in 7 typologically diverse languages\n    and answer annotations that are retrieved from multilingual document collections.\n    There are three sub-tasks: XOR-Retrieve, XOR-EnglishSpan, and XOR-Full.\n\nXOR-Retrieve is a cross-lingual retrieval task where a question is written in the target\nlanguage (e.g., Japanese) and a system is required to retrieve English document that answers the question.\n",
            "url": "https://nlp.cs.washington.edu/xorqa/",
            "license": "",
            "givenLicense": "mit",
            "language": [
                "ar",
                "bn",
                "fi",
                "ja",
                "ko",
                "ru",
                "te"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "xquad",
        "data": {
            "description": "XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering\nperformance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set\nof SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German,\nGreek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi and Romanian. Consequently, the dataset is entirely parallel\nacross 12 languages.\n",
            "url": "https://github.com/deepmind/xquad",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "en",
                "fr",
                "es",
                "de",
                "el",
                "bg",
                "ru",
                "tr",
                "ar",
                "vi",
                "zh"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "xquad_r",
        "data": {
            "description": "XQuAD-R is a retrieval version of the XQuAD dataset (a cross-lingual extractive QA dataset). Like XQuAD, XQUAD-R is an 11-way parallel dataset, where each question appears in 11 different languages and has 11 parallel correct answers across the languages.\n",
            "url": "https://github.com/google-research-datasets/lareqa",
            "license": "",
            "givenLicense": "cc-by-sa-4.0",
            "language": [
                "ar",
                "de",
                "el",
                "en",
                "es",
                "hi",
                "ru",
                "th",
                "tr",
                "vi",
                "zh"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "extractive-qa"
            ]
        }
    },
    {
        "id": "x_stance",
        "data": {
            "description": "The x-stance dataset contains more than 150 political questions, and 67k comments written by candidates on those questions.\n\nIt can be used to train and evaluate stance detection systems.\n\n",
            "url": "https://github.com/ZurichNLP/xstance",
            "license": "",
            "givenLicense": "",
            "language": [],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "xsum",
        "data": {
            "description": "\nExtreme Summarization (XSum) Dataset.\n\nThere are three features:\n  - document: Input news article.\n  - summary: One sentence summary of the article.\n  - id: BBC ID of the article.\n\n",
            "url": "https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "news-articles-summarization"
            ]
        }
    },
    {
        "id": "xsum_factuality",
        "data": {
            "description": "Neural abstractive summarization models are highly prone to hallucinate content that is unfaithful to the input\ndocument. The popular metric such as ROUGE fails to show the severity of the problem. The dataset consists of\nfaithfulness and factuality annotations of abstractive summaries for the XSum dataset. We have crowdsourced 3 judgements\n for each of 500 x 5 document-system pairs. This will be a valuable resource to the abstractive summarization community.\n",
            "url": "https://research.google/tools/datasets/xsum-hallucination-annotations/",
            "license": "https://creativecommons.org/licenses/by/4.0/",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "summarization"
            ],
            "tasks": [
                "summarization-other-hallucinations"
            ]
        }
    },
    {
        "id": "xtreme",
        "data": {
            "description": "\nThe Cross-lingual Natural Language Inference (XNLI) corpus is a crowd-sourced collection of 5,000 test and\n2,500 dev pairs for the MultiNLI corpus. The pairs are annotated with textual entailment and translated into\n14 languages: French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese,\nHindi, Swahili and Urdu. This results in 112.5k annotated pairs. Each premise can be associated with the\ncorresponding hypothesis in the 15 languages, summing up to more than 1.5M combinations. The corpus is made to\nevaluate how to perform inference in any language (including low-resources ones like Swahili or Urdu) when only\nEnglish NLI data is available at training time. One solution is cross-lingual sentence encoding, for which XNLI\nis an evaluation benchmark.\nThe Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark is a benchmark for the evaluation of\nthe cross-lingual generalization ability of pre-trained multilingual models. It covers 40 typologically diverse languages\n(spanning 12 language families) and includes nine tasks that collectively require reasoning about different levels of\nsyntax and semantics. The languages in XTREME are selected to maximize language diversity, coverage in existing tasks,\nand availability of training data. Among these are many under-studied languages, such as the Dravidian languages Tamil\n(spoken in southern India, Sri Lanka, and Singapore), Telugu and Malayalam (spoken mainly in southern India), and the\nNiger-Congo languages Swahili and Yoruba, spoken in Africa.\n",
            "url": "https://github.com/google-research/xtreme\thttps://www.nyu.edu/projects/bowman/xnli/",
            "license": "",
            "givenLicense": "apache-2.0,cc-by-4.0,cc-by-2.0,cc-by-sa-4.0,other,cc-by-nc-4.0",
            "language": [
                "af",
                "ar",
                "bg",
                "bn",
                "de",
                "el",
                "en",
                "es",
                "et",
                "eu",
                "fa",
                "fa-IR",
                "fi",
                "fr",
                "he",
                "hi",
                "hu",
                "id",
                "it",
                "ja",
                "jv",
                "ka",
                "kk",
                "ko",
                "ml",
                "mr",
                "ms",
                "my",
                "nl",
                "pt",
                "ru",
                "sw",
                "ta",
                "te",
                "th",
                "tl",
                "tr",
                "ur",
                "vi",
                "yo",
                "zh"
            ],
            "categories": [
                "multiple-choice",
                "question-answering",
                "token-classification",
                "text-classification",
                "text-retrieval",
                "token-classification"
            ],
            "tasks": [
                "multiple-choice-qa",
                "extractive-qa",
                "open-domain-qa",
                "natural-language-inference",
                "text-classification-other-paraphrase-identification",
                "text-retrieval-other-parallel-sentence-retrieval",
                "named-entity-recognition",
                "part-of-speech-tagging"
            ]
        }
    },
    {
        "id": "yahoo_answers_qa",
        "data": {
            "description": "Yahoo Non-Factoid Question Dataset is derived from Yahoo's Webscope L6 collection using machine learning techiques such that the questions would contain non-factoid answers.The dataset contains 87,361 questions and their corresponding answers. Each question contains its best answer along with additional other answers submitted by users. Only the best answer was reviewed in determining the quality of the question-answer pair.\n",
            "url": "https://ciir.cs.umass.edu/downloads/nfL6/index.html",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering"
            ],
            "tasks": [
                "open-domain-qa"
            ]
        }
    },
    {
        "id": "yahoo_answers_topics",
        "data": {
            "description": "\nYahoo! Answers Topic Classification is text classification dataset. The dataset is the Yahoo! Answers corpus as of 10/25/2007. The Yahoo! Answers topic classification dataset is constructed using 10 largest main categories. From all the answers and other meta-information, this dataset only used the best answer content and the main category information.\n",
            "url": "https://github.com/LC-John/Yahoo-Answers-Topic-Classification-Dataset",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "topic-classification"
            ]
        }
    },
    {
        "id": "yelp_polarity",
        "data": {
            "description": "Large Yelp Review Dataset.\nThis is a dataset for binary sentiment classification. We provide a set of 560,000 highly polar yelp reviews for training, and 38,000 for testing. \nORIGIN\nThe Yelp reviews dataset consists of reviews from Yelp. It is extracted\nfrom the Yelp Dataset Challenge 2015 data. For more information, please\nrefer to http://www.yelp.com/dataset_challenge\n\nThe Yelp reviews polarity dataset is constructed by\nXiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\nIt is first used as a text classification benchmark in the following paper:\nXiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks\nfor Text Classification. Advances in Neural Information Processing Systems 28\n(NIPS 2015).\n\n\nDESCRIPTION\n\nThe Yelp reviews polarity dataset is constructed by considering stars 1 and 2\nnegative, and 3 and 4 positive. For each polarity 280,000 training samples and\n19,000 testing samples are take randomly. In total there are 560,000 trainig\nsamples and 38,000 testing samples. Negative polarity is class 1,\nand positive class 2.\n\nThe files train.csv and test.csv contain all the training samples as\ncomma-sparated values. There are 2 columns in them, corresponding to class\nindex (1 and 2) and review text. The review texts are escaped using double\nquotes (\"), and any internal double quote is escaped by 2 double quotes (\"\").\nNew lines are escaped by a backslash followed with an \"n\" character,\nthat is \"\n\".\n",
            "url": "https://course.fast.ai/datasets",
            "license": "",
            "givenLicense": "",
            "language": [
                "en"
            ],
            "categories": [],
            "tasks": []
        }
    },
    {
        "id": "yelp_review_full",
        "data": {
            "description": "The Yelp reviews dataset consists of reviews from Yelp. It is extracted from the Yelp Dataset Challenge 2015 data.\nThe Yelp reviews full star dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the above dataset.\nIt is first used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun.\nCharacter-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
            "url": "https://www.yelp.com/dataset",
            "license": "https://s3-media3.fl.yelpcdn.com/assets/srv0/engineering_pages/bea5c1e92bf3/assets/vendor/yelp-dataset-agreement.pdf",
            "givenLicense": "other",
            "language": [
                "en"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "sentiment-classification"
            ]
        }
    },
    {
        "id": "yoruba_bbc_topics",
        "data": {
            "description": "A collection of news article headlines in Yoruba from BBC Yoruba.\nEach headline is labeled with one of the following classes: africa,\nentertainment, health, nigeria, politics, sport or world.\n\nThe dataset was presented in the paper:\nHedderich, Adelani, Zhu, Alabi, Markus, Klakow: Transfer Learning and\nDistant Supervision for Multilingual Transformer Models: A Study on\nAfrican Languages (EMNLP 2020).\n",
            "url": "https://github.com/uds-lsv/transfer-distant-transformer-african",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "yo"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "topic-classification"
            ]
        }
    },
    {
        "id": "yoruba_gv_ner",
        "data": {
            "description": "The Yoruba GV NER dataset is a labeled dataset for named entity recognition in Yoruba. The texts were obtained from\nYoruba Global Voices News articles https://yo.globalvoices.org/ . We concentrate on\nfour types of named entities: persons [PER], locations [LOC], organizations [ORG], and dates & time [DATE].\n\nThe Yoruba GV NER data files contain 2 columns separated by a tab ('\t'). Each word has been put on a separate line and\nthere is an empty line after each sentences i.e the CoNLL format. The first item on each line is a word, the second\nis the named entity tag. The named entity tags have the format I-TYPE which means that the word is inside a phrase\nof type TYPE. For every multi-word expression like 'New York', the first word gets a tag B-TYPE and the subsequent words\nhave tags I-TYPE, a word with tag O is not part of a phrase. The dataset is in the BIO tagging scheme.\n\nFor more details, see https://www.aclweb.org/anthology/2020.lrec-1.335/\n",
            "url": "https://www.aclweb.org/anthology/2020.lrec-1.335/",
            "license": "",
            "givenLicense": "cc-by-3.0",
            "language": [
                "yo"
            ],
            "categories": [
                "token-classification"
            ],
            "tasks": [
                "named-entity-recognition"
            ]
        }
    },
    {
        "id": "yoruba_text_c3",
        "data": {
            "description": "Yoruba Text C3 is the largest Yoruba texts collected and used to train FastText embeddings in the \nYorubaTwi Embedding paper: https://www.aclweb.org/anthology/2020.lrec-1.335/\n",
            "url": "https://www.aclweb.org/anthology/2020.lrec-1.335/",
            "license": "",
            "givenLicense": "cc-by-nc-4.0",
            "language": [
                "yo"
            ],
            "categories": [
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "language-modeling",
                "masked-language-modeling"
            ]
        }
    },
    {
        "id": "yoruba_wordsim353",
        "data": {
            "description": "A translation of the word pair similarity dataset wordsim-353 to Yorùbá.\n\nThe dataset was presented in the paper\nAlabi et al.: Massive vs. Curated Embeddings for Low-Resourced\nLanguages: the Case of Yorùbá and Twi (LREC 2020).\n",
            "url": "https://github.com/ajesujoba/YorubaTwi-Embedding",
            "license": "",
            "givenLicense": "unknown",
            "language": [
                "en",
                "yo"
            ],
            "categories": [
                "text-classification"
            ],
            "tasks": [
                "text-scoring",
                "semantic-similarity-scoring"
            ]
        }
    },
    {
        "id": "youtube_caption_corrections",
        "data": {
            "description": "Dataset built from pairs of YouTube captions where both 'auto-generated' and\n'manually-corrected' captions are available for a single specified language.\nThis dataset labels two-way (e.g. ignoring single-sided insertions) same-length\ntoken differences in the `diff_type` column. The `default_seq` is composed of\ntokens from the 'auto-generated' captions. When a difference occurs between\nthe 'auto-generated' vs 'manually-corrected' captions types, the `correction_seq`\ncontains tokens from the 'manually-corrected' captions.\n",
            "url": "https://github.com/2dot71mily/youtube_captions_corrections",
            "license": "MIT License",
            "givenLicense": "mit",
            "language": [
                "en"
            ],
            "categories": [
                "other",
                "text-generation",
                "fill-mask"
            ],
            "tasks": [
                "other-other-token-classification-of-text-errors",
                "slot-filling"
            ]
        }
    },
    {
        "id": "zest",
        "data": {
            "description": "ZEST tests whether NLP systems can perform unseen tasks in a zero-shot way, given a natural language description of\nthe task. It is an instantiation of our proposed framework \"learning from task descriptions\". The tasks include\nclassification, typed entity extraction and relationship extraction, and each task is paired with 20 different\nannotated (input, output) examples. ZEST's structure allows us to systematically test whether models can generalize\nin five different ways.\n",
            "url": "https://allenai.org/data/zest",
            "license": "",
            "givenLicense": "cc-by-4.0",
            "language": [
                "en"
            ],
            "categories": [
                "question-answering",
                "token-classification"
            ],
            "tasks": [
                "closed-domain-qa",
                "extractive-qa",
                "question-answering-other-yes-no-qa",
                "token-classification-other-output-structure"
            ]
        }
    }
]